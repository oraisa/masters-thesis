%% This file is modified by Veli Mäkinen from HY_fysiikka_LuKtemplate.tex authored by Roope Halonen ja Tomi Vainio.
%% Some text is also inherited from engl_malli.tex by Kutvonen, Erkiö, Mäkelä, Verkamo, Kurhila, and Nykänen.


% STEP 1: Choose oneside or twoside
\documentclass[english,twoside,openright]{HYgraduMLDS}
%finnish,swedish

\usepackage{lmodern} % Font package
\usepackage{textcomp} % Package for special symbols
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
\usepackage{amsthm}
%\usepackage[square]{natbib} % For bibliography
\usepackage[footnotesize,bf]{caption} % For more control over figure captions
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage[titletoc]{appendix}
\usepackage[ruled, vlined]{algorithm2e}

\onehalfspacing %line spacing
%\singlespacing
%\doublespacing

%\fussy 
\sloppy % sloppy and fussy commands can be used to avoid overlong text lines

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{Differentially Private Markov Chain Monte Carlo}
\author{Ossi Räisä}
\date{\today}
\prof{Professor Antti Honkela}
\censors{Professor Antti Honkela}{Dr. Antti Koskela}{}
\keywords{Differential Privacy, Markov Chain Monte Carlo}
\depositeplace{}
\additionalinformation{}


\classification{\protect{\ \\
% \  General and reference $\rightarrow$ Document types  $\rightarrow$ Surveys and overviews\  \\
% \  Applied computing  $\rightarrow$ Document management and text processing  $\rightarrow$ Document management $\rightarrow$ Text editing\\
}}

% if you want to quote someone special. You can comment this line and there will be nothing on the document.
%\quoting{Bachelor's degrees make pretty good placemats if you get them laminated.}{Jeph Jacques} 


% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
\hypersetup{
    bookmarks=true,         % show bookmarks bar first?
    unicode=true,           % to show non-Latin characters in Acrobatâs bookmarks
    pdftoolbar=true,        % show Acrobatâs toolbar?
    pdfmenubar=true,        % show Acrobatâs menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,        % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\cl}[1]{\overline{#1}}
\newcommand{\kl}{D_{\mathrm{KL}}}
\newcommand{\dmid}{\mid\mid}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\calm}{{\mathcal{M}}}
\newcommand{\calx}{{\mathcal{X}}}
\newcommand{\calu}{{\mathcal{U}}}
\newcommand{\caln}{{\mathcal{N}}}
\newcommand{\call}{{\mathcal{L}}}
\newcommand{\caly}{{\mathcal{Y}}}
\DeclareMathOperator{\erfc}{erfc}

\begin{document}

% Generate title page.
\maketitle

% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.

\begin{abstract}
 
\end{abstract}

% Place ToC
\mytableofcontents

\mynomenclature

% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\chapter{Introduction}

\chapter{Background}

\section{Differential Privacy}

Differential privacy~\cite{DwR14} is a property of an algorithm that quantifies the 
amount of information about private data an adversary can gain from the 
publication of the algorithm's output.
The most commonly used definition uses two real numbers, 
\(\epsilon\) and \(\delta\), to quantify the information gain, or, from the 
perspective of a data subject, the privacy loss of the algorithm.

The most common definition is called \((\epsilon, \delta)\)-DP, approximate DP 
or ADP~\cite{DwR14}.
The case where \(\delta = 0\) is called \(\epsilon\)-DP or 
pure DP.

\begin{definition}\label{ADP-definition}
    An algorithm \(\calm\colon \calx \to \calu\) is \((\epsilon, \delta)\)-ADP if 
    for all neighbouring inputs \(x\in \calx\) and \(x'\in \calx\) and 
    all measurable sets \(S \subset \calu\)
    \[
        P(\calm(x)\in S) \leq e^\epsilon P(\calm(x')\in S) + \delta
    \]
\end{definition}

The neighbourhood relation in the definition is domain specific. With tabular 
data the most common definitions are the add/remove neighbourhood and 
substitute neighbourhood.
\begin{definition}
    Two tabular datasets are said to be add/remove neighbours if they are equal 
    after adding or removing at most one row to or from one of them. The datasets 
    are said to be in substitute neighbours if they are equal after 
    changing at most one row in one of them.
\end{definition}
The neighbourhood relation is denoted by \(\sim\). The definitions and 
theorems of this section are valid for all neighbourhood relations.

There many other definitions of differential privacy that are mostly used 
to compute \((\epsilon, \delta)\)-bounds for ADP. This thesis uses two of them: 
Rényi-DP (RDP)~\cite{Mironov17} and 
zero-concentrated differential privacy (zCDP)~\cite{BuS16}. Both are based 
on Rényi divergence~\cite{Mironov17}, which is a particular way of 
measuring the difference between random variables.

\begin{definition}
    For random variables with density or probability mass functions 
    \(P\) and \(Q\) the Rényi divergence of order 
    \(1 < \alpha < \infty\) is
    \[
        D_\alpha(P\dmid Q) = \frac{1}{\alpha - 1}\ln E_{x\sim Q}
        \left(\frac{P(x)^\alpha}{Q(y)^\alpha}\right)
    \]
    Orders \(\alpha = 1\) and \(\alpha = \infty\) are defined 
    by continuity:
    \[
        D_1(P\dmid Q) = \lim_{\alpha \to 1-} D_\alpha(P\dmid Q)
    \]
    \[
        D_\infty(P \dmid Q) = \lim_{\alpha\to \infty}D_\alpha(P\dmid Q)
    \]
\end{definition}

Both Rényi-DP and zCDP can be expressed as bounds on the 
Rényi divergence between the outputs of an algorithm with 
neighbouring inputs:

\begin{definition}
    An algorithm \(\calm\) is \((\alpha, \epsilon)\)-Rényi DP 
    if for all \(x \sim x'\)
    \[
        D_\alpha(\calm(x)\dmid \calm(x')) \leq \epsilon
    \]
    \(\calm\) is \(\rho\)-zCDP if for all \(\alpha > 1\)
    and all \(x \sim x'\)
    \[
        D_\alpha(\calm(x)\dmid \calm(x')) \leq \rho \alpha
    \]

\end{definition}

A very useful property of all of these definitions is composition~\cite{DwR14}: 
if algorithms \(\calm\) and \(\calm'\) are DP, the algorithm first computing 
\(\calm\) and then \(\calm'\), outputting both results, 
is also DP, although with worse bounds.
More precisely

\begin{definition}
    Let \(\calm\colon \calx \to \calu\) and 
    \(\calm'\colon \calx\times \calu \to \calu'\) be algorithms.
    Their composition is the algorithm outputting 
    \((\calm(x), \calm'(x, \calm(x)))\) for input \(x\).
\end{definition}

\begin{theorem}\label{composition-theorem}
    Let \(\calm\colon \calx \to \calu\) and 
    \(\calm\colon \calx\times \calu \to \calu'\) be algorithms. Then 
    \begin{enumerate}
        \item 
            If \(\calm\) is \((\epsilon, \delta)\)-ADP and 
            \(\calm'\) is \((\epsilon', \delta')\)-ADP, then 
            their composition is 
            \((\epsilon + \epsilon', \delta + \delta')\)-ADP~\cite{DwR14}
        \item 
            If \(\calm\) is \((\alpha, \epsilon)\)-RDP and 
            \(\calm'\) is \((\alpha, \epsilon')\)-RDP, then 
            their composition is \((\alpha, \epsilon + \epsilon')\)-RDP~\cite{Mironov17}
        \item 
            If \(\calm\) is \(\rho\)-zCDP and 
            \(\calm'\) is \(\rho'\)-zCDP, then 
            their composition is \((\rho + \rho')\)-zCDP~\cite{BuS16}
    \end{enumerate}
\end{theorem}

All of the composition results can be extended to any number of compositions 
by induction. Note that any step of the composition can depend on the results 
of the previous steps, not only on the private data.

As any algorithm that does not use private data in any way is 
\((0, 0)\)-ADP, 0-zCDP and \((\alpha, 0)\)-RDP with all \(\alpha\), 
theorem~\ref{composition-theorem} has the following corollary, called 
post-processing immunity:

\begin{theorem}
    Let \(\calm\colon \calx\to \calu\) be an ADP, RDP or zCDP algorithm with 
    some privacy parameters. Let \(f\colon \calu\to \calu'\) be any algorithm 
    not using the private data. Then the composition of \(\calm\) and \(f\)
    is ADP, RDP or zCDP with the same privacy parameters.
\end{theorem}

There are many different DP algorithms that are commonly used, which are also
called mechanisms~\cite{DwR14}. This thesis only requires one of the most commonly 
used ones: the Gaussian mechanism~\cite{DwR14}.
\begin{definition}
    The Gaussian mechanism with parameter \(\sigma^2\) 
    is an algorithm that, with input \(x\), 
    outputs a sample from \(\caln(x, \sigma^2)\), where \(\caln\) denotes 
    the normal distribution.
\end{definition}

The RDP and zCDP bounds for the Gaussian 
mechanism are quite simple. The ADP bound is more complicated:

\begin{theorem}\label{gauss-DP-bounds}
    If for all inputs \(x\) and \(x'\), \(||x - x'||_2 \leq \Delta\),
    the Gaussian mechanism is 
    \begin{enumerate}
        \item 
            \((\alpha, \frac{\alpha \Delta^2}{2\sigma^2})\)-RDP~\cite{Mironov17}
        \item 
            \(\frac{\Delta^2}{2\sigma^2}\)-zCDP~\cite{BuS16}
        \item 
            \(n\) compositions of the Gaussian mechanism are 
            \((\epsilon, \delta(\epsilon))\)-ADP~\cite{Sommer2019} with 
            \[
                \delta(\epsilon) 
                = \frac{1}{2}\left(
                    \erfc\left(\frac{\sigma(\epsilon - n\mu)}{\sqrt{2n}\Delta}\right)
                    - e^\epsilon \erfc\left(\frac{\sigma(\epsilon + n\mu)}{\sqrt{2n}\Delta}\right)
                \right)
            \]
            where \(\mu = \frac{\Delta^2}{2\sigma^2}\) and \(\erfc\) is 
            the complementary error function.
    \end{enumerate}
\end{theorem}

The most common use case for the Gaussian mechanism is computing a 
function \(f\colon \calx \to \R\) of private data and feeding the result into 
the Gaussian mechanism to privately release the function value. 
The condition that the inputs 
of the Gaussian mechanism cannot vary too much leads into the concept of 
sensitivity of a function
\begin{definition}
    The \(l_p\)-sensitivity \(\Delta_p\), with neighbourhood relation \(\sim\),
    of a function \(f\colon \calx \to \R^n\)
    is 
    \[
        \Delta_p f = \sup_{x\sim x'}||f(x) - f(x')||_p
    \]
\end{definition}

Theorem \ref{gauss-DP-bounds} implies that the value of any function with 
finite \(l_2\)-sensitivity can be privately released using the Gaussian mechanism 
with appropriate noise variance \(\sigma^2\). Of course, the usefulness of the 
released value depends on the magnitude of \(\sigma^2\) compared to the actual 
value.

\section{Bayesian Inference and Markov Chain Monte Carlo}

In Bayesian inference, the parameters of a statistical model are inferred from 
observed data using Bayes' theorem. The result is not just a point estimate 
of the parameters, but a probability distribution describing the likelihood 
of different values of the parameters.

Bayes' theorem relates the \emph{posterior} belief of the parameters 
\(p(\theta \mid D)\) to the \emph{prior} belief \(p(\theta)\) through the 
observed data \(D\) and the likelihood of the data \(p(D\mid \theta)\) as follows:
\[
    p(\theta \mid D) = \frac{p(D \mid \theta)p(\theta)}
    {\int p(D\mid \theta)p(\theta)d\theta}
\]
It is theoretically possible to compute \(p(\theta \mid D)\) given any 
likelihood, prior and data, but the integral in the denominator is in many 
cases difficult to compute. In such cases the posterior cannot be feasibly 
computed. However, many of the commonly used summary statistics of the posterior, 
such as the mean, variance and credible intervals, can be approximated from 
a sample of the posterior. \emph{Markov chain Monte Carlo} (MCMC) is a 
widely used algorithm to obtain such samples.

Markov chain Monte Carlo algorithms sequentially sample values of \(\theta\)
with the goal of eventually having the chain of sampled values converge to 
a given distribution. While this can be done in many ways, this thesis 
focuses on a particular MCMC algorithm: \emph{Metropolis-Hastings} (MH).

The Metropolis-Hastings algorithm samples from a distribution \(\pi\) of 
\(\theta_i\) by first picking a proposal \(\theta^*\) from a proposal 
distribution \(q(\theta_{i-1})\) at iteration \(i\).
A density ratio is calculated
\[
    r = \frac{\pi(\theta^*)}{\pi(\theta_{i-1})}
    \frac{q(\theta_{i-1}\mid \theta^*)}{q(\theta^*\mid \theta_{i-1})}
\]
and the proposal is accepted with probability \(\min\{1, r\}\). 
If the proposal is accepted, 
\(\theta_i = \theta^*\), otherwise \(\theta_i = \theta_{i-1}\).

It can be shown that, with a suitable proposal distribution, the chain of 
\(\theta_i\) values converges to \(\pi\). The Gaussian distribution centered 
at the current value is a commonly used proposal.

When MCMC is used in Bayesian inference, the distribution to approximate is 
\[
    \pi(\theta) = p(\theta \mid D) = \frac{p(D \mid \theta)p(\theta)}
    {\int p(D\mid \theta)p(\theta)d\theta}
\]
The difficult integral \(\int p(D\mid \theta)p(\theta)d\theta\) in the denominator 
cancels out when computing \(r\), so only the likelihood and prior are needed. 
For numerical stability, \(r\) is usually computed in 
log space, which makes the acceptance probability \(\min\{1, e^\lambda\}\)
where 
\[
    \lambda = \ln \frac{p(\theta^* \mid D)}{p(\theta_{i-1}\mid D)}
    + \ln \frac{p(\theta^*)}{p(\theta_{i-1})}
    + \ln \frac{q(\theta_{i-1}\mid \theta^*)}{q(\theta^*\mid \theta_{i-1})}
\]

The dataset \(D\) is typically a table with \(n\) independent rows.
The likelihood is given as 
\[
    p(\theta\mid D_j)
\]
for row \(D_j\). The independence means that 
\[
    p(\theta\mid D) = \prod_{j=1}^k p(\theta\mid D_j)
\]
which means that the log likelihood ratio term of \(\lambda\) is 
\[
    \ln \frac{p(\theta^*\mid D)}{p(\theta_{i-1}\mid D)}
    = \sum_{j=1}^n \ln\frac{p(\theta^*\mid D_j)}{p(\theta_{i-1}\mid D_j)}
\]
Algorithm~\ref{MH_algo} puts all of this together to summarise the MH 
algorithm used for Bayesian inference.

\begin{algorithm}[H]\label{MH_algo}
    \SetAlgoLined
    \For{\(1 \leq i \leq k\)}{
        sample \(\theta^* \sim q(\theta_{i-1})\)\\
        \(\lambda = 
        \sum_{j=1}^n \ln\frac{p(\theta^*\mid D_j)}{p(\theta_{i-1}\mid D_j)}
        + \ln \frac{p(\theta^*)}{p(\theta_{i-1})}
        + \ln \frac{q(\theta_{i-1}\mid \theta^*)}{q(\theta^*\mid \theta_{i-1})}\)
        \\
        \(\theta_i = \begin{cases}
            \theta^* & \text{ with probability } \min\{1, e^\lambda\} \\
            \theta_{i-1} & \text{ otherwise}
        \end{cases}
        \)\\
    }
    \Return \((\theta_1, \dotsc, \theta_k)\)
    \caption{
        Metropolis-Hastings: number of iterations \(k\), proposal 
        distribution \(q\) and initial value \(\theta_0\) and 
        dataset \(D\) as input
    }
\end{algorithm}

\chapter{Differentially Private MCMC}

\chapter{Variations of the Penalty Algorithm}

\chapter{The Gauss-Bernoulli Algorithm}

\chapter{Experiments}

\chapter{Conclusions}
% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\cleardoublepage %fixes the position of bibliography in bookmarks
\phantomsection

\addcontentsline{toc}{chapter}{\bibname} % This lines adds the bibliography to the ToC
\bibliographystyle{abbrv} % numbering alphabetic order
\bibliography{../references.bib}

% \begin{appendices}
% \myappendixtitle
%
% \chapter{Code example\label{appendix:code}}
% Program code can be added as appendix:
% \begin{verbatim}
% #!/bin/bash          
% text="Hello World!"
% echo $text
% \end{verbatim}
%
% \end{appendices}

\end{document}
