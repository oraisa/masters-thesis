%% This file is modified by Veli Mäkinen from HY_fysiikka_LuKtemplate.tex authored by Roope Halonen ja Tomi Vainio.
%% Some text is also inherited from engl_malli.tex by Kutvonen, Erkiö, Mäkelä, Verkamo, Kurhila, and Nykänen.


% STEP 1: Choose oneside or twoside
\documentclass[english,twoside,openright]{HYgraduMLDS}
%finnish,swedish

\usepackage{lmodern} % Font package
\usepackage{textcomp} % Package for special symbols
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
\usepackage{amsthm}
%\usepackage[square]{natbib} % For bibliography
\usepackage[footnotesize,bf]{caption} % For more control over figure captions
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage[titletoc]{appendix}
\usepackage[ruled, vlined]{algorithm2e}

\onehalfspacing %line spacing
%\singlespacing
%\doublespacing

%\fussy 
\sloppy % sloppy and fussy commands can be used to avoid overlong text lines

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{Differentially Private Markov Chain Monte Carlo}
\author{Ossi Räisä}
\date{\today}
\prof{Associate Professor Antti Honkela}
\censors{Associate Professor Antti Honkela}{Dr. Antti Koskela}{}
\keywords{Differential Privacy, Markov Chain Monte Carlo}
\depositeplace{}
\additionalinformation{}


\classification{\protect{\ \\
% \  General and reference $\rightarrow$ Document types  $\rightarrow$ Surveys and overviews\  \\
% \  Applied computing  $\rightarrow$ Document management and text processing  $\rightarrow$ Document management $\rightarrow$ Text editing\\
}}

% if you want to quote someone special. You can comment this line and there will be nothing on the document.
%\quoting{Bachelor's degrees make pretty good placemats if you get them laminated.}{Jeph Jacques} 


% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
\hypersetup{
    % bookmarks=true,         % show bookmarks bar first?
    unicode=true,           % to show non-Latin characters in Acrobatâs bookmarks
    pdftoolbar=true,        % show Acrobatâs toolbar?
    pdfmenubar=true,        % show Acrobatâs menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=blue,        % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\cl}[1]{\overline{#1}}
\newcommand{\kl}{D_{\mathrm{KL}}}
\newcommand{\dmid}{\mid\mid}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\calm}{{\mathcal{M}}}
\newcommand{\calx}{{\mathcal{X}}}
\newcommand{\calu}{{\mathcal{U}}}
\newcommand{\caln}{{\mathcal{N}}}
\newcommand{\call}{{\mathcal{L}}}
\newcommand{\caly}{{\mathcal{Y}}}
\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\ban}{Ban}
\DeclareMathOperator{\diag}{diag}

\begin{document}

% Generate title page.
\maketitle

% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.

\begin{abstract}
 
\end{abstract}

% Place ToC
\mytableofcontents

\mynomenclature

% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\chapter{Introduction}

\chapter{Background}

\section{Differential Privacy}\label{DP_background}

Differential privacy~\cite{DwR14} is a property of an algorithm that quantifies the 
amount of information about private data an adversary can gain from the 
publication of the algorithm's output.
The most commonly used definition uses two real numbers, 
\(\epsilon\) and \(\delta\), to quantify the information gain, or, from the 
perspective of a data subject, the privacy loss of the algorithm.

The most common definition is called \((\epsilon, \delta)\)-DP, approximate DP 
or ADP~\cite{DwR14}.
The case where \(\delta = 0\) is called \(\epsilon\)-DP or 
pure DP.

\begin{definition}\label{ADP-definition}
    An algorithm \(\calm\colon \calx \to \calu\) is \((\epsilon, \delta)\)-ADP if 
    for all neighbouring inputs \(x\in \calx\) and \(x'\in \calx\) and 
    all measurable sets \(S \subset \calu\)
    \[
        P(\calm(x)\in S) \leq e^\epsilon P(\calm(x')\in S) + \delta.
    \]
\end{definition}

The neighbourhood relation in the definition is domain specific. With tabular 
data the most common definitions are the add/remove neighbourhood and 
substitute neighbourhood.
\begin{definition}
    Two tabular datasets are said to be add/remove neighbours if they are equal 
    after adding or removing at most one row to or from one of them. The datasets 
    are said to be in substitute neighbours if they are equal after 
    changing at most one row in one of them.
\end{definition}
The neighbourhood relation is denoted by \(\sim\). The definitions and 
theorems of this section are valid for all neighbourhood relations.

There many other definitions of differential privacy that are mostly used 
to compute \((\epsilon, \delta)\)-bounds for ADP. This thesis uses two of them: 
Rényi-DP (RDP)~\cite{Mironov17} and 
zero-concentrated differential privacy (zCDP)~\cite{BuS16}. Both are based 
on Rényi divergence~\cite{Mironov17}, which is a particular way of 
measuring the difference between random variables.

\begin{definition}
    For random variables with density or probability mass functions 
    \(P\) and \(Q\) the Rényi divergence of order 
    \(1 < \alpha < \infty\) is
    \[
        D_\alpha(P\dmid Q) = \frac{1}{\alpha - 1}\ln E_{x\sim Q}
        \left(\frac{P(x)^\alpha}{Q(y)^\alpha}\right).
    \]
    Orders \(\alpha = 1\) and \(\alpha = \infty\) are defined 
    by continuity:
    \[
        D_1(P\dmid Q) = \lim_{\alpha \to 1-} D_\alpha(P\dmid Q),
    \]
    \[
        D_\infty(P \dmid Q) = \lim_{\alpha\to \infty}D_\alpha(P\dmid Q).
    \]
\end{definition}

Both Rényi-DP and zCDP can be expressed as bounds on the 
Rényi divergence between the outputs of an algorithm with 
neighbouring inputs:

\begin{definition}
    An algorithm \(\calm\) is \((\alpha, \epsilon)\)-Rényi DP 
    if for all \(x \sim x'\)
    \[
        D_\alpha(\calm(x)\dmid \calm(x')) \leq \epsilon.
    \]
    \(\calm\) is \(\rho\)-zCDP if for all \(\alpha > 1\)
    and all \(x \sim x'\)
    \[
        D_\alpha(\calm(x)\dmid \calm(x')) \leq \rho \alpha.
    \]

\end{definition}

Rényi-DP and zCDP bounds can be converted to ADP bounds~\cite{Mironov17, BuS16}:
\begin{theorem}\label{other_dp_to_adp}
    If \(\calm\) is \((\alpha, \epsilon)\)-RDP, \(\calm\) is also 
    \((\epsilon - \frac{\ln \delta}{\alpha - 1}, \delta)\)-ADP for any 
    \(0 < \delta < 1\). If \(\calm\) is \(\rho\)-zCDP, \(\calm\) is also 
    \((\rho + \sqrt{-4\rho\ln \delta}, \delta)\)-ADP for any \(0 < \delta < 1\).
\end{theorem}

A very useful property of all of these definitions is composition~\cite{DwR14}: 
if algorithms \(\calm\) and \(\calm'\) are DP, the algorithm first computing 
\(\calm\) and then \(\calm'\), outputting both results, 
is also DP, although with worse bounds.
More precisely

\begin{definition}
    Let \(\calm\colon \calx \to \calu\) and 
    \(\calm'\colon \calx\times \calu \to \calu'\) be algorithms.
    Their composition is the algorithm outputting 
    \((\calm(x), \calm'(x, \calm(x)))\) for input \(x\).
\end{definition}

\begin{theorem}\label{composition-theorem}
    Let \(\calm\colon \calx \to \calu\) and 
    \(\calm\colon \calx\times \calu \to \calu'\) be algorithms. Then 
    \begin{enumerate}
        \item 
            If \(\calm\) is \((\epsilon, \delta)\)-ADP and 
            \(\calm'\) is \((\epsilon', \delta')\)-ADP, then 
            their composition is 
            \((\epsilon + \epsilon', \delta + \delta')\)-ADP~\cite{DwR14}
        \item 
            If \(\calm\) is \((\alpha, \epsilon)\)-RDP and 
            \(\calm'\) is \((\alpha, \epsilon')\)-RDP, then 
            their composition is \((\alpha, \epsilon + \epsilon')\)-RDP~\cite{Mironov17}
        \item 
            If \(\calm\) is \(\rho\)-zCDP and 
            \(\calm'\) is \(\rho'\)-zCDP, then 
            their composition is \((\rho + \rho')\)-zCDP~\cite{BuS16}
    \end{enumerate}
\end{theorem}

All of the composition results can be extended to any number of compositions 
by induction. Note that any step of the composition can depend on the results 
of the previous steps, not only on the private data. There are also other composition
theorems for ADP that trade increased \(\delta\) for decreased \(\epsilon\)
or vice-versa, but this thesis does not apply them directly.

As any algorithm that does not use private data in any way is 
\((0, 0)\)-ADP, 0-zCDP and \((\alpha, 0)\)-RDP with all \(\alpha\), 
Theorem~\ref{composition-theorem} has the following corollary, called 
post-processing immunity:

\begin{theorem}
    Let \(\calm\colon \calx\to \calu\) be an ADP, RDP or zCDP algorithm with 
    some privacy parameters. Let \(f\colon \calu\to \calu'\) be any algorithm 
    not using the private data. Then the composition of \(\calm\) and \(f\)
    is ADP, RDP or zCDP with the same privacy parameters.
\end{theorem}

There are many different DP algorithms that are commonly used, which are also
called mechanisms~\cite{DwR14}. This thesis only requires one of the most commonly 
used ones: the Gaussian mechanism~\cite{DwR14}.
\begin{definition}
    The Gaussian mechanism with parameter \(\sigma^2\) 
    is an algorithm that, with input \(x\), 
    outputs a sample from \(\caln(x, \sigma^2)\), where \(\caln\) denotes 
    the normal distribution.
\end{definition}

The RDP and zCDP bounds for the Gaussian 
mechanism are quite simple. The ADP bound is more complicated:

\begin{theorem}\label{gauss-DP-bounds}
    If for all inputs \(x\) and \(x'\), \(||x - x'||_2 \leq \Delta\),
    the Gaussian mechanism is 
    \begin{enumerate}
        \item 
            \((\alpha, \frac{\alpha \Delta^2}{2\sigma^2})\)-RDP~\cite{Mironov17}
        \item 
            \(\frac{\Delta^2}{2\sigma^2}\)-zCDP~\cite{BuS16}
        \item 
            \(n\) compositions of the Gaussian mechanism are 
            \((\epsilon, \delta(\epsilon))\)-ADP~\cite{Sommer2019} with 
            \[
                \delta(\epsilon) 
                = \frac{1}{2}\left(
                    \erfc\left(\frac{\sigma(\epsilon - n\mu)}{\sqrt{2n}\Delta}\right)
                    - e^\epsilon \erfc\left(\frac{\sigma(\epsilon + n\mu)}{\sqrt{2n}\Delta}\right)
                \right),
            \]
            where \(\mu = \frac{\Delta^2}{2\sigma^2}\) and \(\erfc\) is 
            the complementary error function.
    \end{enumerate}
\end{theorem}

The most common use case for the Gaussian mechanism is computing a 
function \(f\colon \calx \to \R\) of private data and feeding the result into 
the Gaussian mechanism to privately release the function value. 
The condition that the inputs 
of the Gaussian mechanism cannot vary too much leads into the concept of 
sensitivity of a function
\begin{definition}
    The \(l_p\)-sensitivity \(\Delta_p\), with neighbourhood relation \(\sim\),
    of a function \(f\colon \calx \to \R^n\)
    is 
    \[
        \Delta_p f = \sup_{x\sim x'}||f(x) - f(x')||_p.
    \]
\end{definition}

Theorem \ref{gauss-DP-bounds} implies that the value of any function with 
finite \(l_2\)-sensitivity can be privately released using the Gaussian mechanism 
with appropriate noise variance \(\sigma^2\). Of course, the usefulness of the 
released value depends on the magnitude of \(\sigma^2\) compared to the actual 
value.

\section{Bayesian Inference and Markov Chain Monte Carlo}\label{MCMC_background}

In Bayesian inference, the parameters of a statistical model are inferred from 
observed data using Bayes' theorem~\cite{BDA}. The result is not just a point estimate 
of the parameters, but a probability distribution describing the likelihood 
of different values of the parameters.

Bayes' theorem relates the \emph{posterior} belief of the parameters 
\(p(\theta \mid D)\) to the \emph{prior} belief \(p(\theta)\) through the 
observed data \(D\) and the likelihood of the data \(p(D\mid \theta)\) as follows:
\[
    p(\theta \mid D) = \frac{p(D \mid \theta)p(\theta)}
    {\int p(D\mid \theta)p(\theta)d\theta}.
\]
It is theoretically possible to compute \(p(\theta \mid D)\) given any 
likelihood, prior and data, but the integral in the denominator is in many 
cases difficult to compute~\cite{BDA}. In such cases the posterior cannot be feasibly 
computed. However, many of the commonly used summary statistics of the posterior, 
such as the mean, variance and credible intervals, can be approximated from 
a sample of the posterior. \emph{Markov chain Monte Carlo} (MCMC) is a 
widely used algorithm to obtain such samples~\cite{BDA}.

MCMC algorithms sequentially sample values of \(\theta\)
with the goal of eventually having the chain of sampled values converge to 
a given distribution~\cite{BDA}. While this can be done in many ways, this thesis 
focuses on a particular MCMC algorithm: \emph{Metropolis-Hastings} (MH).

The Metropolis-Hastings algorithm samples from a distribution \(\pi\) of 
\(\theta_i\) by first picking a proposal \(\theta^*\) from a proposal 
distribution \(q(\theta_{i-1})\) at iteration \(i\)~\cite{BDA}.
A density ratio is calculated
\[
    r = \frac{\pi(\theta^*)}{\pi(\theta_{i-1})}
    \frac{q(\theta_{i-1}\mid \theta^*)}{q(\theta^*\mid \theta_{i-1})},
\]
and the proposal is accepted with probability \(\min\{1, r\}\). 
If the proposal is accepted, 
\(\theta_i = \theta^*\), otherwise \(\theta_i = \theta_{i-1}\).

It can be shown that, with a suitable proposal distribution, the chain of 
\(\theta_i\) values converges to \(\pi\)~\cite{BDA}. The Gaussian distribution centered 
at the current value is a commonly used proposal.

When MCMC is used in Bayesian inference, the distribution to approximate is 
\[
    \pi(\theta) = p(\theta \mid D) = \frac{p(D \mid \theta)p(\theta)}
    {\int p(D\mid \theta)p(\theta)d\theta}.
\]
The difficult integral \(\int p(D\mid \theta)p(\theta)d\theta\) in the denominator 
cancels out when computing \(r\), so only the likelihood and the prior are needed. 
For numerical stability, \(r\) is usually computed in 
log space, which makes the acceptance probability \(\min\{1, e^\lambda\}\)
where 
\[
    \lambda = \ln \frac{p(D\mid \theta^*)}{p(D\mid \theta_{i-1})}
    + \ln \frac{p(\theta^*)}{p(\theta_{i-1})}
    + \ln \frac{q(\theta_{i-1}\mid \theta^*)}{q(\theta^*\mid \theta_{i-1})}.
\]

The dataset \(D\) is typically a table with \(n\) independent rows.
The likelihood is given as \(p(D_j\mid \theta)\)
for row \(D_j\). Independence of the rows means that 
\[
    p(D\mid \theta) = \prod_{j=1}^k p(D_j\mid \theta)
\]
which means that the log likelihood ratio term of \(\lambda\) is 
\[
    \ln \frac{p(D\mid \theta^*)}{p(D\mid \theta_{i-1})}
    = \sum_{j=1}^n \ln\frac{p(D_j\mid \theta^*)}{p(D_j\mid \theta_{i-1})}
\]
Algorithm~\ref{MH_algo} puts all of this together to summarise the MH 
algorithm used for Bayesian inference.

\begin{algorithm}[H]\label{MH_algo}
    \SetAlgoLined
    \For{\(1 \leq i \leq k\)}{
        sample \(\theta^* \sim q(\theta_{i-1})\)\\
        \(\ln p(D\mid \theta) = 
            \sum_{j=1}^n (\ln p(D_j \mid \theta^*) - \ln p(D_j\mid \theta_{i-1}))
        \)\\
    \(\lambda = \ln p(D\mid \theta)
        + \ln p(\theta^*) - \ln p(\theta_{i-1})
        + \ln q(\theta_{i-1}\mid \theta^*) - \ln q(\theta^*\mid \theta_{i-1})\)
        \\
        \(\theta_i = \begin{cases}
            \theta^* & \text{ with probability } \min\{1, e^\lambda\} \\
            \theta_{i-1} & \text{ otherwise}
        \end{cases}
        \)\\
    }
    \Return \((\theta_1, \dotsc, \theta_k)\)
    \caption{
        Metropolis-Hastings: number of iterations \(k\), proposal 
        distribution \(q\) and initial value \(\theta_0\) and 
        dataset \(D\) as input
    }
\end{algorithm}

\section{The Banana Distribution}

The banana distribution~\cite{TPK14} is a banana-shaped probability 
distribution that is a challenging target for MCMC algorithms. For this reason it has 
been used to test MCMC algorithms in the literature~\cite{TPK14}. 

\begin{definition}
    Let \(X\) have a bivariate Gaussian distribution with
    mean \(\mu\) and covariance matrix \(\Sigma\). Let
    \[
        g(x) = (x_1, x_2 - a(x_1 - m)^2 - b)
    \]
    with \(a, b, m \in \R\).
    The banana distribution with parameters \(\mu, \Sigma, a, b\) and \(m\)
    is the distribution of \(g(X)\). It is denoted by 
    \(\ban(\mu, \Sigma, a, b, m)\).
\end{definition}

In the literature the banana distribution is simply defined as the target to 
sample from, and is not considered a posterior in a Bayesian inference 
problem~\cite{TPK14}. To test differentially private MCMC algorithms, the 
target distribution must be the posterior of some inference problem, as 
otherwise there is no data to protect with differential privacy.
Theorem~\ref{banana_posterior_theorem} gives a suitable inference problem 
for testing DP MCMC algorithms.

\begin{theorem}\label{banana_posterior_theorem}
    Let
    \begin{align*}
        \theta = (\theta_1, \theta_2) &\sim
        \ban(0, \diag(\sigma_0^2, \sigma_0^2), a, b, m) \\
        X_1 &\sim \caln(\theta_1, \sigma_1^2) \\
        X_2 &\sim \caln(\theta_2 + a(\theta_1 - m)^2 + b, \sigma_2^2).
    \end{align*}
    Given data \(x_1, x_2\in \R^n\) and
    denoting \(\tau_i = \frac{1}{\sigma_i^2}\),
    the posterior of \(\theta\) tempered with \(T\) is the banana distribution
    \(\ban(\mu, \Sigma, a, b, m)\)
    with
    \[
        \mu = \left(\frac{Tn\tau_1\bar{x}_1}{Tn\tau_1 + \tau_0},
        \frac{Tn\tau_2\bar{x}_2}{Tn\tau_2 + \tau_0}\right),
    \]
    \[
        \Sigma = \diag\left(
            \frac{1}{Tn\tau_1 + \tau_0},
            \frac{1}{Tn\tau_2 + \tau_0}
        \right).
    \]
\end{theorem}
\begin{proof}
    Because
    \[
    g^{-1}(y) = (y_1, y_2 + a(y_1 - m)^2 + b)
    \]
    and the Jacobian determinant of \(g^{-1}\) is \(1\),
    for a positive-definite \(\Sigma\) the banana distribution has
    density
    \[
    \frac{1}{2\pi\sqrt{\det(\Sigma)}}\exp
    \left(-\frac{1}{2}(g^{-1}(x) - \mu)^T\Sigma^{-1}(g^{-1}(x) - \mu)\right)
    \]
    With \(\Sigma = \diag(\sigma_1^2, \sigma_2^2)\) the density is
    \[
    % f(x_1, x_2) =
    \frac{1}{2\pi\sigma_1\sigma_2}\exp
    \left(-\frac{1}{2}\left(\left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2
    + \left(\frac{x_2 + a(x_1 - m)^2 + b - \mu_2}{\sigma_2}\right)^2\right)\right)
    \]

    Denote \(u = \theta_2 + a(\theta_1 - m)^2 + b\).
    The tempered posterior of \(\theta\) is
    \begin{align*}
        p(\theta\mid X) &\propto p(X\mid \theta)^Tp(\theta)
        \\&= p(X_1\mid \theta_1)^Tp(X_2\mid \theta_1, \theta_2)^Tp(\theta)
        \\&\propto \left(\prod_{i=1}^n \exp
        \left(-\frac{(x_{i1} - \theta_1)^2\tau_1}{2}\right)\right)^T
        \cdot\left(\prod_{i=1}^n \exp\left(-\frac{(x_{i2} - \theta_2
        - a(\theta_1 - m)^2 - b)^2\tau_2}{2}\right)\right)^T
        \\&\cdot \exp\left(-\frac{1}{2}\left(\tau_0\theta_1^2
        + \tau_0(\theta_2 + a(\theta_1 - m)^2 + b)^2\right)\right)
        \\&=\exp\Bigg(-\frac{1}{2}\Big(T\tau_1\sum_{i=1}^n
        (x_{i1} - \theta_1)^2
        + T\tau_2\sum_{i=1}^n(x_{i2} - u)^2
        + \tau_0\theta_1^2 + \tau_0 u^2\Big)\Bigg)
        \\&=\exp\Bigg(-\frac{1}{2}\Big(T\tau_1\sum_{i=1}^n
        (x_{i1} - \bar{x}_1)^2 + T\tau_1n(\bar{x}_1 - \theta_1)^2
        \\&+ T\tau_2\sum_{i=1}^n (x_{i2}  - \bar{x}_2)^2 + T\tau_2n(\bar{x}_2 - u)^2
        + \tau_0\theta_1^2 + \tau_0 u^2\Big)\Bigg)
        \\&\propto\exp\Bigg(-\frac{1}{2}\Big(T\tau_1n(\bar{x}_1 - \theta_1)^2
        + T\tau_2n(\bar{x}_2 - u)^2
        + \tau_0\theta_1^2 + \tau_0 u^2\Big)\Bigg)
        \\&=\exp\Bigg(-\frac{1}{2}\Big(T\tau_1n\bar{x}_1^2
        - 2T\tau_1n\bar{x}_1\theta_1 + nT\tau_1\theta_1^2 + \tau_0\theta_1^2
        \\&+ T\tau_2n\bar{x}_2^2 - 2T\tau_2n\bar{x}_2u + nT\tau_2u^2
        + \tau_0 u^2\Big)\Bigg)
        \\&\propto\exp\Bigg(-\frac{1}{2}\Big((Tn\tau_1 + \tau_0)\theta_1^2
        - 2T\tau_1n\bar{x}_1\theta_1
        + (Tn\tau_2 + \tau_0)u^2 - 2T\tau_2n\bar{x}_2u \Big)\Bigg)
        \\&=\exp\Bigg(-\frac{1}{2}\Big(
        (Tn\tau_1 + \tau_0)\left(\theta_1^2
        - \frac{2T\tau_1n\bar{x}_1\theta_1}{Tn\tau_1 + \tau_0} \right)
        + (Tn\tau_2 + \tau_0)\left(u^2 - \frac{2T\tau_2n\bar{x}_2u}
        {Tn\tau_2 + \tau_0}\right) \Big)\Bigg)
        \\&\propto\exp\Bigg(-\frac{1}{2}\Big(
        (Tn\tau_1 + \tau_0)\left(\theta_1
        - \frac{T\tau_1n\bar{x}_1}{Tn\tau_1 + \tau_0} \right)^2
        + (Tn\tau_2 + \tau_0)\left(u - \frac{T\tau_2n\bar{x}_2}
        {Tn\tau_2 + \tau_0}\right)^2 \Big)\Bigg)
    \end{align*}

    As \(p(\theta\mid X)\) is proportional to the density of a
    banana distribution, the posterior is the banana distribution
    \(\ban(\mu, \Sigma, a, b, m)\)
    with
    \[
        \mu = \left(\frac{Tn\tau_1\bar{x}_1}{Tn\tau_1 + \tau_0},
        \frac{Tn\tau_2\bar{x}_2}{Tn\tau_2 + \tau_0}\right),
    \]
    \[
        \Sigma = \diag\left(
            \frac{1}{Tn\tau_1 + \tau_0},
            \frac{1}{Tn\tau_2 + \tau_0}
        \right).
    \]
    % with parameters \(\frac{Tn\tau_1\bar{x}_1}{Tn\tau_1 + \tau_0}\),
    % \(\frac{Tn\tau_2\bar{x}_2}{Tn\tau_2 + \tau_0}\), \(Tn\tau_1 + \tau_0\),
    % \(Tn\tau_2 + \tau_0\), \(a\) and \(b\).
\end{proof}

\chapter{Differentially Private MCMC}

As seen in Section~\ref{DP_background}, an algorithm can be made differentially 
private by adding Gaussian noise the its output. The noise could also be added 
to any intermediate value calculated by the algorithm, and post processing immunity 
will guarantee that the same DP bounds that hold for releasing the intermediate 
value also hold for releasing the final result of the algorithm.

In 2019, Yildirim and Ermis~\cite{YildirimE19} realised that if Gaussian noise is added to 
the exact value of \(\lambda\), the noise can be corrected for 
yielding a differentially private MCMC algorithm which converges to 
the correct distribution. In the same year, Heikkilä et.\ al.~\cite{HeikkilaJDH19}
developed another DP MCMC algorithm, called DP Barker, which uses subsampling 
to amplify privacy.

\section{DP Penalty}

In 1999, Ceperley and Dewing~\cite{CeD99} developed a variant of 
Metropolis-Hastings called the penalty 
algorithm, where only a noisy approximation of \(\lambda\) is known. They 
developed the algorithm for simulations in physics where computing \(\lambda\)
requires computing energies of complex systems, which can only be approximated.
The penalty algorithm modifies the acceptance probability to account for the 
noise added to \(\lambda\) and still converges to the correct distribution if 
the noise is Gaussian with known variance.

The DP penalty algorithm adds Gaussian noise to the value of \(\lambda\), and 
uses the penalty algorithm to correct the acceptance probability so that 
the algorithm still converges to the correct distribution~\cite{YildirimE19}.
The corrected acceptance probability for Gaussian noise with variance 
\(\sigma^2\) is 
\[
    \min\{1, e^{\lambda - \frac{1}{2}\sigma^2}\}
\]

Theorem~\ref{DP_penalty_theorem_zcdp} gives the number of iterations DP penalty 
can be run for when the privacy cost is computed through zCDP, which is 
what Yildirim and Ermis prove in their paper~\cite{YildirimE19}. A tighter, but 
harder to use, bound can be reached without using zCDP. This is given by 
Theorem~\ref{DP_penalty_theorem_adp}.

\begin{theorem}\label{DP_penalty_theorem_zcdp}
    Let \(\epsilon > 0\), \(0 < \delta < 1\), \(\alpha > 0\) and \(\tau > 0\).
    Let
    % \[
    %     \beta = -\frac{\ln \delta}{\ln n}
    % \]
    \[
        \rho = (\sqrt{\epsilon - \ln \delta} - \sqrt{-\ln \delta})^2
    \]
    \[
        c(\theta, \theta') = \sup_{D_j, D'_j} (p(D_j\mid \theta') - p(D_j\mid \theta) 
        - (p(D'_j\mid \theta') - p(D'_j\mid \theta)))
    \]
    \[
        \sigma^2(\theta, \theta') = \tau^2 n^{2\alpha}c^2(\theta, \theta')
    \]
    Then DP penalty can be run for 
    \[
        k = \lfloor 2\tau^2 n^{2\alpha} \rho\rfloor
    \]
    iterations when using \(\sigma^2\) as the variance of the Gaussian noise.
\end{theorem}

\begin{theorem}\label{DP_penalty_theorem_adp}
    Let \(\epsilon > 0\) and \(\tau > 0\).
    Define \(c\) and \(\sigma\) as in Theorem~\ref{DP_penalty_theorem_zcdp}.
    The DP penalty algorithm, after running for \(k\) iterations using \(\sigma\)
    as the noise variance, 
    is \((\epsilon, \delta(\epsilon))\)-DP for
    \[
        \delta(\epsilon) 
        = \frac{1}{2}\left(
            \erfc\left(\frac{\tau n^\alpha(\epsilon - k\mu)}{\sqrt{2k}}\right)
            - e^\epsilon \erfc\left(\frac{\tau n^\alpha(\epsilon + k\mu)}{\sqrt{2k}}\right)
        \right)
    \]
    where \(\mu = \frac{1}{\tau^2 n^{2\alpha}}\).
\end{theorem}
\begin{proof}
    DP penalty is an adaptive composition of Gaussian mechanisms that release 
    noisy values of \(\lambda(\theta, \theta')\). The sensitivity of 
    \(\lambda(\theta, \theta')\) is 
    \(c(\theta, \theta')\). For the tight ADP bound used here, the sensitivity must be 
    constant in each iteration. This is achieved by releasing 
    \(\frac{\lambda(\theta, \theta')}{c(\theta, \theta')}\) instead, which 
    has sensitivity 1. \(c(\theta, \theta')\) does not depend on \(D\), 
    so \(\lambda(\theta, \theta')\) can be obtained from 
    \(\frac{\lambda(\theta, \theta')}{c(\theta, \theta')}\) by post processing.

    Adding Gaussian noise with variance \(\sigma_n^2\) to 
    \(\frac{\lambda(\theta, \theta')}{c(\theta, \theta')}\)
    is equivalent to adding Gaussian noise with variance 
    \(\sigma_n^2 c^2(\theta, \theta')\) to \(\lambda(\theta, \theta')\).
    Setting \(\sigma_n^2 = \tau^2n^{2\alpha}\) and plugging into 
    the ADP bound of Theorem~\ref{gauss-DP-bounds} proves the 
    claim.
\end{proof}

Theorem~\ref{DP_penalty_theorem_adp} is harder to use than 
Theorem~\ref{DP_penalty_theorem_zcdp} because the number of iteration 
DP penalty can be run for given an \((\epsilon, \delta)\)-bound cannot be 
computed analytically for the former. However, the maximum number of iterations 
can be solved for numerically.

Theorems~\ref{DP_penalty_theorem_zcdp} and~\ref{DP_penalty_theorem_adp}
require a bound on sensitivity of the log likelihood ratio. If there is a bound 
\[
    |\ln p(D_j\mid \theta') - \ln p(D_j\mid \theta)| \leq L||\theta - \theta'||_2
\]
for all \(D_j, \theta\) and \(\theta'\) then 
\[
    c(\theta, \theta') \leq 2L||\theta - \theta'||_2
\]
The former bound is true in some model, such as logistic regression. In other 
models it can be forced by clipping the log likelihood ratios to the interval 
\([-L||\theta - \theta'||_2, L||\theta - \theta'||_2]\). This will remove the 
guarantee of eventually converging to the correct posterior, but if \(L\) is 
chosen to be large enough, the clipping will not affect the 
acceptance decision frequently. As a tradeoff, picking a large \(L\) will increase 
the variance of the Gaussian noise and slow down convergence through it.


\section{DP Barker}

The DP Barker algorithm of Heikkilä et.\ al.~\cite{HeikkilaJDH19} is based on 
the Barker acceptance test~\cite{Barker65} instead of the Metropolis-Hastings test.
Instead of using the MH acceptance probability, the Barker acceptance test samples 
\(V_{log}\sim \mathrm{Logistic(0, 1)}\) and accepts if 
\[
    \lambda + V_{log} > 0
\]

If Gaussian noise with variance \(\sigma^2\) is added to 
\(\lambda\), there exists a correction 
distribution \(V_{corr}\) such that \(\caln(0, \sigma^2) + V_{corr}\) has the 
same distribution as \(V_{log}\). Because the variance of \(V_{log}\) is 
\(\frac{\pi^2}{3}\)~\cite{HeikkilaJDH19}, the variance of \(V_{corr}\) must be 
\(\frac{\pi^2}{3} - \sigma^2\) which means that there is an upper bound
to the noise variance: \(\sigma^2 < \frac{\pi^2}{3}\). Testing whether 
\(\lambda + \caln(0, \sigma^2) + V_{corr} > 0\) is equivalent to testing 
whether \(\lambda + V_{log} > 0\), which means that it is possible to derive 
a DP MCMC algorithm based on the Barker acceptance test if the correction 
distribution can be sampled from.

However, the analytical form of \(V_{corr}\) is not known~\cite{HeikkilaJDH19}.
Heikkilä et.\ al.\  approximate the distribution with a Gaussian mixture model. 
This means that their 
algorithm only converges to an approximately correct distribution, but the 
approximation error can be made very small.

If the sum in \(\lambda\) was only computed over a subset of the data, the 
algorithm would take less computation to run, and would be less sensitive 
to changes in the data. The latter property is called \emph{subsampling amplification}
of differential privacy~\cite{WangBK19}. Using the \(\lambda\) computed 
with subsampling instead of the full data \(\lambda\) introduces an additional 
error that must be corrected for to have the algorithm converge to the correct 
distribution. 

The \emph{central limit theorem} (CLT) states that the distribution of a sum 
of random variables approaches a Gaussian distribution as more random variables 
are summed, if some conditions on the independence and variance of the random 
variables are met~\cite{HeikkilaJDH19}. With the CLT, it can be argued 
that the error from 
using the subsampled \(\lambda\) instead of the full data \(\lambda\) has an 
approximately Gaussian distribution, if the subsample is large 
enough~\cite{HeikkilaJDH19}. 

The variance of the error from subsampling can 
be estimated by the sample variance of the individual terms in the sum in 
\(\lambda\)~\cite{HeikkilaJDH19}. This allows combining the errors from subsampling and the 
Gaussian noise from the Gaussian mechanism to a single Gaussian noise value.
The \(V_{corr}\) distribution can then be used to approximate the Barker acceptance 
test as above. See algorithm~\ref{DP_barker_algo} for the DP Barker algorithm.
\footnote{
    See~\cite{HeikkilaJDH19} for the sampling procedure of \(V_{corr}\).
}

Heikkilä et.\ al.~\cite{HeikkilaJDH19} do not directly bound the sensitivity 
of \(\lambda\) as is done in DP penalty, because the sample variance also 
depends on input data. Instead they directly bound the Rényi divergence 
between \(\caln(0, \sigma^2 - \sigma^2_b)\), where \(\sigma^2_b\) is the 
batch sample variance, for two adjacent inputs. Subsampling amplification 
is accounted for with an amplification theorem for Rényi DP~\cite{WangBK19}.

\begin{theorem}
    If 
    \[
        |\ln p(D_j\mid \theta') - \ln p(D_j\mid \theta)| \leq \frac{\sqrt{|B|}}{n}
    \]
    \[
        \alpha < \frac{|B|}{5}, \alpha \in \N
    \]
    for all \(\theta, \theta' \in \Theta\), all \(D\) and \(1\leq j \leq n\),
    running \(k\) iterations of DP Barker is \((\alpha, k\epsilon(\alpha))\)-RDP, 
    with 
    \[
        \epsilon(\alpha) = \frac{1}{\alpha - 1}\ln \left(
        1 + q^2\binom{\alpha}{2}\min\{4(e^{\epsilon'(2)} - 1), 2e^{\epsilon'(2)}\}
        + 2 \sum_{j=3}^\alpha q^j\binom{\alpha}{j}e^{j-1\epsilon'(j)}\right)
    \]
    and 
    \[
        \epsilon'(\alpha) = \frac{5}{2|B|} + \frac{1}{2(\alpha - 1)}
        \ln \frac{2|B|}{|B| - 5\alpha} + \frac{2\alpha}{|B| - 5\alpha}
    \]
    where \(n\) is the number of rows in \(D\), \(|B|\) is the size of the 
    minibatch and \(q = \frac{|B|}{n}\).
\end{theorem}

Like DP penalty, DP Barker requires a bound on the log likelihood ratio for 
one row of data. The bound can be forced through clipping if the model does not 
meet it, but because of the \(n\) in the denominator of the bound, it can get 
very tight for large values of \(n\). As a result, clipping may be needed for 
almost all log likelihood ratios, which may cause the algorithm to converge 
to a very different distribution from the posterior.

To alleviate the tight bound on log likelihood sensitivity, DP Barker is best 
used with a tempered likelihood~\cite{HeikkilaJDH19}. In tempering, the 
log likelihood is multiplied by a number \(T = \frac{n_0}{n} < 1\). This 
increases the variance of the resulting posterior and may lower modeling 
error in some cases~\cite{HeikkilaJDH19}.

Using the tempered likelihood, the log likelihood 
bound becomes 
\[
    T|\ln p(D_j\mid \theta') - \ln p(D_j\mid \theta)|
    \leq \frac{\sqrt{|B|}}{n}
\]
which is equivalent to 
\[
    |\ln p(D_j\mid \theta') - \ln p(D_j\mid \theta)|
    \leq \frac{\sqrt{|B|}}{n_0}
\]
Typically \(n_0 \ll n\) for large datasets, so using a tempered likelihood requires 
significantly less clipping than a nontempered likelihood.

\begin{algorithm}[H]\label{DP_barker_algo}
    \SetAlgoLined
    sample \(\theta^* \sim q(\theta_{i-1})\)\\
    sample \(B \subset \{1, \dotsc, n\}\)\\
    \For{\(1 \leq i \leq k\)}{
        \For{\(j \in B\)}{
            \(l_j = \ln \frac{p(\theta^*\mid D_j)}{p(\theta_{i-1}\mid D_j)}\)\\
        }
        \(\sigma^2_b = \mathrm{Var}\{l_j\mid j\in B\}\)\\
        \(\lambda = 
        \frac{n}{|B|}\sum_{j\in B} l_j
        + \ln \frac{p(\theta^*)}{p(\theta_{i-1})}
        + \ln \frac{q(\theta_{i-1}\mid \theta^*)}{q(\theta^*\mid \theta_{i-1})}\)
        \\
        sample \(s \sim \caln(0, \sigma^2 - \sigma^2_b)\)\\
        sample \(c \sim V_{corr}^{\sigma^2}\)\\
        \(\theta_i = \begin{cases}
            \theta^* & \text{ if } \lambda + s + c > 0 \\
            \theta_{i-1} & \text{ otherwise}
        \end{cases}
        \)\\
    }
    \Return \((\theta_1, \dotsc, \theta_k)\)
    \caption{
        DP Barker
    }
\end{algorithm}

\section{Comparing DP Penalty and DP Barker}

\chapter{Variations of the Penalty Algorithm}

\section{The Penalty Algorithm with Subsampling}

\section{DP Metropolis-Adjusted Langevin Algorithm}

\chapter{The Gauss-Bernoulli Algorithm}

\chapter{Experiments}

\section{Banana Distribution}

\chapter{Conclusions}
% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\cleardoublepage %fixes the position of bibliography in bookmarks
\phantomsection

\addcontentsline{toc}{chapter}{\bibname} % This lines adds the bibliography to the ToC
\bibliographystyle{alpha} % numbering alphabetic order
\bibliography{../references.bib}

% \begin{appendices}
% \myappendixtitle
%
% \chapter{Code example\label{appendix:code}}
% Program code can be added as appendix:
% \begin{verbatim}
% #!/bin/bash          
% text="Hello World!"
% echo $text
% \end{verbatim}
%
% \end{appendices}

\end{document}
