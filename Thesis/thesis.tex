%% This file is modified by Veli Mäkinen from HY_fysiikka_LuKtemplate.tex authored by Roope Halonen ja Tomi Vainio.
%% Some text is also inherited from engl_malli.tex by Kutvonen, Erkiö, Mäkelä, Verkamo, Kurhila, and Nykänen.


% STEP 1: Choose oneside or twoside
\documentclass[english,twoside,openright]{HYgraduMLDS}
%finnish,swedish

\usepackage{lmodern} % Font package
\usepackage{textcomp} % Package for special symbols
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
\usepackage{amsthm}
%\usepackage[square]{natbib} % For bibliography
\usepackage[footnotesize,bf]{caption} % For more control over figure captions
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage{booktabs}
\usepackage[titletoc]{appendix}
\usepackage[ruled, vlined]{algorithm2e}

\onehalfspacing %line spacing
%\singlespacing
%\doublespacing

%\fussy 
\sloppy % sloppy and fussy commands can be used to avoid overlong text lines

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{Differentially Private Metropolis-Hastings Algorithms}
\author{Ossi Räisä}
\date{\today}
\prof{Associate Professor Antti Honkela}
\censors{Associate Professor Antti Honkela}{Doctor Antti Koskela}{}
\keywords{Differential privacy, Markov chain Monte Carlo, Hamiltonian Monte Carlo}
\depositeplace{}
\additionalinformation{}


\classification{\protect{\ \\
    Mathematics of computing\(\rightarrow\)Probability and statistics\(\rightarrow\)Probabilistic reasoning
    algorithms\\\(\rightarrow\)Markov-chain Monte Carlo methods\(\rightarrow\)Metropolis-Hastings algorithm  \\
    Security and privacy\(\rightarrow\)Security Services\(\rightarrow\)Privacy-preserving protocols \\
}}

% if you want to quote someone special. You can comment this line and there will be nothing on the document.
%\quoting{Bachelor's degrees make pretty good placemats if you get them laminated.}{Jeph Jacques} 


% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
\hypersetup{
    % bookmarks=true,         % show bookmarks bar first?
    unicode=true,           % to show non-Latin characters in Acrobatâs bookmarks
    pdftoolbar=true,        % show Acrobatâs toolbar?
    pdfmenubar=true,        % show Acrobatâs menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=blue,        % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\cl}[1]{\overline{#1}}
\newcommand{\kl}{D_{\mathrm{KL}}}
\newcommand{\dmid}{\mid\mid}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\dx}{\mathrm{d}}
\newcommand{\calm}{{\mathcal{M}}}
\newcommand{\calx}{{\mathcal{X}}}
\newcommand{\calu}{{\mathcal{U}}}
\newcommand{\caln}{{\mathcal{N}}}
\newcommand{\call}{{\mathcal{L}}}
\newcommand{\caly}{{\mathcal{Y}}}
\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\ban}{Ban}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\clip}{clip}

\begin{document}

% Generate title page.
\maketitle

% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.

\begin{abstract}
  Differential privacy has over the past decade become a widely used framework
  for privacy-preserving machine learning. At the same time,
  Markov chain Monte Carlo (MCMC) algorithms, particularly Metropolis-Hastings (MH)
  algorithms, have become an increasingly popular method
  of performing Bayesian inference. Surprisingly, their combination has not
  received much attention in the literature. This thesis introduces the
  existing research on differentially private MH algorithms, proves tighter
  privacy bounds for them using recent developments in differential privacy, and
  develops two new differentially private MH algorithms:
  an algorithm using subsampling to lower privacy costs, and
  a differentially private variant of the Hamiltonian Monte
  Carlo algorithm. The privacy bounds of both new algorithms are proved, and
  convergence to the exact posterior is proven for the latter.
  The performance of both the old and the new algorithms is compared on several
  Bayesian inference problems, revealing that none of the algorithms is
  clearly better than the others, but subsampling is likely only useful to lower
  computational costs.
\end{abstract}

% Place ToC
\mytableofcontents

\mynomenclature

% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\chapter{Introduction}

As the availability of data on private individuals grows ever larger, both the
potential gains from analysing the data, and the risks associated with the
analysis, grow. This makes analysis techniques that can ensure the privacy
of the individuals important. Differential
privacy~\cite{DMN06} is a formal definition attempting to capture the notion
of a data analysis algorithm that preserves privacy.
Metropolis-Hastings (MH) algorithms~\cite{MRR53, Has70} are a particular class of
Markov chain Monte Carlo (MCMC)~\cite{Robert04} algorithms.
MCMC is a computational method that enables data analysis using
Bayesian inference~\cite{BDA}. This thesis studies the combination of
differential privacy and MH algorithms by introducing existing algorithms, proving
tighter privacy bounds for them, and developing two novel algorithms.
Finally, both the existing and the new algorithms are compared on a variety of
settings.

Traditional approaches in preserving the privacy of data subjects in data
analysis, such as simply omitting identifying information, or
more advanced techniques such as \(k\)-anonymity~\cite{SaS98, Sam01},
can lead to compromises of private information. These leaks can result from
the combination of an
adversary having access to additional data and linking parts of the additional
data and the private data, or the adversary having access to many
public results that use the same private dataset~\cite{DiN03}.

Differential privacy~\cite{DMN06} is a notion of an algorithm that preserves
privacy by making the data analysis algorithm noisy, thus masking the details of
the private input data that are necessary for identifying individuals in the data.
The idea of differential privacy is to require that small changes in the input
data can only cause small changes in the probability distribution of the
output. This makes differential privacy immune to any post-processing and
auxiliary information, and allows quantifying the loss in privacy from releasing
multiple results based on the same dataset, or for an individual, the privacy
loss from participating in multiple studies.

The tradeoff with differential privacy is the noise that must be added to the
analysis process, which will decrease the utility of the analysis. For publishing
aggregate summaries of large datasets, the amount of noise required is not too
large, and differential privacy has been used in such settings in practice by
Apple~\cite{App17}, Microsoft~\cite{DKY17}, the U.\ S.\ Census Bureau~\cite{Abo18},
and others. However, for small datasets, or applications requiring detailed
information on individuals, such as COVID-19 tracing apps, the amount of
noise is likely to be prohibitively large, so other techniques for maintaining
the privacy of individuals are required.

Bayesian inference~\cite{BDA} is a paradigm of statistical inference where
the parameters of a statistical model are considered random variables, and
the language of probability is used to describe uncertainty about the values of the
parameters. Bayesian inference is based on Bayes' theorem, which allows
a data analyst to update his prior knowledge of the parameters based on observed
data to obtain a posterior distribution of the parameters.
Computing the posterior distribution analytically is often not possible,
so numerical methods are needed. Markov chain Monte Carlo (MCMC) is a general
method of drawing samples from a given distribution, such as the
posterior distribution. Metropolis-Hastings algorithms are a particular class
of MCMC algorithms, where
the sampling is done by first picking a proposal from a given proposal distribution,
and then either choosing to accept or reject the proposal based on an acceptance
test. The sample allows
the analyst to gain information about the model parameters through the data.
MH algorithms only require knowing an unnormalised form of the target distribution,
which enables their use on a wide variety of models, as computing the unnormalised
posterior is often much easier than computing the normalised posterior.

Combining MH algorithms with differential privacy allows
an analyst to conduct the comprehensive Bayesian inference that is
possible using MH while preserving the privacy of the data subjects through
differential privacy. One differentially private MH algorithm has been
developed: the DP penalty algorithm of Yildirim and Ermis~\cite{YildirimE19}.
A closely related algorithm is the DP Barker algorithm of
Heikkilä et al.~\cite{HeikkilaJDH19}, which uses the Barker
acceptance~\cite{Barker65} test instead of the MH acceptance test
that MH algorithms use. Although DP Barker is not technically an MH algorithm,
it is much close to DP penalty than other DP MCMC algorithms due to the
acceptance test it performs.

DP penalty builds on the earlier penalty algorithm~\cite{CeD99}, which was
developed for physical simulations where it is difficult to compute even the
unnormalised form of the target distribution, so the target
must be approximated. DP Barker builds on an earlier algorithm that only
uses a part of its input data at a time to reduce computational
costs~\cite{SPC17}, which itself approximates the Barker algorithm~\cite{Barker65}
to correct for the error from not using the full data. In DP Barker, not using the
full data additionally amplifies the privacy of the algorithm,
allowing it to run longer for a given privacy budget.
Of these, the DP penalty algorithm is considerably simpler and more extensible,
so the further development of differentially private MCMC algorithms in this
thesis uses it as the basis.

In 2019, Sommer et al.~\cite{Sommer2019} developed a technique of
computing the privacy of
iterative algorithms that allows running the algorithms longer than the
privacy accounting methods used by Yildirim and Ermis~\cite{YildirimE19} and
Heikkilä et al.~\cite{HeikkilaJDH19}. This technique is applicable to the
DP penalty algorithm, but not the DP Barker algorithm, although new developments
of the technique~\cite{KJH20} may be applicable to it.

As an MH algorithm, DP penalty is very simple, so developing differentially
private variants of more advanced MH algorithms may be fruithful. This thesis
develops a differentially private version of
Hamiltonian Monte Carlo (HMC)~\cite{neal2012mcmc, DKP87}, which is able to make
better use of the structure of the statistical model being sampled than DP penalty.
However, with differential privacy, this comes with a privacy cost, so it is
not clear that differentially private HMC is more effective than DP penalty.

Another way to lower the privacy cost of a differentially private
algorithm is only using a portion of the data at each iteration, instead of
the full dataset. This technique, called subsampling or minibatching, is used
by DP Barker, but it is also applicable to DP penalty. As subsampling will
decrease the accuracy of the algorithm, it is not guaranteed to improve results
over existing algorithms.

There are other frameworks for DP Bayesian inference than DP MH, but none
of them offers the full generality and theoretical guarantees that DP MH
has. DP variational inference~\cite{JHD17} approximates the posterior
distribution with a tractable distribution, but the approximating distribution
may not be able to match the posterior. For exponential
family~\cite{BernsteinS18} and generalized linear models~\cite{KJK20},
differentially private inference is possible by perturbing sufficient statistics
or posterior parameterisations, but these sets of models are limited. Simply sampling
from the exact posterior is differentially private under suitable
conditions~\cite{DNZ17, WFS15, ZRD16}, but for many models, exact sampling of the posterior
is not possible.

Several non-MH DP MCMC algorithms~\cite{WFS15, LCL19} based on
stochastic gradient MCMC algorithms~\cite{WellingT11, CFG14, DFB14} have been developed.
First developed in the non-private setting, stochastic gradient MCMC
algorithms use the gradients of the target density
to draw efficient proposals. To save computation time, the gradients are
only computed over a subsample of the data, and the acceptance test is omitted,
which introduces bias to the resulting posterior~\cite{Bet15}, especially in
high dimensions.
The differentially private variants of these algorithms~\cite{WFS15, LCL19} greatly
benefit from privacy amplification from subsampling and and the lowered
privacy cost from omitting the acceptance test,
but they inherit the bias from omitting the acceptance test, so all of the
algorithm in this thesis include the acceptance test.

This thesis begins with a more detailed, but nevertheless brief, introduction to
differential privacy and MH in Chapter~\ref{background_chapter}.
Chapter~\ref{dp_mcmc_chapter} considers the simpler differentially private
MH algorithms: the existing algorithms, DP penalty and DP Barker, are introduced
in Sections~\ref{dp_penalty_section} and \ref{dp_barker_section},
the tighter privacy bound for DP penalty is developed in
Section~\ref{dp_penalty_adp_section}
and the minibatch DP penalty algorithm is developed in
Section~\ref{dp_minibatch_penalty_section}. Chapter~\ref{hmc_chapter} first
introduces HMC in Section~\ref{hmc_basics_section} and develops differentially
private HMC in Section~\ref{dp_hmc_section}. The performance of the algorithms
is compared on a wide variety of models. The models, and technical details
concerning the comparison, are discussed in Chapter~\ref{experiment_setup_chapter}.
The results of the experiments are presented in Chapter~\ref{experiment_chapter}.

\chapter{Background}\label{background_chapter}

This chapter covers the basics of differential privacy, Bayesian inference,
Markov chain Monte Carlo algorithms and measure theory
needed in the later chapters. To keep
the introduction brief, only directly needed concepts are covered, and much
of the motivation behind the subjects is left out.

\section{Differential Privacy}\label{DP_background}
\emph{Differential privacy} (DP)~\cite{DMN06, DwR14} is a property of
an algorithm that quantifies the
amount of information about private data an adversary can gain from the 
publication of the algorithm's output.
The most commonly used definition uses two real numbers, 
\(\epsilon\) and \(\delta\), to quantify the information gain, or, from the 
perspective of a data subject, the privacy loss of the algorithm.
DP algorithms must necessarily\footnote{Unless the algorithm does not
actually use the private data.} include randomness to mask influence of the
private data, so all of the considered algorithms in this thesis are randomised.
DP randomised algorithms are also called \emph{mechanisms} in this thesis and in the
DP literature.

The most common definition is called \((\epsilon, \delta)\)-approximate
differential privacy (ADP)~\cite{DKM06, DwR14}.
The case where \(\delta = 0\) is called \(\epsilon\)-DP or 
pure DP.

\begin{definition}\label{ADP-definition}
    A mechanism \(\calm\colon \calx \to \calu\) is \((\epsilon, \delta)\)-ADP if
    for all neighbouring inputs \(x\in \calx\) and \(x'\in \calx\) and 
    all measurable sets \(S \subset \calu\)
    \[
        P(\calm(x)\in S) \leq e^\epsilon P(\calm(x')\in S) + \delta.
    \]
\end{definition}

The neighbourhood relation in the definition is domain specific. With tabular 
data the most common definitions are the add/remove neighbourhood and 
substitute neighbourhood.
\begin{definition}
    Two tabular datasets are said to be add/remove neighbours if they are equal 
    after adding or removing at most one row to or from one of them. The datasets 
    are said to be in substitute neighbours if they are equal after 
    changing at most one row in one of them.
\end{definition}
The neighbourhood relation is denoted by \(\sim\). The definitions and 
theorems of this section are valid for all neighbourhood relations, but later
chapters use the substitute neighborhood relation.

There many other definitions of differential privacy that are mostly used
to compute \((\epsilon, \delta)\)-bounds for ADP. This thesis uses two of them: 
Rényi-DP (RDP)~\cite{Mironov17} and 
zero-concentrated differential privacy (zCDP)~\cite{BuS16}. Both are based 
on Rényi divergence~\cite{Mironov17}, which is a particular way of 
measuring the distance\footnote{
  Statistical divergences are commonly called distances, even though they
  typically are not metrics.
} between random variables.

\begin{definition}
    For random variables with density or probability mass functions 
    \(P\) and \(Q\), the Rényi divergence of order
    \(1 < \alpha < \infty\) is
    \[
        D_\alpha(P\dmid Q) = \frac{1}{\alpha - 1}\ln E_{x\sim Q}
        \left(\frac{P(x)^\alpha}{Q(x)^\alpha}\right).
    \]
    Orders \(\alpha = 1\) and \(\alpha = \infty\) are defined 
    by continuity:
    \[
        D_1(P\dmid Q) = \lim_{\alpha \to 1-} D_\alpha(P\dmid Q),
    \]
    \[
        D_\infty(P \dmid Q) = \lim_{\alpha\to \infty}D_\alpha(P\dmid Q).
    \]
\end{definition}

Both RDP and zCDP can be expressed as bounds on the
Rényi divergence between the outputs of a randomised algorithm with
neighbouring inputs:

\begin{definition}
    A mechanism \(\calm\) is \((\alpha, \epsilon)\)-RDP
    if for all \(x \sim x'\)
    \[
        D_\alpha(\calm(x)\dmid \calm(x')) \leq \epsilon.
    \]
    \(\calm\) is \(\rho\)-zCDP if for all \(\alpha > 1\)
    and all \(x \sim x'\)
    \[
        D_\alpha(\calm(x)\dmid \calm(x')) \leq \rho \alpha.
    \]

\end{definition}

Rényi-DP and zCDP bounds can be converted to ADP bounds~\cite{Mironov17, BuS16}:
\begin{theorem}\label{other_dp_to_adp}
    If a mechanism \(\calm\) is \((\alpha, \epsilon)\)-RDP, \(\calm\) is also
    \((\epsilon - \frac{\ln \delta}{\alpha - 1}, \delta)\)-ADP for any 
    \(0 < \delta < 1\). If \(\calm\) is \(\rho\)-zCDP, \(\calm\) is also 
    \((\rho + \sqrt{-4\rho\ln \delta}, \delta)\)-ADP for any \(0 < \delta < 1\).
\end{theorem}

A very useful property of all of these definitions is composition~\cite{DwR14}: 
if mechanisms \(\calm\) and \(\calm'\) are DP, the mechanism first computing
\(\calm\) and then \(\calm'\), outputting both results, 
is also DP, although with worse bounds.
More precisely:

\begin{definition}\label{composition_definition}
    Let \(\calm\colon \calx \to \calu\) and 
    \(\calm'\colon \calx\times \calu \to \calu'\) be mechanisms.
    Their composition is the mechanism outputting
    \((\calm(x), \calm'(x, \calm(x)))\) for input \(x\).
\end{definition}

\begin{theorem}\label{composition-theorem}
    Let \(\calm\colon \calx \to \calu\) and 
    \(\calm\colon \calx\times \calu \to \calu'\) be mechanisms. Then
    \begin{enumerate}
        \item 
            If \(\calm\) is \((\epsilon, \delta)\)-ADP and 
            \(\calm'\) is \((\epsilon', \delta')\)-ADP, then 
            their composition is 
            \((\epsilon + \epsilon', \delta + \delta')\)-ADP~\cite{DKM06}
        \item 
            If \(\calm\) is \((\alpha, \epsilon)\)-RDP and 
            \(\calm'\) is \((\alpha, \epsilon')\)-RDP, then 
            their composition is \((\alpha, \epsilon + \epsilon')\)-RDP~\cite{Mironov17}
        \item 
            If \(\calm\) is \(\rho\)-zCDP and 
            \(\calm'\) is \(\rho'\)-zCDP, then 
            their composition is \((\rho + \rho')\)-zCDP~\cite{BuS16}
    \end{enumerate}
\end{theorem}

All of the composition results can be extended to any number of compositions 
by induction. Note that any step of the composition can depend on the results 
of the previous steps, not only on the private data. There are also other composition
theorems for ADP that trade increased \(\delta\) for decreased \(\epsilon\)
or vice-versa, but this thesis does not apply them directly.

As any randomised algorithm that does not use private data in any way is
\((0, 0)\)-ADP, 0-zCDP and \((\alpha, 0)\)-RDP with all \(\alpha\), 
Theorem~\ref{composition-theorem} has the following corollary, called 
post-processing immunity:

\begin{theorem}
  Let \(\calm\colon \calx\to \calu\) be an \((\epsilon, \delta)\)-ADP,
  \((\alpha, \epsilon)\)-RDP or \(\rho\)-zCDP mechanism.
  Let \(f\colon \calu\to \calu'\) be any randomised algorithm
  not using the private data. Then the composition of \(\calm\) and \(f\)
  is \((\epsilon, \delta)\)-ADP, \((\alpha, \epsilon)\)-RDP or \(\rho\)-zCDP.
\end{theorem}

There are many different DP mechanisms that are commonly used~\cite{DwR14}.
This thesis only requires one of the most commonly 
used ones: the Gaussian mechanism~\cite{DKM06}.
\begin{definition}
  The Gaussian mechanism with parameter \(\sigma^2\) is a randomised algorithm that,
  with input data \(x\) and query \(f\colon \calx\to \R^{d}\), outputs a sample from
  \(\caln(f(x), \sigma^2)\), where \(\caln\) denotes the normal distribution.
\end{definition}

The privacy bounds of the Gaussian mechanism require that the values of the
query \(f\) do not vary too much for neighbouring inputs. This requirement
is formalised as a bound on the \emph{sensitivity} of \(f\).
\begin{definition}
    The \(l_p\)-sensitivity \(\Delta_p\), with neighbourhood relation \(\sim\),
    of a function \(f\colon \calx \to \R^d\)
    is
    \[
        \Delta_p f = \sup_{x\sim x'}||f(x) - f(x')||_p.
    \]
\end{definition}

The RDP and zCDP bounds for the Gaussian 
mechanism are quite simple. The ADP bound is more complicated:

\begin{theorem}\label{gauss-DP-bounds}
  If \(\Delta_{2}f \leq \Delta\), the Gaussian mechanism for query \(f\) with
  noise variance \(\sigma\) is
    \begin{enumerate}
        \item 
            \((\alpha, \frac{\alpha \Delta^2}{2\sigma^2})\)-RDP~\cite{Mironov17}
        \item 
            \(\frac{\Delta^2}{2\sigma^2}\)-zCDP~\cite{BuS16}
        \item 
            \(k\) compositions of the Gaussian mechanism, with
            queries \(f_{i}\), where \(\Delta_{2}f_{i}\leq \Delta\) for
            \(1\leq i \leq k\), are
            \((\epsilon, \delta(\epsilon))\)-ADP~\cite{Sommer2019} with 
            \[
                \delta(\epsilon) 
                = \frac{1}{2}\left(
                    \erfc\left(\frac{\sigma(\epsilon - k\mu)}{\sqrt{2k}\Delta}\right)
                    - e^\epsilon \erfc\left(\frac{\sigma(\epsilon + k\mu)}{\sqrt{2k}\Delta}\right)
                \right),
            \]
            where \(\mu = \frac{\Delta^2}{2\sigma^2}\) and \(\erfc\) is 
            the complementary error function.
    \end{enumerate}
    Additionally, each of the above privacy bounds is tight, meaning that
    the Gaussian mechanism with query \(f\) is not \((\alpha, \epsilon')\)-RDP
    for any \(\epsilon' < \frac{\alpha\Delta^{2}}{2\sigma^{2}}\), \(\rho'\)-zCDP
    for any \(\rho < \frac{\Delta^{2}}{2\sigma^{2}}\), and a composition
    of \(k\) Gaussian mechanisms with queries \(f_{i}\) is not
    \((\epsilon, \delta'(\epsilon))\)-ADP for any
    \(\delta'(\epsilon) < \delta(\epsilon)\).
\end{theorem}

Theorem~\ref{gauss-DP-bounds} implies that the value of any function with
finite \(l_2\)-sensitivity can be privately released using the Gaussian mechanism
with appropriate noise variance \(\sigma^2\). Of course, the utility of the
released value depends on the magnitude of \(\sigma^2\) compared to the actual
value. In the ADP bound of Theorem~\ref{gauss-DP-bounds},
as in Definition~\ref{composition_definition}, each function \(f_{i}\)
can depend on the output of the previous functions \(f_{j}\), \(j < i\).

Note that while the RDP and zCDP bounds of Theorem~\ref{gauss-DP-bounds} are
tight, the conversion from RDP or zCDP to ADP with Theorem~\ref{other_dp_to_adp}
is not tight. As a result, computing
ADP bounds for the Gaussian mechanism through RDP or zCDP, and converting
the resulting bounds to ADP with Theorem~\ref{other_dp_to_adp} does not
result in a tight bound. The difference of the two methods of computing ADP bounds
on the MCMC algorithms considered in this thesis is shown in
Section~\ref{accounting_comparison_section}.

When the dataset is a table, the query \(f\) of the Gaussian mechanism is
typically a sum the values of a function \(g\) for each row of the table,
specifically
\[
  f(X) = \sum_{x\in X} g(x)
\]
for dataset \(X\).
For the substitute neighborhood relation \(f\),
\[
  \Delta_{p}f = \sup_{X\sim X'}||f(X) - f(X')||_{p}
  = \sup_{x, x'}||g(x) - g(x')||_{p}
  = \Delta^{*}_{p}g,
\]
where \(x\) and \(x'\) are the values of the differing row in \(X\) and \(X'\),
and \(\Delta^{*}_{p}\) denotes sensitivity with the neighborhood relation that
considers all values neighboring.

In case \(\Delta^{*}_{p}g\) is not finite, \(g\) can still be used with
the Gaussian mechanism if \emph{clipping} is used. Clipping restricts the
values of \(g\) to a maximum \(l_{p}\)-norm. Specifically, clipping
\(g\) with the bound \(b\in \R\) applies
the function \(\clip_{b}^{p}\colon \R^{d}\to \R^{d}\) where
\[
  \clip_{b}^{p}(y) =
  \begin{cases}
    \frac{y}{||y||_{p}}\min\{||y||_{p}, b\} & \text{if } y \neq 0\\
    0 & \text{if } y = 0
  \end{cases}
\]
to the values of \(g\). Then \(\Delta^{*}_{p}(\clip_{b}^{p} \circ g) \leq 2b\) by the
triangle inequality, as \(||\clip_{b}^{p}(g(x))||_{p} \leq b\) for
all \(x\). As this thesis only uses the Gaussian mechanism and
thus only needs to clip the \(l_{2}\)-norm, the superscript \(p\) is always
\(2\) and the notation \(\clip_{b} = \clip_{b}^{2}\) is used.

\section{Bayesian Inference and the Metropolis-Hastings Algorithm}\label{MCMC_background}

In Bayesian inference, the parameters \(\theta\) of a statistical model are
inferred from 
observed data using Bayes' theorem~\cite{BDA}. The result is not just a point estimate 
of \(\theta\), but a probability distribution describing the likelihood
of different values of \(\theta\).

Bayes' theorem relates the \emph{posterior} belief of \(\theta\),
\(p(\theta \mid X)\) to the \emph{prior} belief \(p(\theta)\) through the 
observed data \(X\), and the likelihood of the data \(p(X\mid \theta)\) as follows:
\[
    p(\theta \mid X) = \frac{p(X \mid \theta)p(\theta)}
    {\int p(X\mid \theta)p(\theta)\dx\theta}.
\]
It is theoretically possible to compute \(p(\theta \mid X)\) given any 
likelihood, prior and data, but the integral in the denominator is in many 
cases difficult to compute~\cite{BDA}. In such cases the posterior cannot be feasibly 
computed. However, many of the commonly used summary statistics of the posterior, 
such as the mean, variance and credible intervals, can be approximated from 
a sample of the posterior. \emph{Markov chain Monte Carlo}
(MCMC) is a widely used method to obtain such samples.

MCMC algorithms sequentially sample values of \(\theta\)
with the goal of eventually having the chain of sampled values converge to 
a given distribution~\cite{Robert04}. While this can be done in many ways, this thesis
focuses on a particular MCMC algorithm:
\emph{Metropolis-Hastings} (MH)~\cite{MRR53, Has70}.

At each iteration \(i\), the Metropolis-Hastings algorithm samples \(\theta_i\)
from a distribution \(\pi\) of \(\theta\)
by first picking a proposal \(\theta'\) from a proposal 
distribution \(q(\cdot \mid \theta_{i-1})\)~\cite{MRR53}, where \(\theta_{i-1}\) is the
previously sampled value\footnote{
    The value of \(\theta_0\) for the first iteration is given as input to the 
    algorithm.
}. We shorten \(\theta_{i-1}\) to \(\theta\) in the following. 
The ratio of posterior and proposal densities is calculated
\[
    r(\theta, \theta') = \frac{\pi(\theta')}{\pi(\theta)}
    \frac{q(\theta\mid \theta')}{q(\theta'\mid \theta)},
\]
and the proposal is accepted with probability \(\min\{1, r\}\). 
If the proposal is accepted, 
\(\theta_i = \theta'\), otherwise \(\theta_i = \theta\).

It can be shown that, with a suitable proposal distribution, the chain of
\(\theta_i\) values is \emph{ergodic} and has \(\pi\) as its \emph{invariant
distribution}, which implies that the chain converges to \(\pi\)~\cite{Has70}.
The invariance of \(\pi\) follows from the \emph{detailed balance condition}~\cite{Robert04}:
\[
  \pi(\theta)q(\theta'\mid \theta)\min\{1, r(\theta, \theta')\}
  = \pi(\theta')q(\theta\mid \theta')\min\{1, r(\theta', \theta)\}.
\]
The ergodicity of the chain depends on the proposal distribution. Proving
ergodicity in general can be difficult, but there is an easy to check
sufficient condition: if the proposal allows moving to any state from any
state with positive probability, the chain is called
\emph{strongly irreducible}. Together with the detailed balance condition,
strong irreducibility implies ergodicity~\cite{Robert04}. The Gaussian
distribution centered
at the current value is a commonly used proposal, which is clearly
strongly irreducible.

When MCMC is used in Bayesian inference, the distribution to approximate is 
\[
    \pi(\theta) = p(\theta \mid X) = \frac{p(X \mid \theta)p(\theta)}
    {\int p(X\mid \theta)p(\theta)\dx\theta}.
\]
The difficult integral \(\int p(X\mid \theta)p(\theta)\dx\theta\) in the denominator
cancels out when computing \(r\), so only the likelihood and the prior are needed. 
For numerical stability, \(r\) is usually computed in 
log-space, which makes the acceptance probability
\(\min\{1, e^{\lambda(\theta, \theta')}\}\) where 
\begin{equation}\label{lambda_equation}
    \lambda(\theta, \theta') = \ln \frac{p(X\mid \theta')}{p(X\mid \theta)}
    + \ln \frac{p(\theta')}{p(\theta)}
    + \ln \frac{q(\theta\mid \theta')}{q(\theta'\mid \theta)}.
\end{equation}

The dataset \(X\) is typically a table with \(n\) independent rows.
The likelihood is given as \(p(x_j\mid \theta)\)
for row \(x_j\). Independence of the rows means that 
\[
    p(X\mid \theta) = \prod_{j=1}^n p(x_j\mid \theta),
\]
which means that the log-likelihood ratio term of \(\lambda\) is
\[
    \ln \frac{p(X\mid \theta')}{p(X\mid \theta)}
    = \sum_{j=1}^n \ln\frac{p(x_j\mid \theta')}{p(x_j\mid \theta)}.
\]
Algorithm~\ref{MH_algo} puts all of this together to summarise the MH 
algorithm used for Bayesian inference.

\begin{algorithm}[H]\label{MH_algo}
    \SetAlgoLined
    \For{\(1 \leq i \leq k\)}{
        denote \(\theta = \theta_{i-1}\)\\
        sample \(\theta' \sim q(\cdot \mid \theta)\)\\
        \(\ln \frac{p(X\mid \theta')}{p(X\mid \theta)} =
            \sum_{j=1}^n (\ln p(x_j \mid \theta') - \ln p(x_j\mid \theta))
        \)\\
        \(\lambda = \ln \frac{p(X\mid \theta')}{p(X\mid \theta)}
        + \ln p(\theta') - \ln p(\theta)
        + \ln q(\theta\mid \theta') - \ln q(\theta'\mid \theta)\)
        \\
        \(\theta_i = \begin{cases}
            \theta' & \text{ with probability } \min\{1, e^\lambda\} \\
            \theta & \text{ otherwise}
        \end{cases}
        \)\\
    }
    \Return \((\theta_1, \dotsc, \theta_k)\)
    \caption{
        Metropolis-Hastings: number of iterations \(k\), proposal 
        distribution \(q\), initial value \(\theta_0\) and
        dataset \(X\) as input.
    }
\end{algorithm}

\section{Measure Theory}

Measure theory is the study of \emph{measures}, which unify the concepts
of length, area and volume of subsets of \(\R\), \(\R^{2}\) and
\(\R^{3}\), the number of elements of a finite set, probabilities of events,
and other similar
concepts. All of the aforementioned concepts assign a positive real number,
or infinity, to a set, that measures the size of the set in some sense.
In all cases, the measure of the empty set is zero, and measure is
additive: the measure of a union of disjoint sets is the sum of the measures
of each set in the union. Ideally, every subset would have a measure, like
every finite set has a number of elements, but assigning a
length to all subsets of \(\R\) in a way that has all the properties expected
of lengths
is not possible~\cite{Cin11}. Similar difficulties occur in higher dimensions and with
continuous probability distributions, so the range of a measure is
restricted to a collection of where a satisfactory assignment of measure is
possible.

Before defining measures, some notation is needed. Denote the set of all
non-negative real numbers with \(\R_{+}\), and denote
\(\bar{\R}_{+} = \R_{+}\cup \{\infty\}\). In sums, the \(\infty\)-symbol
behaves as expected: \(x + \infty = \infty\) for any \(x\in \R\) and
\(\infty + \infty = \infty\). The set of all subsets of a set \(E\), called
the \emph{powerset} of \(E\), is denoted by \(\mathcal{P}(E)\).

\begin{definition}
  Let \(E\) be a set and let \(\mathcal{E}\subset \mathcal{P}(E)\).
  \(\mathcal{E}\) is called a \(\sigma\)-algebra on \(E\) if
  \begin{enumerate}
    \item \(\emptyset \in \mathcal{E}\)
    \item \(A\in \mathcal{E}\Rightarrow A^{C}\in \mathcal{E}\)
    \item \(A_{1},A_{2},\dotsc \in \mathcal{E}
          \Rightarrow \bigcup_{i=1}^{\infty} A_{i}\in \mathcal{E}\)
  \end{enumerate}
  The pair \((E, \mathcal{E})\) is called a measurable space.
\end{definition}
The sets of a \(\sigma\)-algebra \(\mathcal{E}\) are called
\(\mathcal{E}\)-measurable, or simply measurable if the corresponding
\(\sigma\)-algebra is clear from the context.

\begin{definition}
	Let \((E, \mathcal{E})\) be a measurable space. A measure on \((E, \mathcal{E})\)
  is a function \(\mu\colon \mathcal{E}\to \bar{\R}_{+}\) such that
  \begin{enumerate}
    \item
    \(\mu(\emptyset) = 0\)
    \item
    For any sequence of sets \(A_{1}, A_{2},\dotsc \in \mathcal{E}\)
    \[
    \quad \mu\left(\bigcup_{i=1}^{\infty} A_{i}\right)
    = \sum_{i=1}^\infty\mu(A_{i})
    \]
  \end{enumerate}
  The triple \((E, \mathcal{E}, \mu)\) is called a measure space.
\end{definition}

The reason for restricting the domain of a measure to a \(\sigma\)-algebra
is the impossibility of defining a length for all subsets of \(\R\): it is
possible to assign a length to all \emph{Lebesgue-measurable} sets, which form
a \(\sigma\)-algebra~\cite{Cin11}. The measure assigning length in \(\R\),
area in \(\R^{2}\) and their counterparts in higher dimensions is called the
\emph{Lebesgue measure}\footnote{
  Technically the Lebesgue measures on \(\R^{d}\) for each \(d\in \N\) are
  different measures.
} and is denoted by \(\lambda\).

For this thesis, it is sufficient to
work with the \emph{Borel \(\sigma\)-algebra} of \(\R^{d}\), denoted by
\(\mathcal{B}(\R^{d})\), which is
the smallest \(\sigma\)-algebra of \(\R^{d}\) containing all the open sets
of \(\R^{d}\). Members of \(\mathcal{B}(\R^{d})\) are called the Borel sets,
and they include most sets one encounters in mathematics: all open, all closed
sets and all sets obtained from them by countable numbers of set-theoretic
operations like unions and intersections~\cite{Cin11}. As \(\mathcal{B}(\R^{d})\) is the
only \(\sigma\)-algebra used with \(\R^{d}\) in this thesis, the measurable space
\((\R^{d}, \mathcal{B}(\R^{d}))\) is shortened to \(\R^{d}\) when this does not
cause ambiguity.

For measurable spaces \((E, \mathcal{E})\) and \((F, \mathcal{F})\),
there is a \emph{product \(\sigma\)-algebra} \(\mathcal{E}\otimes \mathcal{F}\)
on \(E\times F\) that is the smallest \(\sigma\)-algebra containing
all sets of the form \(A\times B\) for \(A\in \mathcal{E}\) and
\(B\in\mathcal{F}\). The product of \((E, \mathcal{E})\) with itself
is denoted by \((E, \mathcal{E})^{2}\).

A measure \(\mu\) on a measurable space \((E, \mathcal{E})\) is called
a \emph{probability measure} if \(\mu(E) = 1\). \(\mu\) is called
\emph{\(\sigma\)-finite} if there is a countable partition of \(E\)
into measurable sets \(E_{1}, E_{2},\dotsc\) where \(\mu(E_{i}) < \infty\)
for all \(i\in \N\).
Many theorems in measure theory require \(\sigma\)-finiteness from the involved
measures to ensure that they are not infinite for too many sets.
All probability measures are clearly \(\sigma\)-finite, as is the Lebesgue
measure~\cite{Cin11}.

Measure-theoretic concepts that involve functions, particularly the
Lebesgue integral discussed later, typically require that the functions
preserve the measurability of sets.
\begin{definition}
	Let \((E, \mathcal{E})\) and \((F, \mathcal{F})\) be measurable spaces
  and let \(f\colon E\to F\). \(f\) is called
  \(\mathcal{E}\)-\(\mathcal{F}\)-measurable if
  \(B\in \mathcal{F}\Rightarrow f^{-1}B\in \mathcal{E}\).
\end{definition}
As with measurable sets, \(\mathcal{E}\)-\(\mathcal{F}\)-measurable functions
are simply called measurable if \(\mathcal{E}\) and \(\mathcal{F}\) are
clear from the context.
Most functions that one comes across are measurable. In particular,
the indicators of measurable sets are measurable, continuous functions on
are Borel-Borel-measurable, and compositions of measurable functions
are measurable~\cite{Cin11}.

One of the hallmarks of measure theory is the Lebesgue integral, which
extends the Riemann integral to allow integration over all measurable functions,
and allows integrating with respect to an arbitrary measure. The
definition~\cite[Definition 4.3]{Cin11}
of the Lebesgue integral is fairly technical and is not given here. Instead,
the notation used with the integral is defined, and some properties of the
integral are give in Theorem~\ref{lebesgue_integral_theorem}.

\begin{definition}
	Let \((E, \mathcal{E}, \mu)\) be a measure space and let \(f\colon E\to \R_{+}\)
  be a measurable function. The Lebesgue integral of \(f\) over a set
  \(A\in \mathcal{E}\) with respect to \(\mu\) is denoted by
  \[
    \int_{A}\mu(\dx x)f(x).
  \]
\end{definition}
Note that the integral was only defined for non-negative functions. It is possible
to extend the definition to measurable real-valued functions~\cite{Cin11},
but the integral for non-negative functions is sufficient for this thesis.

\begin{theorem}\label{lebesgue_integral_theorem}
  Useful properties of the Lebesgue integral\cite{Cin11}:
	\begin{enumerate}
    \item For a measure space \((E, \mathcal{E}, \mu)\) and
          \(A\in \mathcal{E}\), \(\int_{E}\mu(\dx x)1_{A}(x) = \mu(A)\).
    \item For a measure space \((E, \mathcal{E}, \mu)\), measurable
          \(f, g\colon E\to \R_{+}\), \(A\in \mathcal{E}\) and \(a, b\in \R_{+}\),
          \(\int_{A} \mu(\dx x)(af(x) + bg(x))
          = a\int_{A} \mu(\dx x) f(x) + b\int_{A} \mu(\dx x) g(x)\).
    \item For a Riemann-integrable function \(f\colon \R\to \R_{+}\) and
          \(a, b\in \R\) with \(a \leq b\),
          \(\int_{[a, b]}\lambda(\dx x)f(x) = \int_{a}^{b}f(x)\dx x\),
          where the integral on the right is a Riemann integral.
    \item For a continuous random variable \(X\) on \(\R^{d}\) with
          density \(\pi\) and \(A\in \mathcal{B}(\R^{d})\),
          \(P(X\in A) = \int_{A}\lambda(\dx x)\pi(x)\).
    \item Let \(\mu\) and \(\nu\) be \(\sigma\)-finite measures on
          measure spaces \((E, \mathcal{E})\) and \((F, \mathcal{F})\),
          respectively. Let \(f\colon E\times F\to \R_{+}\) be measurable.
          Then
          \[
          \int_{A}\mu(\dx a)\int_{B}\nu(\dx b)f(a, b)
          = \int_{B}\nu(\dx b)\int_{A}\mu(\dx a)f(a, b).
          \]
          for all \(A\in \mathcal{E}\) and \(B\in \mathcal{F}\).
  \end{enumerate}
\end{theorem}

Randomised functions come up in several places in the study of MH algorithms.
Each sample from an MH algorithm is obtained by evaluating a random function
on the previous value, so the entire algorithm can be thought of as repeatedly
composing that function, called the \emph{transition kernel}, with itself.
Additionally, the proposal is also obtained
from a random function on the previous value. The convergence of an MH
algorithm can be studied by studying the properties of these functions.
Formally, these randomised functions are \emph{Markov kernels},
functions from values to probability measures on values.

\begin{definition}
	Let \((E, \mathcal{E})\) be a measurable space. A Markov kernel on
  \((E, \mathcal{E})\) is a function \(q\colon E\times \mathcal{E}\to [0, 1]\)
  where
  \begin{enumerate}
    \item For all \(A\in \mathcal{E}\), the function \(q(\cdot, A)\) is
          measurable.
    \item
          For all \(a\in E\), the function \(q(a, \cdot)\) is a probability
          measure.
  \end{enumerate}
\end{definition}

\begin{lemma}\label{markov_kernel_composition_lemma}
	The composition of Markov kernels \(q_{1}\) and \(q_{2}\) on a measurable space
  \((E, \mathcal{E})\) is given by
  \[
    (q_{2}\circ q_{1})(a, C) = \int_{E}q_{1}(a, \dx b)q_{2}(b, C)
  \]
\end{lemma}
\begin{proof}
	See~\cite[Equation 6.5]{Cin11}.
\end{proof}

\begin{definition}\label{reversible_definition}
	Let \((E, \mathcal{E})\) be a measurable space. A Markov kernel \(q\) on
  \((E, \mathcal{E})\) is reversible with respect to a \(\sigma\)-finite
  measure \(\mu\) on \((E, \mathcal{E})\)
  if
  \[
    \int_{A}\mu(\dx a)\int_{B}q(a, \dx b) = \int_{B}\mu(\dx b)\int_{A}q(b, \dx a)
  \]
  for all \(A, B\in \mathcal{E}\).
\end{definition}

\begin{lemma}\label{reversible_integration_lemma}
  Let \(q\) be a markov kernel on a measurable space \((E, \mathcal{E})\)
  reversible with respect to a \(\sigma\)-finite measure \(\mu\) on
  \((E, \mathcal{E})\) and let \(f\colon E\times E\to \R_{+}\) be a
  measurable function. Then
  \[
    \int_{A}\mu(\dx a)\int_{B}q(a, \dx b)f(a, b)
    = \int_{B}\mu(\dx b)\int_{A}q(b, \dx a)f(a, b)
  \]
\end{lemma}

\begin{proof}
  Setting
  \[
    \nu_{1}(A\times B) = \int_{A}\mu(\dx a)\int_{B}q(a, \dx b)
  \]
  \[
    \nu_{2}(A\times B) = \int_{B}\mu(\dx b)\int_{A}q(b, \dx a)
  \]
  for \(A, B\in \mathcal{E}\) defines unique measures
  \(\nu_{1}\) and \(\nu_{2}\) on
  \((E, \mathcal{E})^{2}\)~\cite[Theorem 6.11]{Cin11}, which,
  by Definition~\ref{reversible_definition}, are equal. Then
  \begin{align*}
    \int_{A}\mu(\dx a)\int_{B}q(a, \dx b)f(a, b)
    &= \int_{A}\int_{B}\nu_{1}(\dx a, \dx b)f(a, b)
    \\&= \int_{A}\int_{B}\nu_{2}(\dx a, \dx b)f(a, b)
    \\&= \int_{B}\mu(\dx b)\int_{A}q(b, \dx a)f(a, b)
    \qedhere
  \end{align*}
\end{proof}

\begin{lemma}\label{composition_reversible_lemma}
	If Markov kernels \(q_{1}\) and \(q_{2}\) on \((E, \mathcal{E})\)
  are reversible with respect to \(\sigma\)-finite a measure \(\mu\)
  on \((E, \mathcal{E})\),
  \[
    \int_{A}\mu(\dx a)\int_{C}(q_{2}\circ q_{1})(a, \dx c)
    = \int_{C}\mu(\dx c)\int_{A}(q_{1}\circ q_{2})(c, \dx a).
  \]
  for all \(A, C\in \mathcal{E}\).
\end{lemma}
\begin{proof}
  Lemma~\ref{markov_kernel_composition_lemma} can be restated as
  \[
    \int_{C}(q_{2}\circ q_{1})(a, \dx c) = \int_{E}q_{1}(a, \dx b)\int_{C}q_{2}(b, \dx c).
  \]
  Then
	\begin{align*}
    \int_{A}\mu(\dx a)\int_{C}(q_{2}\circ q_{1})(a, \dx c)
    &= \int_{A}\mu(\dx a)\int_{E}q_{1}(a, \dx b)\int_{C}q_{2}(b, \dx c)
    \\&= \int_{E}\mu(\dx b)\int_{A}q_{1}(b, \dx a)\int_{C}q_{2}(b, \dx c)
    \\&= \int_{E}\mu(\dx b)\int_{C}q_{2}(b, \dx c)\int_{A}q_{1}(b, \dx a)
    \\&= \int_{C}\mu(\dx c)\int_{E}q_{2}(c, \dx b)\int_{A}q_{1}(b, \dx a)
    \\&= \int_{C}\mu(\dx c)\int_{A}(q_{1}\circ q_{2})(c, \dx a).
    \qedhere
  \end{align*}
\end{proof}
\begin{lemma}\label{composition_reversible_multiple_lemma}
  For Markov kernels \(q_{1}\dotsc q_{k}\) on \((E, \mathcal{E})\) reversible
  with respect to a \(\sigma\)-finite measure \(\mu\) on \((E, \mathcal{E})\),
  \[
    \int_{A}\mu(\dx a)\int_{C}(q_{k}\circ \dotsb \circ q_{1})(a, \dx c)
    = \int_{C}\mu(\dx a)\int_{A}(q_{1}\circ \dotsb \circ q_{k})(c, \dx a).
  \]
\end{lemma}
\begin{proof}
  The proof is by induction on \(k\). For \(k = 1\), the claim is the definition
  of reversibility with respect to \(\mu\). If the claim holds for \(k - 1\),
  \begin{align*}
    &\int_{A}\mu(\dx a)\int_{C}(q_{k}\circ\dotsb \circ q_{1})(a, \dx c)
    \\=& \int_{A}\mu(\dx a)\int_{E}(q_{k-1}\circ \dotsb \circ q_{1})(a, \dx b)
      \int_{C}q_{k}(b, \dx c)
    \\=& \int_{E}\mu(\dx b)\int_{A}(q_{1}\circ \dotsb \circ q_{k-1})(b, \dx a)
      \int_{C}q_{k}(b, \dx c)
    \\=& \int_{C}\mu(\dx c) \int_{E}q_{k}(c, \dx b)
         \int_{A}(q_{1}\circ \dotsb \circ q_{k-1})(b, \dx a)
    \\=&\int_{C}\mu(\dx c)\int_{A}(q_{1}\circ\dotsb \circ q_{k})(c, \dx a),
  \end{align*}
  so the claim holds for all \(k\in \N\) by induction.

\end{proof}

\begin{lemma}\label{composition_reversible_symmetric_lemma}
  Let \(q_{1}\dotsc q_{k}\) be Markov kernels on \((E, \mathcal{E})\) reversible
  with respect to a \(\sigma\)-finite measure \(\mu\) on \((E, \mathcal{E})\).
  If \(q_{1}\circ \dotsb \circ q_{k} = q_{k}\circ \dotsb \circ q_{1}\),
  the composition \(q_{1}\circ \dotsb \circ q_{k}\) is reversible with
  respect to \(\mu\).
\end{lemma}
\begin{proof}
  Because \(q_{1}\circ \dotsb \circ q_{k} = q_{k}\circ \dotsb \circ q_{1}\),
  by Lemma~\ref{composition_reversible_multiple_lemma}
  \begin{align*}
    \int_{A}\mu(\dx a)\int_{C}(q_{k}\circ \dotsb \circ q_{1})(a, \dx c)
    &= \int_{C}\mu(\dx c)\int_{A}(q_{1}\circ \dotsb \circ q_{k})(c, \dx a)
    \\&= \int_{C}\mu(\dx c)\int_{A}(q_{k}\circ \dotsb \circ q_{1})(c, \dx a).
    \qedhere
  \end{align*}
\end{proof}

Reversibility is linked to the theory of MH algorithms: the detailed balance
condition is equivalent to the reversibility of the transition kernel of
the MH algorithm with respect to the target distribution \(\pi\)~\cite{Tie98}.
Additionally, if target is continuous, and the proposal is reversible with
respect to the Lebesgue measure, the acceptance probability is simplified.

\begin{lemma}\label{reversible_proposal_lemma}
  For an MH-algorithm,
  if the proposal Markov kernel \(q\) on \(\R^{n}\) is reversible with
  respect to the
  Lebesgue measure, the target distribution \(\pi\) is continuous and
  \[
    \alpha(\theta, \theta') = \min\left\{1, \frac{\pi(\theta')}{\pi(\theta)}\right\}
  \]
  is used as the acceptance probability when moving from \(\theta\) to \(\theta'\),
  the algorithm has \(\pi\) as the invariant distribution.
\end{lemma}
\begin{proof}
  With acceptance probability \(\alpha\), the algorithm has \(\pi\) as its
  invariant distribution if the detailed balance condition,
  \[
    \int_{A}\pi(\dx \theta)\int_{B}q(\theta, \dx \theta')\alpha(\theta, \theta')
    = \int_{B}\pi(\dx \theta')\int_{A}q(\theta', \dx \theta)\alpha(\theta', \theta)
  \]
  for all measurable sets \(A, B\subset \R^{d}\), holds~\cite{Tie98}.
  For continuous \(\pi\) and \(q\) reversible with respect to the Lebesque measure,
  \begin{align*}
    \int_{A}\pi(\dx \theta)\int_{B}q(\theta, \dx \theta')\alpha(\theta, \theta')
    &= \int_{A}\dx \theta\int_{B}q(\theta, \dx \theta')\pi(\theta)\alpha(\theta, \theta')
    \\&= \int_{A}\dx\theta\int_{B}q(\theta, \dx \theta')\min\{\pi(\theta), \pi(\theta')\}
    \\&= \int_{B}\dx\theta'\int_{A}q(\theta', \dx \theta)\min\{\pi(\theta'), \pi(\theta)\}
    \\&= \int_{A}\pi(\dx \theta')\int_{B}q(\theta', \dx \theta)\alpha(\theta', \theta).
    \qedhere
  \end{align*}
\end{proof}

Deterministic and partially deterministic proposals studied in
Chapter~\ref{hmc_chapter} require handling Markov kernels that are (partially)
deterministic. This is made possible by the \emph{Dirac measure}, which is
the probability measure for a degenerate random variable that always takes
the same value.
\begin{definition}
	The Dirac measure \(\delta_{x}\) for \(x\in \R^{d}\) is a measure on \(\R^{d}\)
  where
  \[
    \delta_{x}(A) = \begin{cases}
      1 & \mathrm{if}\  x\in A\\
      0 & \mathrm{if}\  x\notin A
    \end{cases}
  \]
\end{definition}
Based on the definition, \(\delta_{x}\) is the probability measure of a random
variable that always has the value \(x\). As a Markov kernel, it represents
the identity function. The Dirac measure can also represent the Markov kernel
of any deterministic function \(f\) by \(\delta_{f(x)}\). It turns out that
with a suitable \(f\), \(\delta_{f(x)}\) is reversible with respect to the
Lebesgue measure, which is applied together with
Lemma~\ref{reversible_proposal_lemma} in Chapter~\ref{hmc_chapter}.

\begin{lemma}\label{dirac_measure_reversible_lemma}
  For a function \(f\colon \R^{d}\to \R^{d}\) that preserves Lebesgue measure and has
  \(f^{-1} = f\),
	the Dirac measure perturbed by \(f\), \(\delta_{f(x)}\), is reversible
  with respect to the Lebesgue measure. As a special case,
  the Dirac measure \(\delta_{x}\) is reversible with respect to the
  Lebesgue measure.
\end{lemma}
\begin{proof}
  Denote the Lebesgue measure by \(\lambda\). Because \(f\) preserves
  Lebesgue measure and is injective,
	\begin{align*}
    \int_{A}\delta_{f(x)}(B)\dx x
    &= \int_{A}1_{B}(f(x))\dx x
    \\&= \lambda(A\cap f^{-1}(B))
    \\&= \lambda(f^{-1}(A)\cap f^{-1}(f^{-1}((B))))
    \\&= \lambda(f^{-1}(A)\cap B)
    \\&= \int_{B}\delta_{f(x)}(A)\dx x
  \end{align*}
  The special case of \(\delta_{x}\) is obtained by setting \(f(x) = x\).
\end{proof}

\chapter{Differentially Private MH}\label{dp_mcmc_chapter}

As seen in Section~\ref{DP_background}, an algorithm can be made differentially 
private by adding Gaussian noise its output. The noise could also be added
to any intermediate value calculated by the algorithm, and post processing immunity 
will guarantee that the same DP bounds that hold for releasing the intermediate 
value also hold for releasing the final result of the algorithm.

In 2019, Yildirim and Ermis~\cite{YildirimE19} realised that if Gaussian noise
is added to the exact value of \(\lambda\) from Equation~\ref{lambda_equation},
the noise can be corrected for,
yielding a differentially private MH algorithm, called DP penalty,
which converges to 
the correct distribution. In the same year, Heikkilä et al.~\cite{HeikkilaJDH19}
developed another DP MCMC algorithm, called DP Barker, which uses subsampling 
to amplify privacy. DP Barker is based on the Barker acceptance
test~\cite{Barker65} instead of the MH test, and uses an approximation
of the Barker acceptance test to correct for the noise added for DP.

Also in 2019, Sommer et al. proved the ADP-bound of
Theorem~\ref{gauss-DP-bounds}~\cite{Sommer2019}, which gives tight bounds for
ADP, unlike the zCDP based privacy accounting Yildirim and
Ermis~\cite{YildirimE19} used. Section~\ref{dp_penalty_adp_section} applies
the new ADP-bound to DP penalty.
Section~\ref{dp_minibatch_penalty_section} combines DP penalty with the
subsampling idea from DP Barker, and formulates a subsampled DP penalty
algorithm.

\section{Differentially Private MH with the Penalty Algorithm}\label{dp_penalty_section}

In 1999, Ceperley and Dewing~\cite{CeD99} developed a variant of 
Metropolis-Hastings called the penalty 
algorithm, where only a noisy approximation of \(\lambda\) is known. The
original algorithm was
developed for simulations in physics where computing \(\lambda\)
requires computing energies of complex systems, which can only be approximated.
The penalty algorithm modifies the acceptance probability to account for the 
noise added to \(\lambda\) and still converges to the correct distribution if 
the noise is Gaussian with known variance.

The DP penalty algorithm adds Gaussian noise to the value of \(\lambda\), and 
uses the penalty algorithm to correct the acceptance probability so that 
the algorithm still converges to the correct distribution~\cite{YildirimE19}.
The corrected acceptance probability for Gaussian noise with variance 
\(\sigma^2\) is 
\[
    \min\{1, e^{\lambda(\theta, \theta') - \frac{1}{2}\sigma^2}\}
\]

Theorem~\ref{DP_penalty_theorem_zcdp} gives the number of iterations DP penalty 
can be run for when the privacy cost is computed through zCDP, which is 
what Yildirim and Ermis prove in their paper~\cite{YildirimE19}.

\begin{theorem}\label{DP_penalty_theorem_zcdp}
  Let \(\epsilon > 0\), \(0 < \delta < 1\), \(\alpha > 0\), \(\tau > 0\) and
  \(n > 0\).
    Let
    \[
        \rho = (\sqrt{\epsilon - \ln \delta} - \sqrt{-\ln \delta})^2
    \]
    \[
        c(\theta, \theta') = \sup_{x_j, x'_j} (p(x_j\mid \theta') - p(x_j\mid \theta) 
        - (p(x'_j\mid \theta') - p(x'_j\mid \theta)))
    \]
    \[
        \sigma^2(\theta, \theta') = \tau^2 n^{2\alpha}c^2(\theta, \theta')
    \]
    Then running DP penalty for
    \[
        k = \lfloor 2\tau^2 n^{2\alpha} \rho\rfloor
    \]
    iterations with dataset size \(n\), when using \(\sigma^2\) as the
    variance of the Gaussian noise
    and the substitute neighborhood relation, is \((\epsilon, \delta)\)-ADP.
\end{theorem}

The \(\tau\)-parameter controls the trade-off between the amount of noise
and the number of iterations the algorithm is allowed to run for. From the
expression for \(\sigma^{2}\) and \(k\) in Theorem~\ref{DP_penalty_theorem_zcdp},
the standard deviation of the noise increases linearly with \(\tau\), while
the number of iterations increases quadratically.

Yildirim and Ermis~\cite{YildirimE19} see \(\alpha\) as a bound on \(c\) of the
form
\[
  c(\theta, \theta') \leq Cn^{-\alpha}
\]
for almost all \(\theta\) and \(\theta'\) and constant
\(C > 0\)~\cite[Assumption A1]{YildirimE19}.
They also consider a weaker form where for any \(\varepsilon\) there is
a \(C\) such that
\(P(c(\theta, \theta') > Cn^{-\alpha}) < \varepsilon\)~\cite[Assumption A4]{YildirimE19}.
These bounds imply that \(\sigma\)
does not increase with \(n\), which Yildirim and Ermis see as a necessary
condition for the DP penalty algorithm to perform well. They also recommend
having \(\alpha = \frac{1}{2}\), which is used in the experiments in
Chapter~\ref{experiment_chapter}.
With clipping, the weaker bound can be achieved by setting the clip bound based
on \(n\). For the experiments
in Chapter~\ref{experiment_chapter}, the parameters are optimised separately
for each \(n\) that is used, so setting any clip bound can be seen as
choosing a value of \(C\) for each \(\varepsilon\).

% However , this condition may not be necessary, as DP penalty adds noise to
% \(\lambda\), which is a sum of \(n\) values. Unless there is a lot of
% cancellation between the different terms, the magnitude of \(\lambda\), would
% increase with \(n\), so the ratio of \(\lambda\) and the noise may not increase,
% even if the variance of the noise increases. Of course, this is a very informal
% argument

Theorem~\ref{DP_penalty_theorem_zcdp}
requires a bound on sensitivity of the log-likelihood ratio. If there is a bound
\[
    |\ln p(x_j\mid \theta') - \ln p(x_j\mid \theta)| \leq L||\theta - \theta'||_2
\]
for all \(x_j, \theta\) and \(\theta'\) then
\[
    c(\theta, \theta') \leq 2L||\theta - \theta'||_2.
\]
The former bound is true in some models, such as logistic
regression~\cite{YildirimE19}. In other
models it can be forced by clipping the log-likelihood ratios with the bound
\(b = L||\theta - \theta'||_{2}\). This will remove the
guarantee of eventually converging to the correct posterior, but if \(L\) is
chosen to be large enough, the clipping will not affect the
acceptance decision frequently. As a tradeoff, picking a large \(L\) will increase
the variance of the Gaussian noise and slow down convergence through it.
The effects of clipping are empirically studied in
Section~\ref{clipping_experiments}.

Yildirim and Ermis~\cite{YildirimE19} propose two potential variations to improve the
performance of the penalty algorithm. The first variation, called
\emph{one component updates} (OCU) in this thesis, is only proposing
changes in one dimension in a multidimensional problem. This decreases
\(||\theta - \theta'||_2\), which means that it decreases the noise variance.

The second variation is called \emph{guided walk Metropolis Hastings}
(GWMH)~\cite{YildirimE19}.
In GWMH, proposals change only one dimension, as in OCU. Additionally, a direction
is associated with each dimension, and proposals are only made in the current
direction of the chosen dimension. After an accepted proposal, the direction is
kept the same, but after a rejection it is switched. This means that the chain can
move towards areas of higher probability faster because, after some initial
proposals are rejected, the directions for each dimension point towards the
area of high probability, so all proposals are towards it. Without GRMH, most
proposals would move the chain away from the area of high probability, and
would likely be rejected.

\section{Applying New Techniques to DP Penalty}\label{dp_penalty_adp_section}

The DP penalty algorithm is a composition of Gaussian mechanisms, so
the ADP-bound of Theorem~\ref{gauss-DP-bounds} is applicable. Unlike
Theorem~\ref{DP_penalty_theorem_zcdp}, Theorem~\ref{gauss-DP-bounds}
gives tight ADP-bounds, so using the latter instead of the former will
always give better privacy bounds. Theorem~\ref{DP_penalty_theorem_adp}
formulates the bound of Theorem~\ref{gauss-DP-bounds} for DP penalty.
The iteration numbers the theorems allow are compared in
Section~\ref{accounting_comparison_section}.

\begin{theorem}\label{DP_penalty_theorem_adp}
    Let \(\epsilon > 0\), \(\alpha > 0\), \(\tau > 0\) and \(n > 0\).
    Define \(c\) and \(\sigma^{2}\) as in Theorem~\ref{DP_penalty_theorem_zcdp}.
    The DP penalty algorithm, after running for \(k\) iterations using \(\sigma^{2}\)
    as the noise variance with dataset size \(n\),
    is \((\epsilon, \delta(\epsilon))\)-DP, with the substitute neighborhood
    relation, for
    \[
        \delta(\epsilon) 
        = \frac{1}{2}\left(
            \erfc\left(\frac{\epsilon - k\mu}{2\sqrt{k\mu}}\right)
            - e^\epsilon \erfc\left(\frac{\epsilon + k\mu}{2\sqrt{k\mu}}\right)
        \right)
    \]
    where \(\mu = \frac{1}{2\tau^2 n^{2\alpha}}\). Furthermore, this privacy
    bound is tight.
\end{theorem}
\begin{proof}
    DP penalty is an adaptive composition of Gaussian mechanisms that release 
    noisy values of \(\lambda(\theta, \theta')\). With the substitute neighborhood
    relation, the sensitivity of \(\lambda(\theta, \theta')\) is
    \(c(\theta, \theta')\). For the tight ADP bound used here, the sensitivity must be 
    constant in each iteration. This is achieved by releasing 
    \(\frac{\lambda(\theta, \theta')}{c(\theta, \theta')}\) instead, which 
    has sensitivity 1. \(c(\theta, \theta')\) does not depend on \(X\), 
    so \(\lambda(\theta, \theta')\) can be obtained from 
    \(\frac{\lambda(\theta, \theta')}{c(\theta, \theta')}\) by post-processing.

    Adding Gaussian noise with variance \(\sigma_n^2\) to 
    \(\frac{\lambda(\theta, \theta')}{c(\theta, \theta')}\)
    is equivalent to adding Gaussian noise with variance 
    \(\sigma_n^2 c^2(\theta, \theta')\) to \(\lambda(\theta, \theta')\).
    Setting \(\sigma_n^2 = \tau^2n^{2\alpha}\) and plugging into 
    the ADP bound of Theorem~\ref{gauss-DP-bounds} proves the 
    claim.
\end{proof}

Theorem~\ref{DP_penalty_theorem_adp} is harder to use than 
Theorem~\ref{DP_penalty_theorem_zcdp} because the number of iterations
DP penalty can be run for given an \((\epsilon, \delta)\)-bound cannot be 
computed analytically for the former. However, the maximum number of iterations 
can be solved for numerically. Algorithm~\ref{max_iterations_algo} shows a simple
procedure that solves the maximum number of iterations using the bisection
method.

\begin{algorithm}[H]\label{max_iterations_algo}
  \SetAlgoLined
	\(low\) = \(\mathrm{zcdp}(\epsilon, \delta, \tau, n)\) \\
  \(high\) = \(\max \{low, 1\}\) \\

  \While{\(\mathrm{adp}(\epsilon, high, \tau, n) < \delta\)}{
    \(high = high \cdot 2\) \\
  }
  \While{\(\lfloor high \rfloor - \lfloor low \rfloor > 1\)}{
    \(new = \frac{high + low}{2}\)\\
    \If{\(\mathrm{adp}(\epsilon, new, \tau, n) > \delta\)}{
      \(high = new\)\\
    }
    \Else{
      \(low = new\)\\
    }
  }
  \If{\(\mathrm{adp}(\epsilon, \lfloor high \rfloor, \tau, n) < \delta\)}{
    \Return \(\lfloor high \rfloor\)\\
  }
  \Else{
    \Return \(\lfloor low \rfloor\)\\
  }

  \caption{
    Maximise the number of iterations given \(\epsilon\), \(\delta\),
    \(\tau\) and \(n\). The
    \(\mathrm{zcdp}\)-function computes the number of iterations
    Theorem~\ref{DP_penalty_theorem_zcdp} allows, and the
    \(\mathrm{adp}\)-function computes \(\delta(\epsilon)\) from
    Theorem~\ref{DP_penalty_theorem_adp}. \(\lfloor \cdot \rfloor\) is the
    floor function that rounds real numbers down. Note that the variables
    \(low\), \(high\) and \(new\) are not necessarily integers, as
    Theorem~\ref{DP_penalty_theorem_adp} can handle a non-integer number of
    iterations.
  }
\end{algorithm}

\section{The Barker Acceptance Test}\label{dp_barker_section}

The DP Barker algorithm of Heikkilä et al.~\cite{HeikkilaJDH19} is based on
the Barker acceptance test~\cite{Barker65} instead of the Metropolis-Hastings test.
Instead of using the MH acceptance probability, the Barker acceptance test samples 
\(V_{log}\sim \mathrm{Logistic(0, 1)}\) and accepts if \(\lambda + V_{log} > 0\).

If Gaussian noise with variance \(\sigma^2\) is added to 
\(\lambda\), as long as \(\sigma^{2}\) is not too large, there exists an
approximate correction
distribution \(V_{corr}\) such that \(\caln(0, \sigma^2) + V_{corr}\) has
approximately the same distribution as \(V_{log}\). Because the variance of
\(V_{log}\) is
\(\frac{\pi^2}{3}\)~\cite{HeikkilaJDH19}, the variance of \(V_{corr}\) must be 
\(\frac{\pi^2}{3} - \sigma^2\), which means that there is an upper bound
to the noise variance: \(\sigma^2 < \frac{\pi^2}{3}\). Testing whether 
\(\lambda + \caln(0, \sigma^2) + V_{corr} > 0\) is approximately equivalent
to testing 
whether \(\lambda + V_{log} > 0\), which means that it is possible to derive 
a DP MCMC algorithm based on the Barker acceptance test if the correction 
distribution can be sampled from.

However, the analytical form of \(V_{corr}\) is not known~\cite{HeikkilaJDH19}.
Heikkilä et al.\  approximate the distribution with a Gaussian mixture model.
This means that their 
algorithm only converges to an approximately correct distribution, but the 
approximation error can be made very small.

By computing the sum in \(\lambda\) only over a subset of the data, the
running time of the algorithm is reduced, and the algorithm becomes less
sensitive to changes in the input data.
The latter property is called \emph{subsampling amplification}
of differential privacy~\cite{WangBK19}. Using the \(\lambda\) computed 
with subsampling instead of the full data \(\lambda\) introduces an additional 
error that must be corrected for to have the algorithm converge to the correct 
distribution. 

The \emph{central limit theorem} (CLT) states that the distribution of a sum 
of random variables approaches a Gaussian distribution as more random variables 
are summed, if some conditions on the independence and variance of the random 
variables are met~\cite{SPC17}. With the CLT, it can be argued
that the error from 
using the subsampled \(\lambda\) instead of the full data \(\lambda\) has an 
approximately Gaussian distribution, if the subsample is large 
enough~\cite{SPC17}.

The variance of the error from subsampling can 
be estimated by the sample variance of the individual terms in the sum in 
\(\lambda\)~\cite{SPC17}. This allows combining the errors from subsampling and the
Gaussian noise from the Gaussian mechanism to a single Gaussian noise value.
The \(V_{corr}\) distribution can then be used to approximate the Barker acceptance 
test as above~\cite{HeikkilaJDH19}. See Algorithm~\ref{DP_barker_algo} for the DP Barker
algorithm\footnote{
    See~\cite{HeikkilaJDH19} for the sampling procedure of \(V_{corr}\).
}.

Heikkilä et al.~\cite{HeikkilaJDH19} do not directly bound the sensitivity
of \(\lambda\) as is done in DP penalty, because the sample variance also 
depends on input data. Instead they directly bound the Rényi divergence 
between \(\caln(0, \sigma^2 - \sigma^2_b)\), where \(\sigma^2_b\) is the 
batch sample variance, for two adjacent inputs. Subsampling amplification 
is accounted for with an amplification theorem for RDP~\cite{WangBK19}.

\begin{theorem}\label{dp_barker_theorem}
    If
    \[
        |\ln p(x_j\mid \theta') - \ln p(x_j\mid \theta)| \leq \frac{\sqrt{b}}{n}
    \]
    \[
        2 < \alpha < \frac{b}{5}, \alpha \in \N
    \]
    for all \(\theta, \theta'\) and \(x_{j}\),
    running \(k\) iterations of DP Barker with dataset size \(n\),
    minibatch size \(b\) and noise variance \(\sigma^{2} = 2\)
    is \((\alpha, k\epsilon(\alpha))\)-RDP for the substitute neighborhood
    relation, with
    \[
        \epsilon(\alpha) = \frac{1}{\alpha - 1}\ln \left(
        1 + q^2\binom{\alpha}{2}\min\{4(e^{\epsilon'(2)} - 1), 2e^{\epsilon'(2)}\}
        + 2 \sum_{j=3}^\alpha q^j\binom{\alpha}{j}e^{(j-1)\epsilon'(j)}\right)
    \]
    and 
    \[
        \epsilon'(\alpha) = \frac{5}{2b} + \frac{1}{2(\alpha - 1)}
        \ln \frac{2b}{b - 5\alpha} + \frac{2\alpha}{b - 5\alpha}
    \]
    where \(q = \frac{b}{n}\)~\cite{HeikkilaJDH19}.
\end{theorem}
The RDP bounds given by Theorem~\ref{dp_barker_theorem} can be converted to
ADP bounds with Theorem~\ref{other_dp_to_adp}. This requires choosing an
optimal \(\alpha\) in the given range that maximises the number of iterations
the algorithm can run for.

% Algorithm~\ref{max_iterations_algo} can modified to compute the maximum number
% of iterations Theorem~\ref{dp_barker_theorem} allows by computing
% \(\epsilon\)-bounds given \(\delta\) with Theorem~\ref{dp_barker_theorem}
% and Theorem~\ref{other_dp_to_adp},
% instead of computing \(\delta\)-bounds given \(\epsilon\). The variable
% \(low\) of Algorithm~\ref{max_iterations_algo} must be initialised to 0,
% and the variable \(high\) must be initialised to a value greater than 0\footnote{
%   Choosing a value close to the maximum number of iterations speeds up the
%   computation of the maximum number of iterations. The experiments in this
%   thesis used the value 1024.
% }.

Like DP penalty, DP Barker requires a bound on the log-likelihood ratio for
one row of data. The bound can be forced through clipping if the model does not 
meet it, but because of the \(n\) in the denominator of the bound, it can get 
very tight for large values of \(n\). As a result, clipping may be needed for 
almost all log-likelihood ratios, which may cause the algorithm to converge
to a very different distribution from the posterior.

To alleviate the tight bound on log-likelihood sensitivity, DP Barker is best
used with a tempered likelihood~\cite{HeikkilaJDH19}. In tempering, the 
log-likelihood is multiplied by a number \(T = \frac{n_0}{n} < 1\). This
increases the variance of the resulting posterior and may lower modeling 
error in some cases~\cite{HeikkilaJDH19}.

Using the tempered likelihood, the log-likelihood
bound becomes 
\[
    T|\ln p(x_j\mid \theta') - \ln p(x_j\mid \theta)|
    \leq \frac{\sqrt{b}}{n},
\]
which is equivalent to 
\[
    |\ln p(x_j\mid \theta') - \ln p(x_j\mid \theta)|
    \leq \frac{\sqrt{b}}{n_0}.
\]
Typically \(n_0 \ll n\) for large datasets, so using a tempered likelihood requires 
significantly less clipping than a nontempered likelihood.

\begin{algorithm}[H]\label{DP_barker_algo}
    \SetAlgoLined
    \For{\(1 \leq i \leq k\)}{
        denote \(\theta = \theta_{i-1}\)\\
        sample \(\theta' \sim q(\cdot \mid \theta)\)\\
        sample \(B \subset \{1, \dotsc, n\}\)\\
        \For{\(j \in B\)}{
          \(r_j = \clip_{\frac{\sqrt{b}}{n_{0}}}(
          \ln p(x_j \mid \theta') - \ln p(x_{j}\mid \theta))\)\\
        }
        \(\sigma^2_b = \mathrm{Var}\{r_j\mid j\in B\}\)\\
        \(\lambda = 
        \frac{n}{b}\sum_{j\in B} r_j
        + \ln \frac{p(\theta')}{p(\theta)}
        + \ln \frac{q(\theta\mid \theta')}{q(\theta'\mid \theta)}\)
        \\
        sample \(s \sim \caln(0, \sigma^2 - \sigma^2_b)\)\\
        sample \(c \sim V_{corr}^{\sigma^2}\)\\
        \(\theta_i = \begin{cases}
            \theta' & \text{ if } \lambda + s + c > 0 \\
            \theta & \text{ otherwise}
        \end{cases}
        \)\\
    }
    \Return \((\theta_1, \dotsc, \theta_k)\)
    \caption{
      DP Barker: number of iterations, initial value \(\theta_{0}\),
      proposal distribution \(q\), noise variance \(\sigma^{2}\),
      data \(X\) as input.
    }
\end{algorithm}

Tuning the parameters of DP Barker is more restrictive than DP penalty, as
the former does not have a tunable clip bound, and its noise variance has an
upper bound. In addition, choosing a noise variance requires computing the
\(V_{corr}\) approximation for that variance. For these reasons, only noise
variance \(\sigma = 2\) from Theorem~\ref{dp_barker_theorem} is used, as was
done by Heikkilä et al.~\cite{HeikkilaJDH19}.

\section{The Penalty Algorithm with Subsampling}\label{dp_minibatch_penalty_section}

In the DP Barker algorithm, the log-likelihood ratio is computed using only
a subsample of the dataset, which amplifies privacy. Subsampling can also be used
with the penalty algorithm in the same way, if the acceptance test is 
corrected for the error from subsampling.

As with DP Barker, the error from subsampling is approximately normally 
distributed by the central limit theorem. The variance of the subsampling 
error can be estimated from the sample variance of individual terms of the 
sum in the log-likelihood ratio. This means that the penalty method can be used
to correct for the subsampling error.

The acceptance probability with subsampling is
\[
    \min\{1, e^{\lambda^*(\theta, \theta') - \frac{1}{2}(\sigma^2 + \sigma_b^2)}\},
\]
where
\[
    \lambda^*(\theta, \theta') = \frac{nT}{b}\sum_{j\in B}
    \ln \frac{p(x_j\mid \theta')}{p(x_j \mid \theta)}
    + \ln \frac{p(\theta')q(\theta \mid \theta')}{p(\theta)q(\theta' \mid \theta)},
\]
\(T\) is the tempering multiplier
and \(\sigma_b^2\) is the sample variance of the log-likelihood ratios in
batch \(B\). Denote
\[
    r_j = \ln \frac{p(x_j\mid \theta')}{p(x_j\mid \theta)},
\]
\[
    R = \sum_{x\in B}r_j.
\]
Then \(\sigma^2_b\) can be estimated from the sample variance of \(r_j\):
\begin{align*}
    \sigma_b^2 
    &= \var\left(\frac{nT}{b}\sum_{j\in B}r_j\right)
    = \frac{nT^2}{b^2}\sum_{j\in B}\var(r_j)
    = \frac{nT^2}{b}\var(r_j)
  \\&\approx\frac{(nT)^2}{b^2}
    \sum_{j\in B}\left(r_j - \frac{R}{b}\right)^2
    = \frac{(nT)^2}{b^2}\left(\sum_{j\in B}r_j^2 - \frac{R^2}{b}\right).
\end{align*}

Because \(\sigma_b^2\) depends on the data, releasing \(\lambda\) privately is 
not enough, \(\lambda - \frac{1}{2}\sigma_b^2\) must be released privately.
This means that using subsampling requires adding additional noise to account 
for the sensitivity of \(\frac{1}{2}\sigma_b^2\).

The sensitivity of \(\lambda - \frac{1}{2}\sigma_b^2\) is 
\[
    \Delta \lambda + \frac{1}{2}\Delta \sigma_b^2.
\]
With the bound \(r_j \leq L||\theta - \theta'||_2\) used in DP penalty, the 
bound sensitivity of \(\lambda\) is the same as without subsampling.
The sensitivity of \(\sigma_b^2\) must be bounded separately.
\begin{lemma}\label{variance_sensitivity_lemma}
    The sensitivity of \(\frac{1}{2}\sigma_b^2\), 
    when \(r_j \leq L||\theta - \theta'||_2\), has upper bound
    \begin{align*}
        \frac{1}{2}\Delta \sigma_b^2
        &\leq \left(\frac{nT}{b}\right)^2 \left|1 - \frac{1}{b}\right|
        L^2||\theta - \theta'||_2^2
        + \frac{2(b - 1)}{b}\left(\frac{nT}{b}\right)^2 L^2||\theta - \theta'||^2_2.
    \end{align*}
\end{lemma}
\begin{proof}
    For substitute neighboring datasets \(X \sim X'\), denote the
    common part they have by \(X^*\), and the differing element by \(x\in X\) 
    and \(x'\in X'\). Then
    \begin{align*}
        \Delta \sigma^2_b &= \sup_{D\sim D'} |\sigma^2_b(X) - \sigma^2_b(X')|
        \\&= \left(\frac{nT}{b}\right)^2 \sup_{X\sim X'}\Big|
        \sum_{x\in X}r^2(x) - \sum_{x\in X'}r^2(x)
        + \frac{1}{b}R^2(X') - \frac{1}{b}R^2(X)\Big|
        \\&= \left(\frac{nT}{b}\right)^2 \sup_{x, x', X^*}\Big|
        r^2(x) - r^2(x')
        + \frac{1}{b}(R(X^*) + r(x'))^2 - \frac{1}{b}(R(X^*) + r(x))^2\Big|
        \\&= \left(\frac{nT}{b}\right)^2 \sup_{x, x', X^*}\Big|
        r^2(x) - r^2(x')
        + \frac{1}{b}(R^2(X^*) + 2R(X^*)r(x') + r^2(x'))
        \\&- \frac{1}{b}(R^2(X^*) + 2R(X^*)r(x) + r^2(x))\Big|
        \\&= \left(\frac{nT}{b}\right)^2 \sup_{x, x', X^*}\Big|
        \left(1 - \frac{1}{b}\right)(r^2(x) - r^2(x'))
        + \frac{2}{b}D(X^*)(r(x') - r(x))\Bigg|
        \\&\leq \left(\frac{nT}{b}\right)^2 \left|1 - \frac{1}{b}\right|
        \sup_{x, x'}\Big|(r^2(x) - r^2(x'))\Big|
        + \frac{2}{b}\left(\frac{nT}{b}\right)^2 \sup_{x, x', X^*}\Big|
        R(X^*)(r(x') - r(x))\Bigg|
        \\&= \left(\frac{nT}{b}\right)^2 \left|1 - \frac{1}{b}\right|
        \sup_{x, x'}\Big|(r^2(x) - r^2(x'))\Big|
        + \frac{2}{b}\left(\frac{nT}{b}\right)^2
        \sup_{x, x'}|r(x') - r(x)|
        \sup_{X^*}|R(X^*)|
        \\&\leq \left(\frac{nT}{b}\right)^2 \left|1 - \frac{1}{b}\right|
        \sup_{x, x'}\Big|(r^2(x) - r^2(x'))\Big|
        + \frac{2}{b}\left(\frac{nT}{b}\right)^2
        \sup_{x, x'}|r(x') - r(x)|(b - 1)\sup_{d}|r(x)|.
    \end{align*}
    Plugging the bound \(\sup_x |r(x)| \leq L||\theta - \theta'||_2\) 
    into the last expression proves the claim.
\end{proof}

\begin{theorem}\label{dp_penalty_minibatch_theorem}
  Let \(\epsilon > 0\), \(0 \leq \delta < 1\), \(\tau > 0\) \(T > 0\),
  \(L > 0\), \(b > 0\), \(n > 0\),
  \[
    \Delta_\lambda = \frac{2nTL}{b}||\theta - \theta'||_2,
  \]
  \[
    \Delta_\sigma = \left(\frac{nT}{b}\right)^2 \left|1 - \frac{1}{b}\right|
    L^2||\theta - \theta'||_2^2,
    + \frac{2(b - 1)}{b}\left(\frac{nT}{b}\right)^2 L^2||\theta - \theta'||^2_2,
  \]
  \[
    c(\theta, \theta') = \Delta_\lambda + \Delta_\sigma,
  \]
  \[
    \sigma^2(\theta, \theta') = \tau c^2(\theta, \theta').
  \]
  Then running minibatch DP penalty with subsample size \(b\) for a dataset
  size \(n\) for \(k\) iterations
  is \((\alpha, k\epsilon(\alpha))\)-RDP for the substitute neighborhood
  relation, with
  \[
    \epsilon(\alpha) = \frac{1}{\alpha - 1}\ln \left(
      1 + q^2\binom{\alpha}{2}\min\{4(e^{\epsilon'(2)} - 1), 2e^{\epsilon'(2)}\}
      + 2 \sum_{j=3}^\alpha q^j\binom{\alpha}{j}e^{(j-1)\epsilon'(j)}\right),
  \]
  \[
    \epsilon'(\alpha) = \frac{\alpha}{2\tau},
  \]
  and \(q = \frac{b}{n}\).
\end{theorem}
\begin{proof}
    By Lemma~\ref{variance_sensitivity_lemma}, 
    \(\Delta_\sigma(\theta, \theta')\) an upper 
    bound to the sensitivity of \(\frac{1}{2}\sigma_b^2\), therefore 
    \(c(\theta, \theta')\) is an upper bound to the sensitivity of 
    \(\lambda - \frac{1}{2}\sigma_b^2\).

    This means that a Gaussian mechanism taking a subsample \(B\) of the data as 
    input and uses \(\sigma(\theta, \theta')\) as the noise variance 
    is \((\alpha, \epsilon'(\alpha))\)-RDP with 
    \[
        \epsilon'(\alpha) = \frac{\alpha}{2\tau}.
    \]
    By the subsampling amplification theorem~\cite[Theorem 9]{WangBK19} and 
    the composition theorem of RDP (Theorem~\ref{composition-theorem}),
    the combination of subsampling and Gaussian mechanism is 
    \((\alpha, k\epsilon(\alpha))\)-RDP with 
    \[
        \epsilon(\alpha) = \frac{1}{\alpha - 1}\ln \left(
        1 + q^2\binom{\alpha}{2}\min\{4(e^{\epsilon'(2)} - 1), 2e^{\epsilon'(2)}\}
        + 2 \sum_{j=3}^\alpha q^j\binom{\alpha}{j}e^{(j-1)\epsilon'(j)}\right)
    \]
    when run for \(k\) iterations for integer \(\alpha \geq 2\).
\end{proof}

Theorem~\ref{dp_penalty_minibatch_theorem} does not give tight bounds for ADP,
as it is based on RDP. Computing tight ADP bounds for the minibatch penalty
algorithm requires an analogue of the ADP bound of Theorem~\ref{gauss-DP-bounds}
for the Gaussian mechanism with subsampling. An analytical and tight ADP bound for
the subsampled Gaussian mechanism is not known, but the \emph{Fourier accountant}
of Koskela et al.~\cite{KJH20} can approximate the tight bound with a very small
error.

The Fourier accountant takes the subsampling ratio \(q\) and noise variance,
and computes the ADP bounds of the subsampled Gaussian mechanism with sensitivity
1. For the minibatch penalty algorithm, the released value
\(\lambda - \frac{1}{2}\sigma_{b}^{2}\) has sensitivity \(c(\theta, \theta')\),
and noise variance \(\tau c^{2}(\theta, \theta')\), but if the released value is
normalised to have sensitivity 1, the noise variance becomes \(\tau\), which makes
the Fourier accountant easy to apply to the minibatch penalty algorithm.
As with the DP penalty algorithm, the maximum number of iterations
for minibatch DP penalty can be computed using Algorithm~\ref{max_iterations_algo}
by computing values of \(\delta\) in Algorithm~\ref{max_iterations_algo}
with the Fourier accountant\footnote{
  The Fourier accountant handles the non-integer iteration numbers used
  by Algorithm~\ref{max_iterations_algo} by internally
  rounding them down.
}.

The DP Barker algorithm, like minibatch penalty, uses RDP to compute privacy
bounds. Using the Fourier accountant with DP Barker may be possible, but it is
not as simple as with minibatch penalty, as DP Barker does not consider
releasing the sample variance \(\sigma_{b}^{2}\) directly.

The appearance of \(n\) as a multiplier of \(\Delta_\lambda\) and 
\(n^2\) as a multiplier of \(\Delta_\sigma\) causes the noise variance to increase 
with \(n\), without a corresponding decrease in \(\epsilon'\). This makes subsampled 
DP penalty unsuitable for problems with large datasets, unless tempering is 
used. Like with DP Barker, using tempering \(T = \frac{n_0}{n}\) cancels \(n\)
out of the noise variance.

The minibatch DP penalty algorithm has the same parameters as DP penalty, with
the addition of the batch size parameter. The restrictions on the noise variance
of DP Barker are not present in minibatch DP penalty. In
Theorem~\ref{dp_penalty_minibatch_theorem}, the expression for \(\sigma\) was
simplified by multiplying \(c^{2}\) by only a single number \(\tau\), instead
of the \(\tau^{2}n^{2\alpha}\) used in Theorem~\ref{DP_penalty_theorem_zcdp}
after Yildirim and Ermis~\cite{YildirimE19}. This makes using the Fourier
accountant particularly simple, as the noise variance given to it is simply
\(\tau\).

The proposed variations of the penalty algorithm, one component updates and guided
walk MH, are also applicable with subsampling. The sensitivity reduction from
OCU may be especially useful, as the variance sensitivity contains the square of
the distance between the current and proposed parameter values, so reducing the
distance by updating only a single component can greatly reduce the added noise.

\chapter{Differentially Private Hamiltonian Monte Carlo}\label{hmc_chapter}

Hamiltonian Monte Carlo (HMC\footnote{
    HMC originally stood for hybrid Monte Carlo, 
    but the name Hamiltonian Monte Carlo has become more common.
})~\cite{DKP87, neal2012mcmc} is a widely used MCMC algorithm that, like
Metropolis-Hastings and the penalty algorithm, was originally developed 
for simulations in physics, but has since been applied to
Bayesian inference. 

HMC simulates the trajectory of a moving particle in a way 
that allows it to draw samples from a target distribution~\cite{neal2012mcmc}. 
With perfect simulation,
an acceptance test would not be needed, but exact simulation of the moving particle 
is typically not possible, so a Metropolis-Hastings acceptance test is used
for proposals generated by the simulation. Despite imperfections, the simulation can generate long jumps
that stay in areas of high probability, giving HMC a higher acceptance rate 
than Metropolis-Hastings using a Gaussian distribution as the proposal, at the
cost of higher computational requirements.

This chapter first introduces HMC in the non-DP setting in Section~\ref{hmc_basics_section}.
In Section~\ref{dp_hmc_section} HMC is made differentially private using the
acceptance test of the penalty algorithm and adding noise to the proposal phase 
appropriately.

\section{MCMC with Hamiltonian Dynamics}\label{hmc_basics_section}

The motion of a particle on a frictionless surface of varying height is governed 
by the initial position and velocity of the particle, and the height of the 
surface at different positions~\cite{neal2012mcmc}. 
The kinetic energy of the particle with 
momentum\footnote{Momentum is velocity times mass.}
\(p \in \R^{2}\) and mass \(m \in \R\) is \(\frac{p^{T}p}{2m}\). The potential energy of the particle
at position \(\theta \in \R^{2}\) is proportional to the height of the surface, given by
a continuously differentiable function \(U\).
The sum of potential and kinetic energies, the total 
energy of the system, is called the Hamiltonian \(H\). Hamilton's equations 
give the equations of motion for a system with a given Hamiltonian:
\begin{align*}
    &\frac{d\theta}{dt} = \frac{\partial H}{\partial p},
    &\frac{dp}{dt} = -\frac{\partial H}{\partial \theta}.
\end{align*}
Given \(H(\theta, p) = U(\theta) + \frac{p^{T}p}{2m}\), Hamilton's equations become
\begin{align*}
    \frac{d\theta}{dt} &= \frac{p}{m}, \\
    \frac{dp}{dt} &= -\nabla U(\theta).
\end{align*}

These equations define a function \(T_t\) taking an initial state 
\((\theta, p)\) of the particle to the state \(T(\theta, p)\) after time \(t\).
Solving \(T_t\) analytically is usually not possible, but \(T_t\) can be 
approximately simulated by the \emph{leapfrog method}. The leapfrog method sequentially
updates an estimate \((\theta_i, p_i)\) of the state in steps \(L\) of \(\eta\) 
as follows:
\begin{align*}
    p_{i+1/2} &= p_i - \frac{\eta}{2}\nabla U(\theta_i), \\
    \theta_{i+1} &= \theta_i + \frac{\eta p_{i+1/2}}{m}, \\
    p_{i+1} &= p_{i+1/2} - \frac{\eta}{2}\nabla U(\theta_{i+1}). \\
\end{align*}
The simulation starts with \(\theta_0 = \theta\) and \(p_0 = p\) and the 
result is given by \(T_{L\eta}(\theta, p) \approx (\theta_L, p_L)\).

The above equations for two-dimensional \(\theta\) and \(p\) generalize to
any number of dimensions \(d\)~\cite{neal2012mcmc}. The physical analogue would be a particle moving
on a \(d\)-dimensional hypersurface with ``height'' given by \(U(\theta)\).
The mass of the particle, \(m\), can also be generalized to any positive
definite matrix \(M \in \R^{d\times d}\), which changes division by
\(m\) to multiplication by \(M^{-1}\). Specifically, the kinetic energy of the
particle becomes \(\frac{1}{2}p^{T}M^{-1}p\) and
\(\frac{d\theta}{dt} = M^{-1}p\). The generalization to a mass matrix does not
have a physical analogue, but it can be useful for MCMC.

Hamiltonian dynamics have three important properties~\cite{neal2012mcmc} 
that make them useful as a
proposal mechanism for MCMC. They are \emph{invertibilityi}\footnote{
  Invertibility is commonly called reversibility~\cite{neal2012mcmc}, but
  calling it reversibility in this thesis risks conflating it with
  the closely related concept of Markov kernel reversibility from
  Definition~\ref{reversible_definition}.
},
\emph{conservation of the Hamiltonian} and
\emph{conservation of the Lebesgue measure}\footnote{Also
  known as conservation of volume.
}.

Invertibility means that the function \(T_t\) is injective, meaning that
it has an inverse \(T_{-t}\) where \(T_t(\theta_1, p_1) = (\theta_2, p_2)\)
implies that \(T_{-t}(\theta_2, p_2) = (\theta_1, p_1)\)~\cite{neal2012mcmc}.
For the Hamiltonian of the particle moving on a surface, the inverse 
is obtained by \(T_t(\theta_2, -p_2) = (\theta_1, -p_1)\).

Conservation of the Hamiltonian follows from Hamilton's equations and the 
chain rule of derivatives:
\[
    \frac{dH}{dt} = \frac{\partial H}{\partial \theta}\frac{d \theta}{dt}
    + \frac{\partial H}{\partial p}\frac{dp}{dt}
    = \frac{\partial H}{\partial \theta}\frac{\partial H}{\partial p}
    - \frac{\partial H}{\partial p}\frac{\partial H}{\partial \theta}
    = 0.
\]

Conservation of the Lebesgue measure in the \((\theta, p)\)-space follows
from the fact that 
the absolute value of the Jacobian determinant of \(T_t\) is 1 and the 
injectivity of \(T_t\). A somewhat 
lengthy proof for the Jacobian determinant of \(T_t\) is given by
Neal~\cite{neal2012mcmc}. 

To see why 
having \(|\det J_f| = 1\), where \(J_f\) is the Jacobian matrix of an injective 
continuously differentiable function \(f\), 
implies \(f\) preserving the Lebesgue measure, recall that the volume of a set
\(A\) is given by 
\[
    \int_A 1\dx x.
\]
The volume of the image of \(A\) under \(f\), \(f(A)\), is given by
\[
    \int_{f(A)}1\dx x = \int_{A}|\det J_f(x)|\dx x = \int_A 1\dx x.
\]
where the first equality follows from the change of variables formula.

The leapfrog method does not conserve the Hamiltonian exactly, but it 
preserves Lebesgue measure and is invertible exactly~\cite{neal2012mcmc}.
These properties are important for its use as a proposal mechanism for MCMC.

The HMC algorithm samples from the distribution of~\cite{neal2012mcmc}
\[
    \pi(\theta, p) \propto \exp(-H(\theta, p)).
\]
With \(H(\theta, p) = U(\theta) + \frac{1}{2}p^{T}M^{-1}p\),
\[
    \pi(\theta, p) \propto \exp(-U(\theta))\exp\left(\frac{1}{2}p^{T}M^{-1}p\right)
\]
This means that the marginal distributions of \(\theta\) and \(p\) are 
independent, the marginal distribution of \(p\) is a \(d\)-dimensional
Gaussian distribution 
with mean 0 and covariance \(M\), and the marginal distribution of
\(\theta\) can have any continuously differentiable log-density \(-U\).
Because of this, \(\theta\) is used to represent the variables of interest,
while \(p\) is a set of auxiliary variables used for the proposals.
In Bayesian inference,
\[
  U(\theta) = -\sum_{x\in X}\ln p(x\mid \theta) - \ln p(\theta)
\]
for dataset \(X\). The requirement that \(U\) is a continuously differentiable
and defined on \(\R^{d}\) means that the log-likelihood \(\ln p(x\mid \cdot)\)
and the log-prior must also meet these conditions. This means that HMC cannot directly
be used with discrete parameters, non-differentiable log-likelihoods or log-priors,
or constrained parameters, such as variance
parameters that must be positive. Constrained parameters can typically be
transformed into unconstrained ones, such as transforming \(\sigma = e^{s}\)
for a variance parameter \(\sigma^{2}\), and using \(s\) in the model instead
of \(\sigma^{2}\). Discrete parameters or non-differentiable models
require other MCMC algorithms, although it may be possible to combine them
with HMC if there are continuous parameters.

Generating a \((\theta, p)\)-sample is done in two stages~\cite{neal2012mcmc}, 
which are technically 
two separate Metropolis-Hastings proposals and acceptance tests. In the first 
stage, \(p\) is proposed from the Gaussian distribution with mean 0 and 
covariance \(M\). Because the proposal matches the marginal distribution of 
\(p\), the Metropolis-Hastings acceptance probability is 1, so the proposal
is always accepted.

In the second stage, Hamiltonian dynamics for the particle on a surface are 
simulated with the leapfrog method, starting from the current value 
of \(\theta\) and value of \(p\) from the previous stage. After the simulation,
\(p\) is negated, and the resulting \((\theta, p)\) pair is used as the
proposed value.

Because the proposal is deterministic, the Markov kernel for the
proposal is \(\delta_{f(\theta, p)}\), where \(f(\theta, p)\) denotes the proposal
of running the leapfrog simulation starting with \((\theta, p)\) and negating
\(p\) after the simulation. The invertibility of the leapfrog simulation means
that \(f = f^{-1}\). Together with the measure preservation of the leapfrog
simulation, this means that \(f\) meets the conditions of
Lemma~\ref{dirac_measure_reversible_lemma}. Combining this with
Lemma~\ref{reversible_proposal_lemma} means that \(\pi\) is the invariant
distribution of HMC when the acceptance probability is simply
\[
  \min\left\{1, \frac{\pi(\theta', p')}{\pi(\theta, p)}\right\}
  = \min\{1, \exp(H(\theta, p) - H(\theta', p'))\}.
\]

Because of the deterministic proposal, the ergodicity of HMC cannot be
proved with strong irreducibility, but it has been shown that HMC is
ergodic under some conditions on \(U\)~\cite{DMS19}.

If the simulation conserved \(H\) exactly, the acceptance probability would 
always be 1. Because this is usually not possible, there is always some probability 
of rejecting, which depends on the length of the jump taken by the leapfrog 
method, \(\eta\), and the number of leapfrog steps, \(L\). It might be expected 
that the errors from the leapfrog steps would accumulate during the simulation, 
but in practice the errors in different steps tend to cancel each 
other~\cite{neal2012mcmc}, meaning 
that the acceptance probability mainly depends on \(\eta\).

\section{Differential Privacy with HMC}\label{dp_hmc_section}

HMC only uses the model log-likelihood, and thus the data, through \(U\).
\(U\) is used in two ways. Firstly, its value is used in the acceptance test,
and secondly, its gradients are used to obtain proposals. Adding appropriate
amounts of noise to both accesses would make HMC differentially private, but the addition of
noise may break some of the properties required for the algorithm's correctness.

The addition of noise to the log-likelihood ratios can be corrected using the penalty
acceptance test, i.e. changing the acceptance probability to
\[
    \min\left\{\exp\left(H(\theta, p) - H(\theta', p') - \frac{1}{2}\sigma_{l}^{2}\right)\right\},
\]
where \(\sigma_{l}^{2}\) is the variance of the noise added to the log-likelihood
ratios. This is because with
\(U(\theta) = -\ln p(X\mid \theta) - \ln p(\theta)\),  for data \(X\),
the difference of the Hamiltonians in the acceptance probability is
\[
  H(\theta, p) - H(\theta', p') = \ln p(X\mid \theta') - \ln p(X\mid \theta)
  - \ln p(\theta) + \ln p(\theta')
  + \frac{1}{2}(p^{T}M^{-1}p - p'^{T}M^{-1}p').
\]
As with the penalty algorithm, the sensitivity of the log-likelihood ratios must
be bounded. If a bound does not exist, clipping must be used, nullifying the asymptotic
converge guarantee of the algorithm. However, in Section~\ref{clipping_experiments}
it is shown that
clipping low numbers of gradients does not affect convergence in practice.

Releasing the gradients privately requires a trade-off. The leapfrog method remains
reversible with noisy and potentially clipped gradients, as is shown in the following,
but it no longer simulates the correct Hamiltonian dynamics~\cite{CFG14}. This does not affect
asymptotic convergence, but it can potentially lower acceptance rates.
The dynamics can be corrected by adding a friction term to the equations of
motion of the simulation~\cite{CFG14}, but this removes volume preservation of the simulation,
which means that the acceptance probability is no longer valid.

The leapfrog update equations with noisy and clipped gradients become
\begin{align*}
  p_{i+1/2} &= p_{i} - \frac{\eta}{2}(g(\theta_{i}) + \caln(0, \sigma_{g}^{2})) \\
  \theta_{i+1} &= \theta_{i} + \eta M^{-1}p_{i+1/2} \\
  p_{i+1} &= p_{i+1/2} - \frac{\eta}{2}(g(\theta_{i+1}) + \caln(0, \sigma_{g}^{2})),
\end{align*}
where
\[
  g(\theta) = \sum_{x\in X}\clip_{b}(\nabla \ln p(x\mid \theta)) + \nabla\ln p(\theta)
\]
denotes the clipped gradients summed over the data \(X\).

Using the leapfrog update equations as is results in unnecessary evaluations
of the gradients, as the last momentum update of a step and the first momentum
update of the next step use the same gradients. Computing the same gradients
and incurring their privacy cost
twice can be avoided by writing the leapfrog simulation as
\[
  l_{p_{1/2}}\circ l_{\theta}\circ l_{p_{1}}\circ l_{\theta}\dotsb
  l_{\theta}\circ l_{p_{1}}\circ l_{\theta}\circ l_{p_{1/2}}
\]
where
\[
  l_{\theta}(\theta, p) = (\theta + \eta M^{-1}p, p)
\]
and
\[
  l_{p_{x}}(\theta, p) = (\theta, p - \eta x(g(\theta) + \caln(0, \sigma_{g}^2))).
\]

Both \(l_{\theta}\) and \(l_{p_{x}}\) preserve measure, as \(l_{\theta}\) in
unchanged from the non-DP case and the Jacobian of \(l_{p_{x}}\) is
\[
  J_{l_{p_{x}}}(\theta, p) =
  \begin{bmatrix}
    I_{d\times d} & 0_{d\times d} \\
    \eta xJ_{g}(\theta) & I_{d\times d}
  \end{bmatrix},
\]
where \(I_{d\times d}\) and \(0_{d\times d}\) are the \(d\)-by-\(d\) identity and
zero matrices, respectively.
It is possible that \(J_{g}(\theta)\) does not exist, but this can only happen
in a null set\footnote{A set of zero Lebesgue measure.}, if \(g\) is
sufficiently well-behaved. The non-existence of \(J_{g}(\theta)\) in a null set
does not affect the volume preservation of \(l_{p_{x}}\).
As \(g\) is a sum of clipped log-likelihood
gradients and the log prior gradient, it suffices to show that the
log-likelihood is sufficiently well-behaved, as the prior is already assumed
to be differentiable and is not clipped. Appendix~\ref{clip_diff_chapter}
gives very general sufficient conditions for the log-likelihood, and proves
that all of the models considered in Chapter~\ref{experiment_setup_chapter} meet
the conditions.

Because \(J_{l_{p_{x}}}(\theta, p)\) is triangular, \(\det J_{l_{p_{x}}}(\theta, p) = 1\)
for all \(\theta\) and \(p\), which means that \(l_{p_{x}}\) preserves volume.
As the step updating \(\theta\) also preserves volume, the entire leapfrog
simulation preserves Lebesgue measure. Showing that the proposal of
DP-HMC is reversible
with respect to the Lebesgue measure requires more thought, as the individual
leapfrog steps are partially deterministic.

With the momentum negation after the leapfrog simulation, the DP-HMC proposal
can be written as
\[
  l_{-}\circ l_{p_{1/2}}\circ l_{\theta}\circ l_{p_{1}}\circ l_{\theta}\dotsb
  l_{\theta}\circ l_{p_{1}}\circ l_{\theta}\circ l_{p_{1/2}}
\]
where \(l_{-}(\theta, p) = (\theta, -p)\). Reversibility of the proposal
with respect to the Lebesgue measure can then shown by
decomposing the sequence into parts that are individually reversible.

\begin{lemma}\label{leapfrog_step_reversible_lemma}
  \(l_{\theta}\circ l_{-}\) and \(l_{-}\circ l_{p_{x}}\) for any \(x\in \R\)
  are reversible with respect to the Lebesgue measure.
\end{lemma}
\begin{proof}
  Starting with \(l_{\theta}\circ l_{l}\), denote
  \(f(\theta, p) = (\theta - \eta M^{-1}p, -p)\). Then
  \[
    f(f(\theta, p)) = f(\theta - \eta M^{-1}p, -p) = (\theta, p)
  \]
  which means that \(f^{-1} = f\). Because \(l_{\theta}\) and \(l_{-}\) preserve
  Lebesgue measure, \(f\) also preserves Lebesgue measure.
  Then the Markov kernel for \(l_{\theta}\circ l_{-}\) is \(\delta_{f(\theta, p)}\),
  so Lemma~\ref{dirac_measure_reversible_lemma} proves the claim for
  \(l_{\theta}\circ l_{l}\).

  For \(l_{-}\circ l_{p_{x}}\), denote \(f_{x}(\theta) = x\eta g(\theta)\) and
  \(\sigma^{2} = x^{2}\eta^{2}\sigma_{g}^{2}\). Then
  \[
    \int_{B}(l_{-}\circ l_{p_{x}})(\theta, p, \dx(\theta', p')) =
    \int_{B}\delta_{\theta}(\dx \theta')\dx p'\caln(p'\mid -p + f_{x}(\theta), \sigma^{2})
  \]
  for \(B\in \mathcal{B}(\R^{2d})\).
  Using Lemma~\ref{dirac_measure_reversible_lemma},
  for any \(A, B\in \mathcal{B}(\R^{2d})\),
	\begin{align*}
    \int_{A}\dx(\theta, p)\int_{B}(l_{-}\circ l_{p_{x}})(\theta, p, \dx(\theta', p'))
    &= \int_{A}\dx(\theta, p)\int_{B}\delta_{\theta}(\dx \theta')\dx p'
      \caln(p'\mid -p + f_{x}(\theta), \sigma^{2})
    \\&= \int_{B}\dx(\theta', p')\int_{A}\delta_{\theta'}(\dx \theta)\dx p
    \caln(p\mid -p' + f_{x}(\theta'), \sigma^{2})
    \\&= \int_{B}\dx(\theta', p')\int_{A}(l_{-}\circ l_{p_{x}})(\theta', p', \dx(\theta, p))
    \qedhere
  \end{align*}
\end{proof}

\begin{lemma}\label{leapfrog_reversibility_lemma}
	The DP-HMC proposal \(l_{-} \circ l\) is reversible with respect
  to the Lebesque measure.
\end{lemma}
\begin{proof}
  The DP-HMC proposal
  \[
    l_{-}\circ l_{p_{1/2}}\circ l_{\theta}\circ l_{p_{1}}\circ l_{\theta}
    \dotsb \circ l_{\theta}\circ l_{p_{1/2}}
  \]
  can be written as
  \[
    (l_{-}\circ l_{p_{1/2}})\circ (l_{\theta} \circ l_{-})\circ (l_{-}\circ l_{p_{1}})
    \circ (l_{\theta} \circ l_{-}) \circ (l_{-} \circ l_{p_{1}})\circ
    \dotsb \circ (l_{\theta}\circ l_{-})\circ (l_{-}\circ l_{p_{1/2}})
  \]
  which is a composition of \(l_{-}\circ l_{p_{1/2}}\), \(l_{-}\circ l_{p_{1}}\)
  and \(l_{\theta}\circ l_{-}\), which are all reversible with respect to the
  Lebesgue measure by Lemma~\ref{leapfrog_step_reversible_lemma}.
  As the composition is symmetric,
  Lemma~\ref{composition_reversible_symmetric_lemma} extends the reversibility
  with respect to the Lebesgue measure to the entire composition.
\end{proof}

\begin{theorem}
  If
  \[
    \min\left\{1, \exp\left(H(\theta, p)
        - H(\theta', p') - \frac{1}{2}\sigma_{l}^{2}\right)\right\}
  \]
  is used as the acceptance probability for DP-HMC, the invariant distribution
  is \(\pi\).
\end{theorem}
\begin{proof}
Lemmas~\ref{reversible_proposal_lemma} and \ref{leapfrog_reversibility_lemma}
together show that when using
\[
  \alpha(\theta, p, \theta', p')
  = \min\left\{1, \frac{\pi(\theta', p')}{\pi(\theta, p)}\right\}
\]
as the acceptance probability, the invariant distribution is \(\pi\).
Recall that in HMC
\[
  \pi(\theta, p) \propto \exp(-H(\theta, p))
\]
so
\[
  \alpha(\theta, p, \theta', p')
  = \min\left\{1, \exp\left(H(\theta, p) - H(\theta', p')\right)\right\}.
\]
When Gaussian noise with variance \(\sigma_{l}^{2}\) is added to the
log-likelihood ratios and the penalty correction is applied,
  \[
    \alpha(\theta, p, \theta', p')
    = \min\left\{1, \exp\left(H(\theta, p)
        - H(\theta', p') - \frac{1}{2}\sigma_{l}^{2}\right)\right\}.
    \qedhere
  \]
\end{proof}

Proving the ergodicity of DP-HMC turns out to be easier than in the
non-DP case as a result of adding noise to the proposal. Consider the
last four updates of the proposal,
\[
  l_{-}\circ l_{p_{1/2}}\circ l_{\theta}\circ l_{p_{1}}.
\]
Denote the values of \((\theta, p)\) before the last four updates by \((\theta^{*}, p^{*})\) and
the values after the updates by \((\theta', p')\).
The first of these updates adds Gaussian noise to \(p\). In the next
\(l_{\theta}\) step, the previous value of \(p\) is added to \(\theta\)
after multiplication with a non-singular matrix, and the further updates
do not change \(\theta\), so it is possible
to obtain any value for \(\theta'\), no matter the starting value of
\((\theta^{*}, p^{*})\). In the next \(l_{p_{1/2}}\) update, Gaussian noise is
added to the value of \(p\) and the \(l_{-}\) update does not change the distribution
of \(p\), so it is possible to obtain any value
for \(p'\) from any \((\theta^{*}, p^{*})\). This means that
DP-HMC is in fact strongly irreducible, and thus ergodic.

If the friction term that corrects the dynamics is used, the equations of
motion become~\cite{CFG14}
\begin{align*}
  \frac{d\theta}{dt} &= M^{-1}p \\
  \frac{dp}{dt} &= -\nabla U(\theta) - \frac{1}{2}\sigma_{g}^{2}M^{-1}p + \caln(0, \sigma_{g}^{2})
\end{align*}
The equation for \(\theta\) is unchanged, but the equation for \(p\) has the
additional term \(-\frac{1}{2}\sigma_{g}^{2}M^{-1}p\).
For the leapfrog updates, \(l_{\theta}\) does not change, and \(l_{p}\) changes
to
\[
  l_{p}(\theta, p) = \left(\theta, p - \frac{\eta}{2}(g(\theta)
  + \frac{1}{2}\sigma_{g}^{2}M^{-1}p + \caln(0, \sigma_{g}^{2}))\right).
\]
\(l_{p}\) no longer preserves volumes, as the Jacobian of \(l_{p}\) is
\[
  J_{l_{p}}(\theta, p) =
  \begin{bmatrix}
    I_{d\times d} & 0_{d\times d} \\
    J_{g}(\theta) & I_{d\times d} - \frac{\eta}{4}\sigma_{g}^{2}M^{-1}
  \end{bmatrix}
\]
which means that generally \(|\det J_{l_{p}}(\theta, p)| \neq 1\).
This means that the acceptance test is no longer correct, so
the friction term in not used in this thesis beyond this brief discussion,
but finding a way to correct the acceptance test is a potential avenue for
future research.

\begin{algorithm}[H]
    \SetAlgoLined
    \(G_{1,0} = g_{b_{g}}(\theta_{0}) + \caln(0, \sigma_{g}^{2})\)\\
    \For{\(1 \leq i \leq k\)}{
        \(p_0 = \caln_d(0, M)\)\\
        \(\theta_{i,0} = \theta_{i-1}\)\\
        \For{\(1 \leq j \leq L\)}{
            \(p_{j-\frac{1}{2}} = p_{j-1} + \frac{1}{2}\eta G_{i,j-1} \)\\
            \(\theta_{i,j} = \theta_{i,j-1} + M^{-1}p_{j-\frac{1}{2}}\)\\
            \(G_{i,j} = g_{b_{g}}(\theta_{i,j}) + \caln(0, \sigma_{g}^{2})\)\\
            \(p_{j} = p_{j-\frac{1}{2}} + \frac{1}{2}\eta G_{i,j} \)\\
        }
        \(r_l = \ln \frac{p(\theta_{i,L}\mid x_l)}{p(\theta_{i-1}\mid x_l)}\)\\
        \(R = \sum_{l=1}^n \mathrm{clip}_{b_l||\theta_{i-1} - \theta_{i,L}||_2}(r_l)\)\\
        % \(s_{var} = (2\sigma_lb_l||\theta_{i-1} - \theta_{i,L}||_2)^2\)\\
        \(\Delta H = R + \frac{1}{2}p_0^TM^{-1}p_0 - \frac{1}{2}p_L^TM^{-1}p_L
        + \ln \frac{p(\theta_{i,L})}{p(\theta_{i-1})}
        + \caln(0, \sigma_{l}^{2}(\theta_{i-1}, \theta_{i,L}))\)\\
        \(u = \mathrm{Unif}(0, 1)\)\\
        \If{\(u < \Delta H - \frac{1}{2}\sigma_{l}^{2}(\theta_{i-1}, \theta_{i,L})\)}{
            \(\theta_{i} = \theta_{i, L}\)\\
            \(G_{i+1,0} = G_{i,L}\)\\
        }
        \Else{
            \(\theta_i = \theta_{i-1}\)\\
            \(G_{i+1,0} = G_{i,0}\)\\
        }
    }
    \caption{
      DP HMC: log likelihood clip bound \(b_{l}\), gradient clip bound
      \(b_{g}\), clipped gradient function \(g_{b_{g}}\),
      number of iterations \(k\), number of leapfrog steps \(L\),
      leapfrog step size \(\eta\), initial point \(\theta_{0}\),
      gradient noise variance \(\sigma_{g}^{2}\), log likelihood ratio
      noise variance \(\sigma_{l}^{2}\), positive-definite mass matrix \(M\)
      and dataset \(X\) as input.
    }
    \label{dp_hmc_algo}
\end{algorithm}

DP HMC is shown in Algorithm~\ref{dp_hmc_algo}.
Like the DP penalty algorithm, DP HMC is a composition of Gaussian mechanisms,
and its privacy bounds can be computed through Theorems~\ref{composition-theorem}
and \ref{gauss-DP-bounds} for zCDP.

\begin{theorem}\label{dp_hmc_theorem_zcdp}
  Let \(\epsilon \geq 0\), \(0 \leq \delta < 1\), \(\tau_{g} > 0\), \(\tau_{l} > 0\),
  \(b_{g} > 0\), \(b_{l} > 0\) \(n > 0\),
  \[\sigma_l(\theta, \theta') = 2\tau_l\sqrt{n}b_l||\theta - \theta'||_2,\]
  \[\sigma_g = 2\tau_g\sqrt{n}b_g,\]
  \[
    \rho = \left(\sqrt{\epsilon - \ln \delta} - \sqrt{-\ln \delta}\right)^2,
  \]
  \[
    \rho_l = \frac{1}{2\tau_l^2n},
  \]
  \[
    \rho_g = \frac{1}{2\tau_g^2n},
  \]
  Then runnning DP HMC for
  \[
    k = \left\lfloor\frac{\rho - \rho_{g}}{\rho_l + L\rho_g}\right\rfloor
  \]
  iterations using \(\sigma_{l}^{2}\) as log-likelihood ratio noise variance,
  \(\sigma_{g}^{2}\) as gradient noise variance, clipping log-likelihood ratios
  with \(b_{l}\) and clipping gradients by \(b_{g}\), for a dataset of size \(n\)
  is \((\epsilon, \delta)\)-ADP for the substitute neighborhood relation.
\end{theorem}
\begin{proof}
Each iteration of the outer for-loop computes the gradient \(L\) times and
the log-likelihood ratio once. The gradient is also computed once before the
outer for-loop. Releasing a single gradient has zCDP privacy
cost of 
\[
    \rho_g = \frac{4b_g^2}{2\sigma_g^2} = \frac{1}{2\tau_g^2n}.
\]
Releasing the log-likelihood ratio of \(\theta\) and \(\theta'\) has privacy cost
\[
    \rho_l = \frac{4b_l^2||\theta - \theta'||_2^2}{2\sigma_l(\theta, \theta')^2} = \frac{1}{2\tau_l^2n}.
\]

The total zCDP budget with given \((\epsilon, \delta)\)-bound is 
\[
    \rho = \left(\sqrt{\epsilon - \ln \delta} - \sqrt{-\ln \delta}\right)^2.
\]
This means that the number of iteration that the algorithm is allowed to run for 
is 
\[
    k = \left\lfloor\frac{\rho - \rho_{g}}{\rho_l + L\rho_g}\right\rfloor
    \qedhere
\]
\end{proof}

zCDP based privacy accounting for ADP is loose, so the above bound could be improved
by using the tight bound for the Gaussian mechanism from
Theorem~\ref{gauss-DP-bounds}. Because DP HMC has both
different sensitivities and different noise variances between the log-likelihood
ratios and gradients, Theorem~\ref{gauss-DP-bounds} is not directly applicable.
However, the bound of Theorem~\ref{gauss-DP-bounds} does generalise to differing
variances between composition.

\begin{theorem}\label{dp_hmc_theorem_adp}
  Let \(\epsilon \geq 0\), \(0 \leq \delta < 1\), \(\tau_{l} > 0\),
  \(\tau_{g} > 0\), \(b_{l} > 0\), \(b_{g} > 0\), \(n > 0\),
  \[
    \sigma_{l}'^{2} = \tau_{l}^{2}n,
  \]
  \[
    \sigma_{g}'^{2} = \tau_{g}^{2}n.
  \]
  Then running DP HMC for \(k\) iterations with \(L\) leapfrog steps,
  using \(2b_{l}\sigma_{l}'^{2}||\theta - \theta'||_{2}\) as the log-likelihood ratio
  noise variance, \(2b_{g}\sigma_{g}'^{2}\) as the gradient noise variance
  and \(b_{l}\) and \(b_{g}\) as the log-likelihood and gradient clip bounds,
  with dataset size \(n\), is \((\epsilon, \delta(\epsilon))\)-ADP for the
  substitute neighborhood relation, where
  \[
    \delta(\epsilon) = \frac{1}{2}\left(
      \erfc\left(\frac{\epsilon - \mu}{2\sqrt{\mu}}\right)
      - e^{\epsilon}\erfc\left(\frac{\epsilon + \mu}{2\sqrt{\mu}}\right)\right)
  \]
  and
  \[
    \mu = \frac{k}{2\sigma_{l}'^{2}} + \frac{kL + 1}{2\sigma_{g}'^{2}}.
  \]
\end{theorem}
\begin{proof}
Sommer et al.~\cite{Sommer2019} prove the ADP bound of Theorem~\ref{gauss-DP-bounds} with
the \emph{privacy loss distribution} (PLD) of the Gaussian mechanism. First, they
show that the PLD of the Gaussian mechanism with sensitivity \(\Delta\) and variance
\(\sigma^{2}\) is a Gaussian distribution
\(\caln(\mu, 2\mu)\), where \(\mu = \frac{\Delta^{2}}{2\sigma^{2}}\)~\cite[Lemma 11]{Sommer2019}.
Next, they show that the ADP bounds \((\epsilon, \delta(\epsilon))\) for a
Gaussian PLD \(\caln(\mu, 2\mu)\) are given by~\cite[Lemma 12]{Sommer2019}
\[
  \delta(\epsilon) = \frac{1}{2}\left(
    \erfc\left(\frac{\epsilon - \mu}{2\sqrt{\mu}}\right)
    - e^{\epsilon}\erfc\left(\frac{\epsilon + \mu}{2\sqrt{\mu}}\right)\right).
\]
Finally, they show that the PLD of an adaptive composition of Gaussian mechanisms
is the convolution of
the PLDs of the individual parts of the composition~\cite[Theorem 1]{Sommer2019}.
As convolution of distributions corresponds to summing random variables, the PLD of a
convolution of Gaussian mechanisms with PLDs \(\caln(\mu_{i}, 2\mu_{i})\) is
simply \(\caln(\sum \mu_{i}, 2\sum \mu_{i})\).

Because the sensitivity of the log-likelihood ratio is
\(2b_{l}||\theta - \theta'||_{2}\) and the sensitivity of the gradient is
\(2b_{g}\), both the gradient and log-likelihood ratio are divided by their
sensitivities before adding noise, which normalises both to have sensitivity
one. Adding noise with variance \(\sigma_{l}^2(\theta, \theta')\)
and \(\sigma_{g}^{2}\) to the log-likelihood ratio and gradient respectively
is equivalent to adding noise with variance
\(\sigma_{l}'^{2} = \tau_{l}^{2}n\) and \(\sigma_{g}'^{2} = \tau_{g}^{2}n\)
to the normalised log-likelihood ratio and gradient.

DP HMC releases the log-likelihood ratio \(k\) times and the gradient
\(kL + 1\) times. This means that the PLD of DP HMC is a Gaussian distribution
\(\caln(\mu, 2\mu)\) with
\[
  \mu = \frac{k}{2\sigma_{l}'^{2}} + \frac{kL + 1}{2\sigma_{g}'^{2}}.
  \qedhere
\]
\end{proof}

The maximum number of iterations DP HMC can run for can be computed with
Algorithm~\ref{max_iterations_algo} by using Theorem~\ref{dp_hmc_theorem_adp}
to compute \(\delta\), and using Theorem~\ref{dp_hmc_theorem_zcdp}
to initialise the variable \(low\).

The \(\tau_{l}\) and \(b_{l}\) parameters of DP HMC are analogous to the
\(\tau\) and clip bound parameters of DP penalty. Additionally, DP HMC
releases gradients, which requires a separate parameter \(\tau_{g}\) to
control the trade-off between noise and privacy cost of a single iteration,
and a clip bound \(b_{g}\) for the gradients. The number of leapfrog iterations
per sample \(L\) also affects the privacy of the algorithm, as it affects the
number of gradients released.

Non-DP HMC is known to be hard to tune~\cite{neal2012mcmc},
as changes in \(\eta\) can cause large changes in the acceptance probability,
and picking a too large \(L\) can cause the leapfrog simulation to turn
around and propose small jumps. These issues are only amplified for DP HMC,
as 4 new parameters are introduced.

The \(\alpha\) value from Theorem~\ref{DP_penalty_theorem_zcdp} for
DP penalty could be included in Theorem~\ref{dp_hmc_theorem_zcdp},
which would make Theorem~\ref{dp_hmc_theorem_zcdp} the special case where
\(\alpha = \frac{1}{2}\). Because, as argued for DP penalty in
Section~\ref{dp_penalty_section}, having \(\alpha = \frac{1}{2}\) can be
guaranteed by setting the clip bound appropriately, and the clip bound can
be chosen to be anything if the parameters are only used with a single value
of \(n\),
Theorem~\ref{dp_hmc_theorem_zcdp} was simplified to not include \(\alpha\).

Figure~\ref{hmc_trajectory_fig} shows an example of a leapfrog trajectory
of DP HMC on a circular posterior (Section~\ref{circle_section}).
Even with noisy and clipped gradients,
the trajectory is able to stay inside the thin, circular area of high probability,
while moving a substantial distance on the circle.

\begin{figure}[h]
	\centering
  \includegraphics[width=\textwidth]{figures/hmc_trajectory}
  \caption{
    DP HMC proposal trajectory on a circular posterior
    (Section~\ref{circle_section}). The leapfrog simulation progress is shown
    in successive images from left to right and top to bottom.
    The orange point is the starting
    point of the leapfrog simulation and the blue line shows the progress
    of the leapfrog steps. The background is a contour plot of the circular
    posterior density.
  }
  \label{hmc_trajectory_fig}
\end{figure}

% The DP Stochastic gradient Langevin dynamics (DP-SGLD) algorithm~\cite{WFS15}
% is similar to DP HMC, but omits the MH acceptance test. Instead, in DP-SGLD,
% the step size \(\eta\) must be lowered towards 0 with each iteration to maintain
% the asymptotic convergence guarantee. In practice, DP-SGLD is run for a finite
% number of iterations, so \(\eta\) does not decay to 0, and the algorithm
% does not sample from the correct posterior. With the inclusion of the
% acceptance test, MCMC algorithms can sample from the correct posterior, even
% when run for a finite time~\cite{BDA}, though this is not theoretically guaranteed.

\chapter{Experimental Setup}\label{experiment_setup_chapter}

To compare the performance of the DP MH algorithms, they were run on
several models. This chapter introduces these models in Sections~\ref{gauss_model},
\ref{banana_section} and \ref{circle_section}. The practicalities, such as
the hyperparameter values of the model and the initialisation of the
algorithms, are considered in Section~\ref{practical_section}.
Finally, the main performance
metric used in the experiments is introduced in Section~\ref{mmd_section}.
The results of the experiments are discussed in Chapter~\ref{experiment_chapter}.

\section{The Gaussian Model}\label{gauss_model}

The simplest model used for the experiments is the Gaussian model with
known covariance. The likelihood and prior for \(n\) points of \(d\)-dimensional
data \(X \in \R^{n\times d}\) and parameters \(\theta\in \R^{d}\) are
\begin{align*}
  \theta &\sim \caln_{d}(\mu_{0}, \Sigma_{0}), \\
  x_{i} &\sim \caln_{d}(\theta, \Sigma),
\end{align*}
where \(x_{i}\) is a row of \(X\), \(\Sigma\) is the known covariance,
and \(\mu_{0}\) and \(\Sigma_{0}\) are the prior hyperparameters.
The posterior of this model is another \(d\)-dimensional Gaussian distribution
with parameters expressible in closed form~\cite[Section 3.5]{BDA},
so it is easy to sample from the posterior.

\section{The Banana Distribution}\label{banana_section}

The banana distribution~\cite{TPK14} is a banana-shaped probability
distribution that is a challenging target for MCMC algorithms. For this reason it has
been used to test MCMC algorithms in the literature~\cite{TPK14}.

\begin{definition}
    Let \(x\) have a \(d\)-variate Gaussian distribution with
    mean \(\mu\) and covariance matrix \(\Sigma\). Let
    \[
        g(x) = (x_1, x_2 - a(x_1 - m)^2 - b, x_3, \dotsc, x_d),
    \]
    with \(a, b, m \in \R\).
    The banana distribution with parameters \(\mu, \Sigma, a, b\) and \(m\)
    is the distribution of \(g(x)\). It is denoted by
    \(\ban(\mu, \Sigma, a, b, m)\).
\end{definition}

In the literature, the banana distribution is simply used as the target to
sample from, and is not the posterior in a Bayesian inference
problem~\cite{TPK14}. To test differentially private MCMC algorithms, the
target distribution must be the posterior of some inference problem, as
otherwise there is no data to protect with differential privacy.
Theorem~\ref{banana_posterior_theorem} gives a suitable inference problem
for testing DP MCMC algorithms.

\begin{theorem}\label{banana_posterior_theorem}
    Let
    \begin{align*}
        \theta = (\theta_1,\dotsc, \theta_d) &\sim
        \ban(0, \sigma_0^2I, a, b, m), \\
        x_1 &\sim \caln(\theta_1, \sigma_1^2), \\
        x_2 &\sim \caln(\theta_2 + a(\theta_1 - m)^2 + b, \sigma_2^2),\\
        x_3 &\sim \caln(\theta_3, \sigma_3^2), \\
            &\vdots \\
        x_d &\sim \caln(\theta_d, \sigma_d^2). \\
    \end{align*}
    Given data \(x_1,\dotsc, x_n\in \R^d\) and
    denoting \(\tau_i = \frac{1}{\sigma_i^2}\),
    the posterior of \(\theta\) tempered with \(T\) is the banana distribution
    \(\ban(\mu, \Sigma, a, b, m)\)
    with
    \[
        \bar{x}_i = \frac{1}{n}\sum_{j=1}^n x_{ji} \quad i\in \{1, 2\}.
    \]
    \[
        \mu = \left(\frac{Tn\tau_1\bar{x}_1}{Tn\tau_1 + \tau_0},\dotsc,
        \frac{Tn\tau_d\bar{x}_d}{Tn\tau_d + \tau_0}\right),
    \]
    \[
        \Sigma = \diag\left(
            \frac{1}{Tn\tau_1 + \tau_0},\dotsc,
            \frac{1}{Tn\tau_d + \tau_0}
        \right).
    \]
\end{theorem}
\begin{proof}
  See Appendix~\ref{banana_posterior_theorem_proof}.
\end{proof}
\newcounter{banana_posterior_theorem_number}
\setcounter{banana_posterior_theorem_number}{\value{theorem}}

The Gaussian distribution is a special case of the banana distribution,
as setting \(a = 0\) makes \(g\) the identity function. Similarly, setting
\(a = 0\) turns the Bayesian banana model into a Gaussian model.

\section{Circle Model}\label{circle_section}

Random walk MH algorithms may struggle with posteriors which are concentrated
on long, but thin regions. Having a large proposal variance causes the algorithm
to frequently jump out of the region of high probability, but lowering the
variance to stay in the region causes the chain to take a very long time to move
around in the region. An example of such a posterior is the circular posterior
described in this section.

The circle posterior is obtained from a model where the log-likelihood is
\[
    \ln p(r\mid x, y) = -a(x^2 + y^2 - r^2)^2
\]
where \(r\in \R\) is an observed data point, \(a > 0\) is a hyperparameter,
and \((x, y)\in \R^{2}\) are the parameters.
The likelihood is circular in the \((x, y)\)-plane, as it only depends on the
squared distance of the point \((x, y)\) from the origin. By choosing a flat
prior, \(p(x, y) = 1\) for all \((x, y)\in \R^{2}\), the posterior will be
proportional to the likelihood, meaning that the posterior will also be circular.

As the prior does not integrate to 1, it is not a proper probability density,
so Bayes' theorem does not guarantee that the posterior integrates to a finite
value. In this case, for a single data point \(r\) and any \(a > 0\),
\begin{align*}
  \int_{\R^{2}}p(r\mid x, y)\dx x\dx y
  &= \int_{\R^{2}}e^{-a(x^{2} + y^{2} - r^{2})^{2}}\dx x\dx y
  \\&= \int_{0}^{2\pi}\dx \phi \int_{0}^{\infty}\dx t
  \cdot te^{-a(t^{2}(\cos^{2} \phi + \sin^{2} \phi) - r^{2})^{2}}
  \\&= \int_{0}^{2\pi}\dx \phi \int_{0}^{\infty}\dx t
  \cdot te^{-a(t^{2} - r^{2})^{2}}
  \\&= \int_{0}^{2\pi}\dx \phi \int_{0}^{\infty}\dx u\cdot e^{-a(u - r^{2})^{2}}
  \\&< \infty,
\end{align*}
as, on the second to last line, the inner integral is over an unnormalised
Gaussian density, and
the outer integral is over a finite interval. For multiple data points, the
integral of the likelihood is
\[
  \int_{\R^{2}}\prod_{i}^{n}p(r_{i}\mid x,y)\dx x\dx y
  \leq \prod_{i}^{n}\left(\int_{\R^{2}}p(r_{i}\mid x,y)^{n}\dx x\dx y\right)^{\frac{1}{n}}
  < \infty,
\]
where the first inequality is Hölder's inequality applied \(n - 1\) times,
and the second follows from the single data point case, as
\(p(r\mid x,y)^{n} = e^{-an(x^{2} + y^{2} - r^{2})^{2}}\).
As the likelihood integrates to a
finite value, it can be considered an unnormalised density and sampled from
with MCMC.

Obtaining samples from the true posterior with the circle model is not
as easy as with the banana and Gaussian models, so using MMD, the metric
introduced in Section~\ref{mmd_section} to evaluate
the performance of an MCMC algorithm on the circle is nontrivial. However,
as the mean of the circle is in its center, and the samples from an MCMC
algorithm are likely to lie on the circle, the distance of the mean of the
sample from the origin indicates how evenly the samples a distributed throughout
the circle.

It remains to show that the mean of the circle is actually the origin. It turns
out that this is fairly simple. For the mean of the \(x\)-coordinate
\begin{align*}
  E_{x} &= \int_{\R}\dx x\cdot x\int_{\R} \dx y p(r\mid x, y)
  \\&= \int_{-\infty}^{\infty}\dx x\cdot x\int_{-\infty}^{\infty}\dx y
  \prod_{i} e^{-a(x^{2} + y^{2} - r_{i}^{2})^{2}}
  \\&= \int_{0}^{\infty}\dx x\cdot x\int_{-\infty}^{\infty}\dx y
  \prod_{i} e^{-a(x^{2} + y^{2} - r_{i}^{2})^{2}}
  + \int_{-\infty}^{0}\dx x\cdot x\int_{-\infty}^{\infty}\dx y
  \prod_{i} e^{-a(x^{2} + y^{2} - r_{i}^{2})^{2}}
  \\&= \int_{0}^{\infty}\dx x\cdot x\int_{-\infty}^{\infty}\dx y
  \prod_{i} e^{-a(x^{2} + y^{2} - r_{i}^{2})^{2}}
  - \int_{0}^{\infty}\dx x\cdot x\int_{-\infty}^{\infty}\dx y
  \prod_{i} e^{-a(x^{2} + y^{2} - r_{i}^{2})^{2}}
  \\&= 0.
\end{align*}
The mean of the \(y\)-coordinate is calculated similarly.

Unlike the banana and Gaussian models,
the circle model does not give a method to generate the data points
\(r_{i}\) from given values of \(x\) and \(y\). The \(r_{i}\) values used
in the experiments of Chapter~\ref{experiment_chapter} were sampled
from a normal distribution with mean 3 and variance 1.

\section{Practicalities of Running the Comparison}\label{practical_section}

Eight sets of model hyperparameters were used in the experiments.
The varied hyperparameters are shown in
Table~\ref{model_params_table} and contour plots of the
2-dimensional models are shown in Figure~\ref{posterior_plots_fig}.
All banana models had \(b = m = 0\). The likelihood variance of the banana
and 30-dimensional Gaussian
models was \(\sigma_{1}^{2} = 20\), \(\sigma_{2}^{2} = 2.5\) and
\(\sigma_{i}^{2} = 1\)
for \(i > 2\). The likelihood covariance for the correlated Gaussian model
was
\[
\begin{bmatrix}
  1 & 0.999 \\
  0.999 & 1
\end{bmatrix}.
\]
The true values of \(\theta\) in all experiment except the circle are
\(\theta_{2} = 3\) and \(\theta_{i} = 0\) for \(i \neq 2\).

All experiments ran 20 chains for each value
of \(\epsilon\) for DP MH experiments, or clip bound for clipping experiments,
for each algorithm included. Each of the
20 chains had a different starting point,
% chosen from a Gaussian
% distribution value with standard deviation as
% shown in Table~\ref{model_params_table},
but the starting points did
not vary across algorithms, or values of \(\epsilon\) or clip bound.
For the banana and Gaussian experiments, the starting points were chosen
from a Gaussian distribution centered on the true parameter values. The standard
deviation of the Gaussian was the mean of the component-wise standard deviations
of the a sample of the true posterior. In the circle experiment, the starting points
were chosen from a Gaussian distribution centered on \((0, 1)\) with
variance 1. Figure~\ref{posterior_plots_fig} shows the starting points for
each model as orange points.

The first half of the obtained samples were discarded as they may not represent
the true posterior, which is standard practice with MCMC~\cite{BDA}. The
latter half was compared to a reference sample obtained from the true posterior,
except in the circle experiment, where the mean of the sample was compared to
the true posterior mean of \((0,0)\).

Publicly available implementations of DP
Barker\footnote{\url{https://github.com/DPBayes/DP-MCMC-NeurIPS2019}}
and the Fourier
accountant\footnote{\url{https://github.com/DPBayes/PLD-Accountant}} were
used, and the rest of the algorithms were implemented by the author.
The accuracy and running time of the Fourier acountant is
determined by two parameters~\cite{KJH20}. They were left at their default values.
The code for running the experiments is freely
available\footnote{\url{https://github.com/oraisa/masters-thesis}}.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{figures/posterior_plots}
  \caption{
    Contour plots of the posterior densities of the 2-dimensional models.
    The orange points are the 20 starting points for each chain.
  }
  \label{posterior_plots_fig}
\end{figure}

\include{model_params_table}

\section{Maximum Mean Discrepancy}\label{mmd_section}

The convergence of non-DP MCMC algorithms is typically assessed using 
\(\hat{R}\)~\cite{BDA}, which measures how well multiple chains started from 
different points have mixed together. The utility of a sample produced by an 
MCMC algorithm can be evaluated using effective sample size (ESS)~\cite{BDA},
which is an estimate of the size of an uncorrelated sample of the posterior
with the same estimation utility as the MCMC sample. 

Both \(\hat{R}\) and ESS require that the MCMC algorithm asymptotically targets 
the true posterior, as they cannot detect an algorithm that has converged to the 
wrong distribution. Because some of the DP MCMC algorithms use approximations 
that may cause the algorithms to not converge to the true posterior, 
such as clipping the log-likelihood ratios, \(\hat{R}\) and ESS are not suitable
for assessing the performance of the algorithms.

Because the DP MCMC algorithms may not converge to the correct distribution, 
their performance should be evaluated with a metric that measures how close 
to the true distribution they are. A very general such metric is maximum mean
discrepancy (MMD)~\cite{GrettonBRSS12}. MMD between distributions \(p\) and \(q\) 
is defined as 
\[
    \mathrm{MMD}(p, q) = \sup_{f\in \mathcal{F}}(E_{x\sim p}f(x) - E_{y\sim q}f(y))
\]
where \(\mathcal{F}\) is some class of functions. By 
choosing a suitable \(\mathcal{F}\), \(\mathrm{MMD}(p, q)\) can be estimated from a 
sample from \(p\) and \(q\). The suitable classes \(\mathcal{F}\) can be 
characterised by kernel functions \(k\colon P\times Q \to \R\), where
\(P\) and \(Q\) are the supports of \(p\) and \(q\), respectively.
The Gaussian radial basis function (RBF) kernel 
\[
    k(x, y) = \exp\left(\frac{||x - y||_2^2}{2\sigma^2}\right)
\]
is particularly well suited, as it has the property that 
\(\mathrm{MMD}(p, q) = 0\) if and only if \(p = q\). After choosing a kernel,
\(\mathrm{MMD}(p, q)\) may be estimated from finite samples of \(p\) and \(q\).
To evaluate MCMC algorithms, one of the samples is the output of the algorithm 
to be evaluated, and the other is a sample from the true posterior.

The choice of the \(\sigma\) parameter of \(k\) affects the way MMD evaluates 
different kinds of differences in \(p\) and \(q\). For some preliminary experiments
in this thesis, \(\sigma\) was chosen to be 1, which penalised error in the 
mean much more than errors in higher moments in the experiments. 
The \(\sigma\) used for the final experiments is chosen by picking 50 subsamples
from both samples with replacement and setting \(\sigma\) to be the median 
between distances of points of the subsamples. This is following the procedure of 
Gretton et al.~\cite{GrettonBRSS12}, with the addition of the subsampling step
to handle samples of different sizes.

\chapter{Experiments}\label{experiment_chapter}

This chapter contains the experiments performed on both non-DP and DP MH
algorithms. Section~\ref{accounting_comparison_section} compares the
zCDP, RDP and ADP based privacy accounting methods that are available
for DP penalty, minibatch DP Penalty and DP HMC.
Section~\ref{clipping_experiments}
examines the effect clipping log-likelihood ratios, which is necessary for
DP MH algorithms, but may adversely affect their convergence.
Section~\ref{dp_mcmc_comparison} compares the different DP MH algorithms
on several models.

\section{Comparing Privacy Accounting Methods}\label{accounting_comparison_section}

DP penalty, minibatch DP penalty and DP HMC have two different methods to
compute the number of iterations the algorithm can run for, given a privacy
bound and parameters for the algorithm. These privacy accounting methods
are given by Theorems~\ref{DP_penalty_theorem_zcdp} and
\ref{DP_penalty_theorem_adp} for DP penalty, Theorems~\ref{dp_hmc_theorem_zcdp}
and \ref{dp_hmc_theorem_adp} for DP HMC, and
Theorem~\ref{dp_penalty_minibatch_theorem} and the Fourier
accountant~\cite{KJH20} for minibatch DP penalty.

Figure~\ref{accounting_comparison_fig} compares the number of iterations
each of the algorithms can run for for both accounting methods and different
values of \(\epsilon\) in the 2-dimensional flat banana experiment.
The ADP based methods of Theorems~\ref{DP_penalty_theorem_adp}
and \ref{dp_hmc_theorem_adp}, as well as the Fourier accountant, significantly
outperform the other accounting methods. This makes it clear that the tight
bounds given by the ADP based methods should be used in favor of loose methods.
The Fourier accountant is not easily applicable to DP Barker, as DP Barker
does not release the sample variance directly, but it may still be possible
to use the Fourier accountant with DP Barker. This is a potential question
for future research, as using a better privacy accountant may significantly
improve the performance of DP Barker.

\begin{figure}[h]
	\centering
  \includegraphics[width=\textwidth]{figures/accountant_comparison}
  \caption{
    Comparing the zCDP or RDP and ADP based privacy accounting methods
    for the algorithms with multiple accounting methods. The left panel
    compares zCDP based accounting (Theorem~\ref{DP_penalty_theorem_zcdp})
    and ADP based accounting (Theorem~\ref{DP_penalty_theorem_adp}) for
    the DP penalty algorithm. The middle panel compares the RDP accounting
    (Theorem~\ref{dp_penalty_minibatch_theorem}) and the Fourier accountant.
    The right panel compares the zCDP (Theorem~\ref{dp_hmc_theorem_zcdp})
    and ADP (Theorem~\ref{dp_hmc_theorem_adp}) based accounting.
    The ADP based methods significantly outperform the other methods
    in all cases.
  }
  \label{accounting_comparison_fig}
\end{figure}

\section{The Effects of Clipping}\label{clipping_experiments}

The this section looks at the effect of clipping log-likelihood ratios,
which is necessary for DP MH algorithms, but may affect their convergence
to the correct posterior.
Both HMC and random walk Metropolis-Hastings (RWMH)\footnote{Metropolis-Hastings using the Gaussian
distribution as the proposal distributions.}
algorithms were run on the 2 and 10 dimensional flat banana models.
DP was not used so that error from the extra noise would not affect the results.

For both 2 and 10 dimensions, 500 samples from HMC and 3000 samples from
RWMH were taken and the latter half of them were
compared to 1000 samples from the true posterior. The reference posterior sample
was also compared to other samples from the posterior to obtain a baseline.
The sample sizes and other parameters of the algorithms were tuned so that
the algorithms converged without clipping.

Figure~\ref{clip_effect_fig} shows the results of the clipping experiment.
The top left and bottom left panels show MMD as a function of the clip bound 
and the fraction of log-likelihoods that was actually clipped for both
HMC and RWMH in the 2-dimensional model. The effect of clipping on MMD 
is nonexistent for all but the lowest clip bounds. The top and bottom right 
panels show results for the 10-dimensional model. This time there are chains 
that did not converge correctly with most clip bounds, but the chains with 
the higher bounds converged. Based on these results, if the clip fraction is 
less than 20\%, clipping is likely undetectable without a large sample.
For the other experiments, clip bounds were tuned to achieve less than
10\% clipping, to ensure that the error from clipping is minimal.
As this experiment only used a single model in fairly low
dimensions, it is possible that in other settings the clip fraction should be
even lower, so future research should look into clipping in other models.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/clipping.pdf}
    \caption{
        The effect of log-likelihood ratio clipping on the posterior of the
        banana model for random walk Metropolis-Hastings and HMC.
        The top row shows posterior MMD as a function of the clip bound, and 
        the bottom row shows MMD as a function of the fraction of log-likelihoods
        that were clipped. The left columns used a 2-dimensional posterior 
        while the right columns had a 10-dimensional posterior. 
        The black lines show the MMDs of ten different samples of the true posterior
        compared to the reference sample. All of the lines are close the each 
        other and appear as a single line.
    }
    \label{clip_effect_fig}
\end{figure}

\section{Comparison of DP MH Algorithms}\label{dp_mcmc_comparison}

The experiments in this section compare the DP MH algorithms discussed in this
thesis. The compared algorithms are the DP penalty algorithm with and without
GWMH and OCU~\cite{YildirimE19}
(Section~\ref{dp_penalty_section}), the DP Barker
algorithm~\cite{HeikkilaJDH19}\footnote{
  While DP Barker is not technically an MH algorithm, is still uses the
  Barker acceptance test, and is very close to DP MH algorithms.
}
(Section~\ref{dp_barker_section}), the minibatch DP penalty algorithm, again
with and without GWMH and OCU (Section~\ref{dp_minibatch_penalty_section}), and
DP HMC (Section~\ref{dp_hmc_section}).

The \(\delta\) for all experiments is \(\frac{0.1}{n}\), and \(\epsilon\) is
varied.
All algorithms used the best privacy accounting methods discussed in this thesis.
For both variants of DP penalty and DP HMC, these are the PLD based
Theorems~\ref{DP_penalty_theorem_adp} and \ref{dp_hmc_theorem_adp}.
DP penalty with subsampling uses the Fourier accountant~\cite{KJH20}
and DP Barker uses Theorem~\ref{dp_barker_theorem}. The RDP based theorems
for the minibatch algorithms are not tight for the ADP bounds that were used,
so their results could be improved by using an accounting method for ADP.

The hyperparameters of the algorithms
were tuned by running the algorithms with \(\epsilon = 4\) and examining the
results, trying to find hyperparameters that minimise MMD. Clip bounds were
tuned so that less than 10\% of the log-likelihood ratios were clipped,
as the results of Section~\ref{clipping_experiments} show minimal effect on
MMD at that point.

Figure~\ref{banana_mmd_fig} shows the MMD for each algorithm as a function of
\(\epsilon\) for the flat and tempered banana models with 2 and 10 dimensions.
In the non-tempered models, the minibatch algorithms are not very useful,
with the exception of DP Barker in 10 dimensions. Of the non-minibatch algorithms,
with tempering HMC beats the penalty algorithms, but without tempering this is
reversed. The black lines at the bottom show MMDs of 10 different samples of the
true posterior compared to the reference sample. In 2 dimensions, the best
algorithms get close to the MMDs achieved by the non-DP algorithms from
the clipping experiment of Section~\ref{clipping_experiments}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/banana_mmd.pdf}
  \caption{
    MMD as a function of \(\epsilon\) for the different MCMC algorithms,
    on flat and tempered banana models with both a low number of dimensions (2)
    and a high number of dimensions (10). The black lines show the MMD of 10
    different samples of the true posterior compared to the reference sample.
  }
  \label{banana_mmd_fig}

\end{figure}

Figure~\ref{banana_extra_mmd_fig} shows MMDs for the more complicated models,
the 30-dimensional Gaussian, the narrow banana and the highly correlated Gaussian.
The minibatch algorithms were not included as the previous experiment indicates
that they perform poorly without tempering.
In the 30-dimensional Gaussian, DP penalty with OCU and GWMH beats the other
two algorithms. In the narrow
banana all algorithms are approximately equal. In the highly correlated
Gaussian experiment, the penalty algorithms perform equally, and HMC beats
both of them. In the 30-dimensional Gaussian, all algorithms have clearly
decreasing MMD with increasing \(\epsilon\), but in the narrow banana and
correlated Gaussian, only HMC on the correlated Gaussian is able to achieve
significantly lower MMD with higher \(\epsilon\).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/banana_extra.pdf}
  \caption{
    MMD for the 30-dimensional Gaussian, hard banana and highly correlated
    2-dimensional Gaussian. The black lines show the MMD of 10 samples of the
    true posterior compared to the reference sample.
  }
  \label{banana_extra_mmd_fig}
\end{figure}

Figure~\ref{banana_clipping_fig} shows the fraction of log-likelihood ratios
that were clipped for each \(\epsilon\) and algorithm. Almost all runs had
less than 10\% clipping, with the exception of DP Barker, as adjusting its
clip bound is not possible, and the 2-dimensional tempered experiment where
some clip fractions are getting closer to 20\%.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/banana_clipping.pdf}
  \caption{
    Clipping for the easy and tempered banana experiments.
  }
  \label{banana_clipping_fig}
\end{figure}

Figure~\ref{banana_extra_clipping_fig} shows clipping for the harder models.
Again, almost all runs have less than 10\% clipping, with the exception of
some runs with on the narrow banana, especially with \(\epsilon = 1\).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/banana_extra_clipping}
  \caption{
    Clipping for 30-dimensional Gaussian, hard banana and highly correlated
    2-dimensional Gaussian.
  }
  \label{banana_extra_clipping_fig}
\end{figure}

Figure~\ref{circle_fig} shows the results for the circle model. The distance
of the sample mean from the true mean (the origin) was used instead of MMD,
as computing MMD requires a sample from the posterior, which is not trivial
to obtain for the circle model. Both algorithms perform well on average.
Clipping is again low
for both algorithms, and HMC has a fairly large variance in clipping between
different runs.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/circle}
  \caption{
    Circle mean error on the left and clipping on the right.
  }
  \label{circle_fig}
\end{figure}

Figure~\ref{grad_clipping_fig} shows the fraction of clipped
gradients for DP HMC. Clipping gradients does not affect the asymptotic
convergence of DP HMC, but it may lower the acceptance rate and thus
the performance of the algorithm. As raising the clip bound to lower
gradient clipping increases the noise added to gradient, thus also lowering
the acceptance rate, it is not clear what an appropriate level of gradient
clipping would be. The observed levels of clipping are fairly constant across
values of \(\epsilon\), but vary in the models, and in the narrow banana model,
there is very large variance in clipping.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/grad_clipping}
  \caption{
    Gradient clipping for DP HMC.
  }
  \label{grad_clipping_fig}
\end{figure}

\chapter{Conclusions}\label{conclusion_chapter}

The results of the experiments of Section~\ref{dp_mcmc_comparison} show that
using DP MH for Bayesian inference is possible, but will require large
datasets, especially with complex models. With the already fairly large datasets
with \(10^{5} \leq n \leq 2\cdot 10^{5}\), the DP MH algorithms were able to
achieve significantly lower MMDs with increasing \(\epsilon\) on most models,
but the algorithms only got close to the baseline MMDs from samples of the
true posterior on the simplest models, the flat and tempered bananas. This also
required a large value of \(\epsilon = 6\), while performance with the more
reasonable values of \(\epsilon\) like 1 and 2 was much worse. The circle model
is the only exception, where performance with \(\epsilon = 1\) was reasonable.
The experiments done by Heikkilä et al.~\cite{HeikkilaJDH19}
and Yildirim and Ermis~\cite{YildirimE19} have similar results: good performance
of a DP MH algorithm with a reasonable \(\epsilon \leq 2\) requires a large
dataset with \(n \geq 10^{5}\).

When comparing the different DP MH algorithms, the results of
Section~\ref{dp_mcmc_comparison} show that neither DP HMC or DP penalty is
clearly better than the other, as both algorithms beat the other on some models.
The usefulness of OCU+GWHM for DP penalty is also debatable, as DP penalty
OCU+GWMH only clearly beat DP penalty on the 30-dimensional Gaussian. The only
clear conclusion from the comparisons is that the minibatch algorithms are
only useful with tempered posteriors, and even there they are not better
than the algorithms using the full data, so subsampling the log-likelihood ratios
does not seem to be a useful addition to DP MH, which is surprising, as
subsampling significantly lowers the privacy cost of releasing the log-likelihood
ratio.

The failure of subsampling log-likelihood ratios does not imply that subsampling
HMC gradients will not be useful. Gradient subsampling has been done with
non-DP HMC~\cite{CFG14, DFB14} to reduce computation time, and using subsampling
in the DP setting is a potential direction for future research. However,
naive gradient subsampling in HMC has been shown to fail in high
dimensions~\cite{Bet15}, so additional methods to control the error from subsampling
are likely needed, such as the friction term~\cite{CFG14} briefly discussed in
Section~\ref{dp_hmc_section}, or the Nosé-Hoover thermostat~\cite{DFB14} that
dynamically adjusts the friction.

The large number of parameters DP MH algorithms have compared to non-DP
MH makes tuning them particularly difficult. DP HMC is the clearest example
of this, where 4 parameters tune the different trade-offs between privacy and
accuracy, in addition to the parameters of HMC, which are already known to
be difficult to tune in the non-DP case~\cite{neal2012mcmc}.

Several methods and guidelines for tuning the parameters of non-DP MH algorithms
have been developed. The simplest of these are heuristic guidelines or in some
cases proofs~\cite{RGG97} of optimal acceptance rates. For MH with a Gaussian
proposal and a suitable target distribution, it has been shown that the optimal
acceptance rate approaches approximately \(23\%\) as the dimensionality of the
target approaches infinity~\cite{RGG97}. For HMC, a \(65\%\) acceptance rate
is recommended~\cite{neal2012mcmc}. These guidelines can be used to tune the
proposal variance,
or, in the case of HMC, \(\eta\), until the algorithm has the desired acceptance
rate. They can be used when tuning DP MH algorithms, and were used to some
extent in tuning the parameters for the experiments of this thesis, but it
is not clear how well these guidelines apply under DP.
Yildirim and Ermis show that DP penalty has a lower acceptance
rate than the non-DP MH algorithm with the same proposal~\cite{YildirimE19},
so the optimal
acceptance rates of DP MH algorithms are likely lower their non-DP
counterparts, but the exact optimal acceptance rates will likely depend on the
privacy bounds.

Tuning the parameters of an MH algorithm by hand is time consuming,
even with a guideline on the optimal acceptance rate, and requires the user of
the algorithm to know the guidelines and their applicability. Tuning the
parameters automatically alleviates these problems. In the context of HMC, these
automatically tuned algorithms are called dynamic HMC algorithms. The most
prominent dynamic HMC algorithms is the No-U-turn sampler (NUTS)~\cite{HoG14},
that is able to tune both \(\eta\) and \(L\) automatically.
The automatic tuning of both parameters made HMC usable in automatic inference
libraries such as
Stan~\cite{stan} that are typically used by applied practitioners who are not
experts in MCMC algorithms. Extending NUTS and other dynamic HMC algorithms that
can tune some parameters automatically to the DP case is a potential avenue of
future research. The empirical results on clipping in
Section~\ref{clipping_experiments} may be usable as guidelines for tuning the
clip bounds, perhaps even automatically.

A fundamental difficulty in tuning DP MH algorithms is the fact that the
metrics used to tune the algorithm, such as acceptance and clipping rates,
cannot be released without incurring a privacy cost and adding noise to the
metrics. Even tuning the algorithm
internally based on the metrics, and releasing only the final parameters incurs
some privacy cost, which can be argued to be negligible, but cannot be
computed, as formalising the way a human chooses parameters based on the metrics
is impossible. This can be sidestepped by tuning the algorithm on synthetic or
publicly available data, and only using the private data with the final parameters,
but this risks overfitting the parameters to the tuning dataset and having
poor performance on the private dataset.
This thesis largely ignores these issues for the sake of comparing
the algorithms, but potential users of DP MH on real private data should not
ignore them, which makes developing automatic methods for tuning parameters
that allow computing the privacy cost of tuning
even more important than in the non-DP case.

Other practical issues that the experiments of Section~\ref{dp_mcmc_comparison}
ignore are the initialisation of the algorithms, and the fact that
computer implementations cannot sample from continuous distributions exactly.
In existing work, DP MH algorithms have been initialised using a point
estimate obtained from another DP method with very low privacy
cost~\cite{HeikkilaJDH19, WFS15}. The initialisation method used
Section~\ref{dp_mcmc_comparison} mimics the effects of using a point estimate by
choosing random values around the true parameter values. This is unlikely to
affect the comparisons of the DP MH algorithms, as all of the algorithms
would use the same method to obtain the point estimate in any case.

The issue of sampling continuous distributions on computers for DP were first
identified for the Laplace distribution~\cite{Mir12}. When sampling the Laplace
distribution with finite precision floating point numbers, the bits of the
results can contain information about the private data that destroys DP, even
when the analysis is theoretically DP with exact sampling of the Laplace
distribution. These issues likely exist when sampling the Gaussian distribution,
which means that actual implementations of the algorithms in this thesis may
not have their theoretical DP guarantees. Recently, the differential privacy
of a discrete variant of the Gaussian distribution has been
analysed~\cite{C0S20}. Using a discrete distribution instead of a continuous one
sidesteps floating point issues, but in the context of DP MH, the penalty
correction requires adding continuous Gaussian noise. Investigating the effect
of using discrete Gaussian noise instead with the penalty algorithm is another
potential avenue of future research.

The results presented in this thesis show that DP MH is a viable way to
conduct DP Bayesian inference with large enough datasets. There are still
issues that should be investigated before deploying DP MH with real
private data, but they are likely solvable, and most, like parameter tuning,
and inexact sampling, also affect other methods of DP Bayesian inference.
The benefits of MH over other Bayesian inference methods are present in the
DP setting, so research into the many unknowns of DP MH is likely to be
fruitful.

% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\cleardoublepage %fixes the position of bibliography in bookmarks
\phantomsection

\addcontentsline{toc}{chapter}{\bibname} % This lines adds the bibliography to the ToC
\bibliographystyle{alpha} % numbering alphabetic order
\bibliography{../references.bib}

\begin{appendices}
\myappendixtitle

\chapter{Proof of Theorem~\ref{banana_posterior_theorem}}\label{banana_posterior_theorem_proof}

\newcounter{temp_counter}
\setcounter{temp_counter}{\value{theorem}}
\setcounter{theorem}{\value{banana_posterior_theorem_number}}
\addtocounter{theorem}{-1}
\begin{theorem}
    Let
    \begin{align*}
        \theta = (\theta_1,\dotsc, \theta_d) &\sim
        \ban(0, \sigma_0^2I, a, b, m) \\
        x_1 &\sim \caln(\theta_1, \sigma_1^2) \\
        x_2 &\sim \caln(\theta_2 + a(\theta_1 - m)^2 + b, \sigma_2^2)\\
        x_3 &\sim \caln(\theta_3, \sigma_3^2) \\
            &\vdots \\
        x_d &\sim \caln(\theta_d, \sigma_d^2) \\
    \end{align*}
    Given data \(x_1,\dotsc, x_n\in \R^d\) and
    denoting \(\tau_i = \frac{1}{\sigma_i^2}\),
    the posterior of \(\theta\) tempered with \(T\) is the banana distribution
    \(\ban(\mu, \Sigma, a, b, m)\)
    with
    \[
        \bar{x}_i = \frac{1}{n}\sum_{j=1}^n x_{ji} \quad i\in \{1, 2\}
    \]
    \[
        \mu = \left(\frac{Tn\tau_1\bar{x}_1}{Tn\tau_1 + \tau_0},\dotsc,
        \frac{Tn\tau_d\bar{x}_d}{Tn\tau_d + \tau_0}\right),
    \]
    \[
        \Sigma = \diag\left(
            \frac{1}{Tn\tau_1 + \tau_0},\dotsc,
            \frac{1}{Tn\tau_d + \tau_0}
        \right).
    \]
\end{theorem}
\begin{proof}
    Because
    \[
    g^{-1}(y) = (y_1, y_2 + a(y_1 - m)^2 + b, y_3\dotsc, y_d)
    \]
    and the Jacobian determinant of \(g^{-1}\) is \(1\),
    for a positive-definite \(\Sigma\) the banana distribution has
    density proportional to
    \[
    \exp\left(-\frac{1}{2}(g^{-1}(x) - \mu)^T\Sigma^{-1}(g^{-1}(x) - \mu)\right)
    \]
    With \(\Sigma = \diag(\sigma_1^2, \sigma_2^2)\) the density is proportional
    to
    \[
    \exp
    \left(-\frac{1}{2}\left(\left(\frac{x_1 - \mu_1}{\sigma_1}\right)^2
    + \left(\frac{x_2 + a(x_1 - m)^2 + b - \mu_2}{\sigma_2}\right)^2
    + \sum_{i=3}^d\left(\frac{x_i - \mu_i}{\sigma_i}\right)^2\right)\right)
    \]

    Denote \(u = \theta_2 + a(\theta_1 - m)^2 + b\).
    The tempered posterior of \(\theta\) is
    \begin{align*}
        p(\theta\mid X) &\propto p(X\mid \theta)^Tp(\theta)
        \\&= p(x_1\mid \theta_1)^Tp(x_2\mid \theta_1, \theta_2)^T
        \prod_{i=3}^d p(x_i\mid \theta_i)^T p(\theta)
        \\&= p(x_1\mid \theta_1)^Tp(x_2\mid \theta_1, \theta_2)^T
        \prod_{i=3}^d p(x_i\mid \theta_i)^T
        \\&\cdot \exp\left(-\frac{1}{2}\left(\tau_0\theta_1^2
        + \tau_0(\theta_2 + a(\theta_1 - m)^2 + b)^2
        \sum_{i=3}^d \tau_0\theta_i^2\right)\right)
        \\&= p(x_1\mid \theta_1)^Tp(x_2\mid \theta_1, \theta_2)^T
        \exp\left(-\frac{1}{2}\left(\tau_0\theta_1^2
        + \tau_0(\theta_2 + a(\theta_1 - m)^2 + b)^2\right)\right)
        \\&\cdot \prod_{i=3}^d p(x_i\mid \theta_i)^T
        \exp\left(-\frac{1}{2}\sum_{i=3}^d \tau_0\theta_i^2\right)
    \end{align*}

    Considering the upper and lower part of the last expression separately
    \begin{align*}
        &p(x_1\mid \theta_1)^Tp(x_2\mid \theta_1, \theta_2)^T
        \exp\left(-\frac{1}{2}\left(\tau_0\theta_1^2
        + \tau_0(\theta_2 + a(\theta_1 - m)^2 + b)^2\right)\right)
        \\&\propto \left(\prod_{i=1}^n \exp
        \left(-\frac{(x_{i1} - \theta_1)^2\tau_1}{2}\right)\right)^T
        \cdot\left(\prod_{i=1}^n \exp\left(-\frac{(x_{i2} - \theta_2
        - a(\theta_1 - m)^2 - b)^2\tau_2}{2}\right)\right)^T
        \\&\cdot \exp\left(-\frac{1}{2}\left(\tau_0\theta_1^2
        + \tau_0(\theta_2 + a(\theta_1 - m)^2 + b)^2\right)\right)
        \\&=\exp\Bigg(-\frac{1}{2}\Big(T\tau_1\sum_{i=1}^n
        (x_{i1} - \theta_1)^2
        + T\tau_2\sum_{i=1}^n(x_{i2} - u)^2
        + \tau_0\theta_1^2 + \tau_0 u^2\Big)\Bigg)
        \\&=\exp\Bigg(-\frac{1}{2}\Big(T\tau_1\sum_{i=1}^n
        (x_{i1} - \bar{x}_1)^2 + T\tau_1n(\bar{x}_1 - \theta_1)^2
        \\&+ T\tau_2\sum_{i=1}^n (x_{i2}  - \bar{x}_2)^2 + T\tau_2n(\bar{x}_2 - u)^2
        + \tau_0\theta_1^2 + \tau_0 u^2\Big)\Bigg)
        \\&\propto\exp\Bigg(-\frac{1}{2}\Big(T\tau_1n(\bar{x}_1 - \theta_1)^2
        + T\tau_2n(\bar{x}_2 - u)^2
        + \tau_0\theta_1^2 + \tau_0 u^2\Big)\Bigg)
        \\&=\exp\Bigg(-\frac{1}{2}\Big(T\tau_1n\bar{x}_1^2
        - 2T\tau_1n\bar{x}_1\theta_1 + nT\tau_1\theta_1^2 + \tau_0\theta_1^2
        \\&+ T\tau_2n\bar{x}_2^2 - 2T\tau_2n\bar{x}_2u + nT\tau_2u^2
        + \tau_0 u^2\Big)\Bigg)
        \\&\propto\exp\Bigg(-\frac{1}{2}\Big((Tn\tau_1 + \tau_0)\theta_1^2
        - 2T\tau_1n\bar{x}_1\theta_1
        + (Tn\tau_2 + \tau_0)u^2 - 2T\tau_2n\bar{x}_2u \Big)\Bigg)
        \\&=\exp\Bigg(-\frac{1}{2}\Big(
        (Tn\tau_1 + \tau_0)\left(\theta_1^2
        - \frac{2T\tau_1n\bar{x}_1\theta_1}{Tn\tau_1 + \tau_0} \right)
        + (Tn\tau_2 + \tau_0)\left(u^2 - \frac{2T\tau_2n\bar{x}_2u}
        {Tn\tau_2 + \tau_0}\right) \Big)\Bigg)
        \\&\propto\exp\Bigg(-\frac{1}{2}\Big(
        (Tn\tau_1 + \tau_0)\left(\theta_1
        - \frac{T\tau_1n\bar{x}_1}{Tn\tau_1 + \tau_0} \right)^2
        + (Tn\tau_2 + \tau_0)\left(u - \frac{T\tau_2n\bar{x}_2}
        {Tn\tau_2 + \tau_0}\right)^2 \Big)\Bigg)
    \end{align*}
    and
    \begin{align*}
        &\prod_{i=3}^d p(x_i\mid \theta_i)^T
        \cdot \exp\left(-\frac{1}{2}\sum_{i=3}^d \tau_0\theta_i^2\right)
      \\&\propto \exp\left(-\frac{1}{2}T\sum_{j=3}^d\tau_j\sum_{i=1}^n (x_{ij} - \theta_j)^2
      - \frac{1}{2}\sum_{j=3}^d\tau_0\theta_j^2\right)
      \\&= \exp\left(-\frac{1}{2}\sum_{j=3}^d\left(T\tau_j\sum_{i=1}^n (x_{ij} - \theta_j)^2
      + \tau_0\theta_j^2\right)\right)
      \\&= \exp\left(-\frac{1}{2}\sum_{j=3}^d\left(T\tau_j\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2
      + T\tau_j n(\bar{x}_j - \theta_j)^2 + \tau_0\theta_j^2\right)\right)
      \\&\propto \exp\left(-\frac{1}{2}\sum_{j=3}^d\left(T\tau_j n(\bar{x}_j - \theta_j)^2
      + \tau_0\theta_j^2\right)\right)
      \\&\propto \exp\left(-\frac{1}{2}\sum_{j=3}^d\left(
      -2T\tau_j n\bar{x}_j\theta_j + T\tau_jn\theta_j^2
      + \tau_0\theta_j^2\right)\right)
      \\&\propto \exp\left(-\frac{1}{2}\sum_{j=3}^d\left(
      (Tn\tau_j + \tau_0)\theta_j^2
      - 2Tn\tau_j\bar{x}_j\theta_j + \frac{(Tn\tau_j\bar{x}_j)^2}{Tn\tau_j + \tau_0}\right)\right)
      \\&= \exp\left(-\frac{1}{2}\sum_{j=3}^d (Tn\tau_j + \tau_0)\left(\theta_j
      - \frac{Tn\tau_1\bar{x}_i}{Tn\tau_j + \tau_0}\right)^2\right)
    \end{align*}
    Multiplying the resulting expression above gives a density proportional
    to the banana distribution.
    As \(p(\theta\mid X)\) is proportional to the density of a
    banana distribution, the posterior is the banana distribution
    \(\ban(\mu, \Sigma, a, b, m)\)
    with
    \[
        \mu = \left(\frac{Tn\tau_1\bar{x}_1}{Tn\tau_1 + \tau_0},\dotsc,
        \frac{Tn\tau_d\bar{x}_d}{Tn\tau_d + \tau_0}\right),
    \]
    \[
        \Sigma = \diag\left(
            \frac{1}{Tn\tau_1 + \tau_0},\dotsc,
            \frac{1}{Tn\tau_d + \tau_0}
        \right).
        \qedhere
    \]
\end{proof}
\setcounter{theorem}{\value{temp_counter}}

\chapter{Differentiability of the \(\clip\)-function}\label{clip_diff_chapter}

This appendix proves that functions of the form \(\clip\circ f\) are
almost everywhere differentiable for sufficiently well-behaved \(f\),
as required by the proof of volume preservation
of DP HMC leapfrog iterations (Section~\ref{dp_hmc_section}).
Lemma~\ref{clip_is_almost_everywhere_differentiable} gives very general
sufficient conditions, and Lemma~\ref{model_clip_ok_lemma} proves that the
models considered in this thesis meet the conditions.

\begin{lemma}\label{level_set_lemma}
  Let \(d \geq 2\) and \(g\colon U \to \R\), where \(U\) is an open subset of
  \(\R^{d}\), be continuously differentiable. Let \(S = \{x\in U\mid f(x) = b\}\),
  \(b\in \R\).
  If for all \(x\in S\), \(\nabla f(x) \neq 0\), \(S\) is a \((d - 1)\)-dimensional
  hypersurface.
\end{lemma}
\begin{proof}
	See~\cite[Section 9.2]{Tu11}.
\end{proof}

\begin{lemma}\label{clip_is_almost_everywhere_differentiable}
  Let \(f\colon \R^{d}\to \R^{d}\) be continuously differentiable.
  If the set of saddle points \(U\) of \(||f||\) is a null set,
  the set \(A\subset \R^{d}\)
  where \(\clip_{b} \circ f\) is not differentiable is a null set.
\end{lemma}
\begin{proof}
  \[
    \clip_{b}(x) =
    \begin{cases}
      \frac{x}{||x||}\min\{||x||, b\} & \text{if } x \neq 0\\
      0 & \text{if } x = 0
    \end{cases}
  \]
  which means that \(\clip_{b}\) is differentiable for all \(x\in \R^{d}\) with
  \(||x|| \neq b\). Consider \(\clip_{b}\circ f\) in a neighbourhood \(B\) of
  point \(x_{0}\in \R^{d}\). If \(||f(x)|| \leq b\) for all \(x\in B\),
  \(\clip_{b}(f(x)) = f(x)\) in \(B\), which is differentiable. If \(||f(x)|| \geq b\)
  for all \(x\in B\), \(\clip_{b}(f(x)) = \frac{f(x)b}{||f(x)||}\) in \(B\),
  which is also differentiable because \(f(x) \neq 0\) when
  \(||f(x)|| \geq b > 0\). In both cases \(\clip_{b}\circ f\) is differentiable
  at \(x_{0}\). This means that for all \(x_{0}\in A\), \(||f(x_{0})|| = b\), but
  every neighborhood
  of \(x_{0}\) has points \(x'\) and \(x''\) such that \(||f(x')|| < b\) and
  \(||f(x'')|| > b\). With the differentiability of \(f\), this means that
  if \(\nabla ||f(x_{0})|| = 0\), \(x_{0}\) is a saddle point of \(||f||\).

  Let \(S = \{x\in \R^{d}\mid ||f(x)|| = b \text{ and } \nabla ||f(x)|| \neq 0\}\).
  % With the assumptions on \(f\),
  % there is a neighborhood \(E\subset U\)
  % for every \(x_{0}\in S\) where \(f(x) \neq 0\) for all \(x\in E\) and
  % \(||f||\) is differentiable.
  % \(\nabla ||f(x)|| = \frac{2f(x)}{||f(x)||}J_{f}(x) \neq 0\) for \(x\in S\).
  By Lemma~\ref{level_set_lemma},
  \(S\) is a \((d-1)\)-dimensional hypersurface, which has zero measure
  in \(d\)-dimensional space. Because \(A\cap U^{C} \subset S\), \(A\cap U^{C}\)
  is also a null set. Since \(A = (A\cap U^{C}) \cup (A\cap U)\),
  \(A\) is a null set.

  % Let \(E(x_{1}) = \{(x_{2}, \dotsc, \x_{d})\in \R^{d-1}\mid (x_{1}\dotsc, x_{d})\in A\}\).
  % \(E(x_{1})\) has measure zero for almost all \(x_{1}\).

  % Let \(x_{1}\in \R\) where \(E(x_{1})\) has non-zero measure.
\end{proof}

\begin{lemma}\label{model_clip_ok_lemma}
  The log-likelihoods of the Gaussian, banana and circle models meet the
  conditions of Lemma~\ref{clip_is_almost_everywhere_differentiable}.
\end{lemma}
\begin{proof}
  For DP HMC, set \(f(\theta) = \nabla \ln p(x\mid \theta)\) for some
  datapoint \(x\). Denote \(l(\theta) = \ln (x\mid \theta)\).
  The conditions of
  Lemma~\ref{clip_is_almost_everywhere_differentiable} for \(l\) are met if
  \(l\) is twice continuously differentiable and
  \[
    \nabla ||\nabla l(\theta)||
    = \frac{2\nabla l(\theta)}{||\nabla l(\theta)||}J_{\nabla l}(\theta)
    = \frac{2\nabla l(\theta)}{||\nabla l(\theta)||}H_{l}(\theta) \neq 0
  \]
  almost everywhere,
  where \(H_{l}\) is the Hessian matrix of \(l\). Because \(\nabla l = 0\) and
  \(||\nabla l|| = 0\) at only one point, having \(\det H_{l}(x) \neq 0\)
  almost everywhere is sufficient.

  For the Gaussian log-likelihood
  \[
    l(\theta) = \frac{1}{2}(x - \theta)^{T}\Sigma^{-1}(x - \theta),
  \]
  so
  \[
    \nabla l(\theta) = \Sigma^{-1}(x - \theta)
  \]
  and \(H_{l}(\theta) = \Sigma^{-1}\).

  For the banana log-likelihood, using the notation from
  Appendix~\ref{banana_posterior_theorem_proof},
  \[
    l(\theta) = \frac{1}{2}(g^{-1}(x) - \theta)^{T}\Sigma^{-1}(g^{-1}(x) - \theta),
  \]
  so
  \[
    \nabla l(\theta) = \Sigma^{-1}(g^{-1}(x) - \theta)
  \]
  and \(H_{l}(\theta) = \Sigma^{-1}\).

  The circle log-likelihood is
  \[
    l(x, y) = -a(x^{2} + y^{2} - r^{2})^{2},
  \]
  \[
    \nabla l(x, y) = -4a(x^{3} + xy^{2} - xr^{2}, x^{2}y + y^{3} - yr^{2}),
  \]
  so
  \[
    H_{l}(x, y) = -4a
    \begin{bmatrix}
      3x^{2} + y^{2} - r^{2} & 2xy \\
      2xy & 3y^{2} + x^{2} - r^{2} \\
    \end{bmatrix}
  \]
  and
  \begin{align*}
    \det H_{l}(x, y) &= -4a((3x^{2} + y^{2} - r^{2})(3y^{2} + x^{2} - r^{2})
                       - 4x^{2}y^{2})
    \\&= -4a(9x^{2}y^{2} + 3x^{4} - 3x^{2}r^{2} + 3y^{4} + y^{2}x^{2} -y^{2}r^{2}
    - 3y^{2}r^{2} - x^{2}r^{2} + r^{4} - 4x^{2}y^{2})
    \\&= -4a(6x^{2}y^{2} + 3x^{4} - 4x^{2}r^{2} + 3y^{4} - 4y^{2}r^{2} + r^{4})
    \\&= -4a(6x^{2}y^{2} + 3(x^{4} + y^{4}) - 4r^{2}(x^{2} + y^{2}) + r^{4}).
  \end{align*}
  \[
    \nabla \det H_{l}(x, y) = -4a(12xy^{2} + 12x^{3} - 8r^{2}x,
    12yx^{2} + 12y^{3} - 8r^{2}y),
  \]
  which is zero in the origin and on the circle
  \[
    x^{2} + y^{2} = \frac{2}{3}r^{2}.
  \]
  Elsewhere, Lemma~\ref{level_set_lemma} can be applied to \(\det H_{l}\), which
  means that the set where \(\det H_{l}(x, y) = 0\) is a 1-dimensional curve.
\end{proof}
\end{appendices}

\end{document}
